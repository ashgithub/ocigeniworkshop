{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Cache\n",
    "\n",
    "### What this file does:\n",
    "\n",
    "Demonstrates semantic caching using OCI Generative AI embeddings and Oracle Database vector search. It stores Q&A pairs as embeddings and retrieves semantically similar answers for user queries, reducing the need for repeated LLM calls.\n",
    "\n",
    "**Documentation to reference:**\n",
    "\n",
    "- OCI Gen AI: https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm\n",
    "- Oracle DB Vectors: https://docs.oracle.com/en/database/oracle/oracle-database/23/vecse/\n",
    "- OCI Python SDK: https://github.com/oracle/oci-python-sdk/tree/master/src/oci/generative_ai_inference/models\n",
    "\n",
    "**Relevant slack channels:**\n",
    "\n",
    "- #generative-ai-users: *for questions on OCI Gen AI* \n",
    "- #igiu-innovation-lab: *general discussions on your project* \n",
    "- #igiu-ai-learning: *help with sandbox environment or help with running this code* \n",
    "\n",
    "**Env setup:**\n",
    "\n",
    "- sandbox.yaml: Contains OCI config, compartment, DB details, and wallet path.\n",
    "- .env: Load environment variables (e.g., API keys if needed).\n",
    "- configure cwd for jupyter match your workspace python code: \n",
    "    -  vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "    -  change from `${fileDirname}` to `${workspaceFolder}`\n",
    "\n",
    "\n",
    "**How to run in notebook:**\n",
    "\n",
    "- Make sure your runtime environment has all dependencies and access to required config files.\n",
    "- Run the notebook cells in order.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import array\n",
    "import oci\n",
    "import oracledb\n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML\n",
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import OnDemandServingMode, EmbedTextDetails\n",
    "\n",
    "# Constants\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "load_dotenv()\n",
    "\n",
    "EMBED_MODEL = \"cohere.embed-multilingual-v3.0\"\n",
    "# Available embedding models: cohere.embed-english-v3.0, cohere.embed-multilingual-v3.0, cohere.embed-english-light-v3.0, cohere.embed-multilingual-light-v3.0\n",
    "\n",
    "LLM_ENDPOINT = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "# Available models with tool calling support: cohere.command-r-08-2024, cohere.command-r-plus-08-2024, cohere.command-a-03-2025\n",
    "LLM_MODEL = \"cohere.command-a-03-2025\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load config and initialize clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from a YAML file.\"\"\"\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            return EnvYAML(config_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Configuration file '{config_path}' not found.\")\n",
    "        return None\n",
    "    \n",
    "scfg = load_config(SANDBOX_CONFIG_FILE)\n",
    "config = oci.config.from_file(\n",
    "    os.path.expanduser(scfg[\"oci\"][\"configFile\"]),\n",
    "    scfg[\"oci\"][\"profile\"]\n",
    ")\n",
    "compartment_id = scfg[\"oci\"][\"compartment\"]\n",
    "table_prefix = scfg[\"db\"][\"tablePrefix\"]\n",
    "wallet_path = os.path.expanduser(scfg[\"db\"][\"walletPath\"])\n",
    "\n",
    "# Create LLM client\n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "    config=config,\n",
    "    service_endpoint=LLM_ENDPOINT,\n",
    "    retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "    timeout=(10, 240)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Sample Q&A pairs for semantic caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Q&A pairs for semantic caching\n",
    "qa_pairs = [\n",
    "    {\n",
    "        \"question\": \"What is the largest continent by land area?\",\n",
    "        \"answer\": \"Asia is the largest continent, covering about 30% of Earth's land area. It is home to diverse cultures, languages, and ecosystems.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which country has the longest coastline in the world?\",\n",
    "        \"answer\": \"Canada has the longest coastline, stretching over 202,080 kilometers. Its vast coastlines are along the Atlantic, Pacific, and Arctic Oceans.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What river is the longest in the world?\",\n",
    "        \"answer\": \"The Nile River is traditionally considered the longest river, flowing over 6,650 kilometers through northeastern Africa. It passes through countries like Egypt and Sudan.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which desert is the largest hot desert in the world?\",\n",
    "        \"answer\": \"The Sahara Desert is the largest hot desert, covering approximately 9.2 million square kilometers. It spans across North Africa from the Atlantic Ocean to the Red Sea.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the smallest country in the world by land area?\",\n",
    "        \"answer\": \"Vatican City is the smallest country, with an area of just 44 hectares (110 acres). It serves as the spiritual and administrative center of the Roman Catholic Church.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which mountain is the highest in the world above sea level?\",\n",
    "        \"answer\": \"Mount Everest is the highest mountain above sea level, standing at 8,848 meters (29,029 feet). It is part of the Himalayas and located on the border between Nepal and China.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What ocean is the deepest in the world?\",\n",
    "        \"answer\": \"The Pacific Ocean is the deepest ocean, with an average depth of about 4,280 meters. The Mariana Trench within it reaches depths of over 10,900 meters.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which two continents are entirely located in the Southern Hemisphere?\",\n",
    "        \"answer\": \"Australia and Antarctica are entirely located in the Southern Hemisphere. Both continents have unique ecosystems and climates.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What country has the most time zones?\",\n",
    "        \"answer\": \"France has the most time zones when including its overseas territories. In total, it spans 12 different time zones across various regions worldwide.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which lake is considered the world's largest by surface area?\",\n",
    "        \"answer\": \"Lake Superior, part of North America's Great Lakes, is often considered the largest freshwater lake by surface area. It covers approximately 82,100 square kilometers.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Set up Oracle DB connection and create vector table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = oracledb.connect(\n",
    "    config_dir=wallet_path,\n",
    "    user=scfg[\"db\"][\"username\"],\n",
    "    password=scfg[\"db\"][\"password\"],\n",
    "    dsn=scfg[\"db\"][\"dsn\"],\n",
    "    wallet_location=wallet_path,\n",
    "    wallet_password=scfg[\"db\"][\"walletPass\"]\n",
    ")\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "def create_table(cursor, table_prefix):\n",
    "    \"\"\"Drop and create semantic cache table.\"\"\"\n",
    "    sql_statements = [\n",
    "        f\"DROP TABLE {table_prefix}_semantic_cache PURGE\",\n",
    "        f\"\"\"\n",
    "        CREATE TABLE {table_prefix}_semantic_cache (\n",
    "            id NUMBER,\n",
    "            question VARCHAR2(4000),\n",
    "            answer VARCHAR2(4000),\n",
    "            embedding VECTOR,\n",
    "            PRIMARY KEY (id)\n",
    "        )\n",
    "        \"\"\"\n",
    "    ]\n",
    "\n",
    "    for stmt in sql_statements:\n",
    "        try:\n",
    "            cursor.execute(stmt)\n",
    "        except Exception as e:\n",
    "            # Ignore if table doesn't exist and create a new one\n",
    "            print(f\"Skipping error: {e}\")\n",
    "\n",
    "print(\"Creating table for semantic cache...\")\n",
    "create_table(cursor, table_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Generate embeddings for Q&A pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_payload(questions, embed_type, compartment_id):\n",
    "    \"\"\"Build embedding payload for OCI Generative AI.\"\"\"\n",
    "    embed_text_detail = EmbedTextDetails()\n",
    "    embed_text_detail.serving_mode = OnDemandServingMode(model_id=EMBED_MODEL)\n",
    "    embed_text_detail.truncate = embed_text_detail.TRUNCATE_END\n",
    "    embed_text_detail.input_type = embed_type\n",
    "    embed_text_detail.compartment_id = compartment_id\n",
    "    embed_text_detail.inputs = questions\n",
    "    return embed_text_detail\n",
    "\n",
    "print(\"Generating embeddings for Q&A pairs...\")\n",
    "embed_payload = get_embed_payload(\n",
    "    [pair[\"question\"] for pair in qa_pairs],\n",
    "    EmbedTextDetails.INPUT_TYPE_SEARCH_DOCUMENT,\n",
    "    compartment_id\n",
    ")\n",
    "embed_response = llm_client.embed_text(embed_payload)\n",
    "embeddings = embed_response.data.embeddings\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Insert embeddings into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(cursor, table_prefix, id, question, answer, vec):\n",
    "    \"\"\"Insert Q&A pair with embedding into table.\"\"\"\n",
    "    cursor.execute(f\"INSERT INTO {table_prefix}_semantic_cache VALUES (:1, :2, :3, :4)\", [\n",
    "        id, question, answer, vec\n",
    "    ])\n",
    "\n",
    "for i, emb in enumerate(embeddings):\n",
    "    insert_data(cursor, table_prefix, i, qa_pairs[i]['question'], qa_pairs[i]['answer'], array.array(\"f\", emb))\n",
    "    print(f\"Inserted Q&A pair {i}\")\n",
    "\n",
    "db_connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Read stored Q&A pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(cursor, table_prefix):\n",
    "    \"\"\"Read and display all Q&A pairs from table.\"\"\"\n",
    "    cursor.execute(f\"SELECT id, question, answer FROM {table_prefix}_semantic_cache\")\n",
    "    for row in cursor:\n",
    "        print(f\"{row[0]}:{row[1]}:{[row[2]]}\")\n",
    "\n",
    "print(\"Reading stored Q&A pairs...\")\n",
    "read_data(cursor, table_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Interactive semantic search and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_data(cursor, table_prefix, vec, top_k=3):\n",
    "    \"\"\"Perform semantic search with cosine similarity.\"\"\"\n",
    "    # Try adding the constraint the distance of < 0.5, something we will need to finetune based on data\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT id, question, answer, vector_distance(embedding, :1, COSINE) d\n",
    "        FROM {table_prefix}_semantic_cache\n",
    "        ORDER BY d\n",
    "        FETCH FIRST {top_k} ROWS ONLY\n",
    "    \"\"\", [vec])\n",
    "\n",
    "    rows = []\n",
    "    for row in cursor:\n",
    "        r = [row[0], row[1], row[2], row[3]]\n",
    "        print(r)\n",
    "        rows.append(r)\n",
    "\n",
    "    return rows\n",
    "\n",
    "print(\"\\nReady for queries. Try rephrasing questions to test semantic matching.\")\n",
    "print(\"Suggestions for experimentation:\")\n",
    "print(\"- Change the embedding model (e.g., to 'cohere.embed-english-v3.0')\")\n",
    "print(\"- Adjust top_k parameter in search_data for more/less results\")\n",
    "print(\"- Add distance threshold filtering in search query\")\n",
    "print(\"- Experiment with different similarity algorithms (COSINE, DOT, EUCLIDEAN)\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\nAsk a question (or 'q' to exit): \").strip()\n",
    "    if query.lower() == \"q\":\n",
    "        break\n",
    "\n",
    "    query_list = [query]\n",
    "    query_payload = get_embed_payload(\n",
    "        query_list,\n",
    "        EmbedTextDetails.INPUT_TYPE_SEARCH_QUERY,\n",
    "        compartment_id\n",
    "    )\n",
    "    query_response = llm_client.embed_text(query_payload)\n",
    "    query_vec = array.array(\"f\", query_response.data.embeddings[0])\n",
    "\n",
    "    print(f\"Searching for: '{query}'\")\n",
    "    results = search_data(cursor, table_prefix, query_vec, top_k=5)\n",
    "\n",
    "    print(\"\\n************************** SEMANTIC CACHE RESULTS **************************\")\n",
    "    if results:\n",
    "        print(f\"Best match answer: {results[0][2]}\")\n",
    "        print(\"\\nOther similar questions and distances:\")\n",
    "        for result in results:\n",
    "            print(f\"{result[3]:.4f}: {result[1]}\")\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "\n",
    "# Close DB connections\n",
    "cursor.close()\n",
    "db_connection.close()\n",
    "print(\"Database connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Exercises\n",
    "\n",
    "Here are some ideas for experimenting with semantic caching:\n",
    "\n",
    "1. **Integrate with RAG**: Add semantic cache to your RAG agent and measure performance differences.\n",
    "   - Check if a question is already answered before querying the LLM.\n",
    "   - Set a distance threshold (e.g., < 0.5) to determine if a cached answer is relevant.\n",
    "   - If no match, ask the LLM and store the new Q&A pair.\n",
    "\n",
    "2. **Tune similarity search**: Experiment with different parameters:\n",
    "   - Try other distance algorithms: DOT, EUCLIDEAN.\n",
    "   - Adjust the top_k value and observe results.\n",
    "   - Add WHERE clauses to filter by distance thresholds.\n",
    "\n",
    "3. **Expand the cache**: Add more Q&A pairs on different topics and test generalization.\n",
    "   - Try questions with synonyms or different phrasings.\n",
    "   - Compare multilingual embeddings for different languages.\n",
    "\n",
    "4. **Performance optimization**: Implement caching strategies:\n",
    "   - Use approximate nearest neighbor (ANN) search for larger datasets.\n",
    "   - Add indexing on the embedding column for faster queries.\n",
    "   - Implement cache expiration or LRU eviction policies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
