{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Step Function Calling with OCI Generative AI\n",
    "\n",
    "### What this file does:\n",
    "Demonstrates multi-step function calling using OCI Generative AI Cohere models. Shows how the model can make multiple tool calls sequentially to accomplish a complex task, with both regular and streaming versions.\n",
    "\n",
    "**Documentation to reference:**\n",
    "- OCI Gen AI: https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm\n",
    "- Cohere Command Models: https://docs.cohere.com/docs/command-r\n",
    "- OCI Python SDK: https://github.com/oracle/oci-python-sdk/tree/master/src/oci/generative_ai_inference\n",
    "\n",
    "**Relevant slack channels:**\n",
    "- #generative-ai-users: *for questions on OCI Gen AI*\n",
    "- #igiu-innovation-lab: *general discussions on your project*\n",
    "- #igiu-ai-learning: *help with sandbox environment or help with running this code*\n",
    "\n",
    "**Env setup:**\n",
    "- sandbox.yaml: Contains OCI config, compartment, and other details.\n",
    "- .env: Load environment variables (e.g., API keys if needed).\n",
    "- configure cwd for jupyter match your workspace python code: \n",
    "    -  vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "    -  change from `${fileDirname}` to `${workspaceFolder}`\n",
    "\n",
    "\n",
    "**How to run in notebook:**\n",
    "- Make sure your runtime environment has all dependencies and access to required config files.\n",
    "- Run the notebook cells in order.\n",
    "\n",
    "### Supported models (https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm)\n",
    "- cohere.command-a-03-2025\n",
    "- cohere.command-r-08-2024\n",
    "- cohere.command-r-plus-08-2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the  variables\n",
    "\n",
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import OnDemandServingMode, EmbedTextDetails,CohereChatRequest, ChatDetails\n",
    "import oci\n",
    "import json, os\n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML\n",
    "\n",
    "#####\n",
    "#make sure your sandbox.yaml file is setup for your environment. You might have to specify the full path depending on  your `cwd` \n",
    "# you can also try making your cwd ofr jupyter match your workspace python code: \n",
    "# vscopde menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "# change from ${fileDirname} to ${workspaceFolder}\n",
    "#####\n",
    "\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "load_dotenv()\n",
    "\n",
    "LLM_MODEL = \"cohere.command-a-03-2025\" \n",
    "     \n",
    "PREAMBLE = \"\"\"\n",
    "        Analyze the problem and pick the right set of tools to answer the question\n",
    "\"\"\"\n",
    "MESSAGE = \"\"\"\n",
    "       Total sales amount over the 28th and 29th of September.\n",
    "\"\"\"\n",
    "llm_service_endpoint= \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sep up the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report tool \n",
    "date_param = oci.generative_ai_inference.models.CohereParameterDefinition()\n",
    "date_param.description = \"Retrieves sales data for this day, formatted as YYYY-MM-DD.\"\n",
    "date_param.type = \"str\"\n",
    "date_param.is_required = True\n",
    "\n",
    "report_tool = oci.generative_ai_inference.models.CohereTool()\n",
    "report_tool.name = \"query_daily_sales_report\"\n",
    "report_tool.description = \"Connects to a database to retrieve overall sales volumes and sales information for a given day.\"\n",
    "report_tool.parameter_definitions = {\n",
    "    \"date\": date_param\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculator tool \n",
    "\n",
    "expression_param = oci.generative_ai_inference.models.CohereParameterDefinition()\n",
    "expression_param.description = \"The expression to caculate.\"\n",
    "expression_param.type = \"str\"\n",
    "expression_param.is_required = True\n",
    "\n",
    "calculator_tool = oci.generative_ai_inference.models.CohereTool()\n",
    "calculator_tool.name = \"simple_calculator\"\n",
    "calculator_tool.description = \"Connects to a database to retrieve overall sales volumes and sales information for a given day.\"\n",
    "calculator_tool.parameter_definitions = {\n",
    "    \"expression\": expression_param\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the config file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scfg = EnvYAML(SANDBOX_CONFIG_FILE)\n",
    "if scfg is None or \"oci\" not in scfg or \"bucket\" not in scfg:\n",
    "    raise RuntimeError(\"Invalid sandbox configuration.\")\n",
    "\n",
    "#read the oci config\n",
    "config = oci.config.from_file(os.path.expanduser(scfg[\"oci\"][\"configFile\"]),scfg[\"oci\"][\"profile\"])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## specify the tools to use in the chat request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat request      \n",
    "llm_chat_request = CohereChatRequest()\n",
    "llm_chat_request.preamble_override = PREAMBLE \n",
    "llm_chat_request.message = MESSAGE\n",
    "llm_chat_request.is_stream = False \n",
    "llm_chat_request.max_tokens = 500 # max token to generate, can lead to incomplete responses\n",
    "llm_chat_request.is_force_single_step = False\n",
    "llm_chat_request.tools = [ report_tool, calculator_tool ]\n",
    "\n",
    "\n",
    "\n",
    "# set up chat details\n",
    "chat_detail = ChatDetails()\n",
    "chat_detail.serving_mode = OnDemandServingMode(model_id=LLM_MODEL)\n",
    "chat_detail.compartment_id = scfg[\"oci\"][\"compartment\"]\n",
    "chat_detail.chat_request = llm_chat_request\n",
    "\n",
    "# set up the LLM client \n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "                config=config,\n",
    "                service_endpoint=llm_service_endpoint,\n",
    "                retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "                timeout=(10,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## call the LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "print(f\"**************************Step {step} Result**************************\")\n",
    "print(f\"message = {chat_response.data.chat_response.text}\")\n",
    "print(f\"tool calls = {chat_response.data.chat_response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the tools \n",
    "\n",
    "Note: \n",
    "1. in this example we are not explicity calling the tool, we are just returning a made up response.  you will insert an explicit call to teh toolapi for real code\n",
    "2. we have to keep calling chat  in a toop, so that llm can look at the tool reponse in generating it response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_results = []\n",
    "llm_chat_request.message = \"\"\n",
    "while chat_response.data.chat_response.tool_calls is not None:   # we have invoke the llm till there are no more tools left \n",
    "    for call in chat_response.data.chat_response.tool_calls: # there amay be more than one tool to call \n",
    "        tool_result = oci.generative_ai_inference.models.CohereToolResult()\n",
    "        tool_result.call = call\n",
    "        if call.name == \"query_daily_sales_report\":\n",
    "            if call.parameters[\"date\"] == \"2023-09-29\":\n",
    "                # We should   call tool here we re simulating teh json response here \n",
    "                tool_result.outputs = [\n",
    "                    {\n",
    "                        \"date\": call.parameters[\"date\"],\n",
    "                        \"summary\": \"Total Sales Amount: 8000, Total Units Sold: 200\"\n",
    "                    }\n",
    "                ] \n",
    "            else:\n",
    "                # We should   call tool here we re simulating teh json response here \n",
    "                tool_result.outputs = [\n",
    "                    {\n",
    "                        \"date\": call.parameters[\"date\"],\n",
    "                        \"summary\": \"Total Sales Amount: 5000, Total Units Sold: 125\"\n",
    "                    }\n",
    "                ] \n",
    "        else:\n",
    "            # We should   call tool here we re simulating teh json response here \n",
    "            tool_result.outputs = [\n",
    "                {\n",
    "                    \"expression\": call.parameters[\"expression\"],\n",
    "                    \"answer\": \"13000\"\n",
    "                }\n",
    "            ]\n",
    "        tool_results.append(tool_result)  # the tool responses are collectec to feed back to the llm \n",
    "\n",
    "    llm_chat_request.chat_history = chat_response.data.chat_response.chat_history\n",
    "    llm_chat_request.tool_results = tool_results\n",
    "    # call the llm again with responses from previous round of tools \n",
    "    step = step +1 \n",
    "    chat_response = llm_client.chat(chat_detail)\n",
    "\n",
    "    # Print result\n",
    "\n",
    "    print(f\"**************************Step {step} Result**************************\")\n",
    "    print(f\"message = {chat_response.data.chat_response.text}\")\n",
    "    print(f\"tool calls = {chat_response.data.chat_response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming version \n",
    "\n",
    "Steaming response reduces latency, specially if response has a lot of text. but its involved as we have to process events \n",
    "\n",
    "we first define the function to process the evnets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_tool_calls_and_chat_history(chat_response):\n",
    "    for event in chat_response.data.events():\n",
    "        res = json.loads(event.data)\n",
    "        text = res['text']\n",
    "        if 'finishReason' in res:\n",
    "            if 'toolCalls' in res:\n",
    "                #print(f\"\\ntools to use : {res['toolCalls']}\",flush=True)\n",
    "                return text,res['toolCalls'], res['chatHistory']\n",
    "            else:\n",
    "                return text,None, res['chatHistory']\n",
    "        else:\n",
    "            if 'text' in res:\n",
    "                print(res['text'], end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### call the llm in streaming mode \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rest from earlier run \n",
    "llm_chat_request.is_stream = True\n",
    "llm_chat_request.tool_results = None\n",
    "llm_chat_request.chat_history = None\n",
    "llm_chat_request.preamble_override = PREAMBLE \n",
    "llm_chat_request.message = MESSAGE\n",
    "\n",
    "\n",
    "\n",
    "step =1 \n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "\n",
    "text,tool_calls, chat_history = get_tool_calls_and_chat_history(chat_response)\n",
    "print(f\"\\n **************************Step {step} Result**************************\")\n",
    "print(f\"message = {text}\")\n",
    "print(f\"tool calls = {tool_calls}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# call tools & iterate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_results = []\n",
    "llm_chat_request.message = \"\"\n",
    "while tool_calls is not None:\n",
    "    for call in tool_calls:\n",
    "        call = oci.generative_ai_inference.models.CohereToolCall(**call)\n",
    "        tool_result = oci.generative_ai_inference.models.CohereToolResult()\n",
    "        tool_result.call = call\n",
    "        if call.name == \"query_daily_sales_report\":\n",
    "            if call.parameters[\"date\"] == \"2023-09-29\":\n",
    "                tool_result.outputs = [\n",
    "                    {\n",
    "                        \"date\": call.parameters[\"date\"],\n",
    "                        \"summary\": \"Total Sales Amount: 8000, Total Units Sold: 200\"\n",
    "                    }\n",
    "                ] \n",
    "            else:\n",
    "                tool_result.outputs = [\n",
    "                    {\n",
    "                        \"date\": call.parameters[\"date\"],\n",
    "                        \"summary\": \"Total Sales Amount: 5000, Total Units Sold: 125\"\n",
    "                    }\n",
    "                ] \n",
    "        else:\n",
    "            tool_result.outputs = [\n",
    "                {\n",
    "                    \"expression\": call.parameters[\"expression\"],\n",
    "                    \"answer\": \"13000\"\n",
    "                }\n",
    "            ]\n",
    "        tool_results.append(tool_result)\n",
    "\n",
    "    llm_chat_request.chat_history = chat_history\n",
    "    llm_chat_request.tool_results = tool_results\n",
    "    step = step+1\n",
    "    chat_response = llm_client.chat(chat_detail)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"\\n**************************Step {step} Result**************************\",flush=True)\n",
    "    #print(vars(chat_response))\n",
    "    text,tool_calls, chat_history = get_tool_calls_and_chat_history(chat_response)\n",
    "    print(f\"message = {text}\")\n",
    "    print(f\"tool calls = {tool_calls}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Clothes Recommender app \n",
    "Create an App that  Answers the Question : \n",
    "    * What clothes should I wear to Oracles headquarter tomm\n",
    "\n",
    "1. Create following APis :\n",
    "    * Weather API\n",
    "    * City API\n",
    "    * Clothes API\n",
    "1. Things to try \n",
    "    * Conversational History/ Documents for part of information eg: gender / location etc\n",
    "    * Context in preamble\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
