{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Step Function Calling with OCI Generative AI\n",
    "\n",
    "### What this file does:\n",
    "Demonstrates single-step function calling using OCI Generative AI Cohere models. Shows how to define tools, make tool calls, and provide tool results to get a final response, with both regular and streaming versions.\n",
    "\n",
    "**Documentation to reference:**\n",
    "- OCI Gen AI: https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm\n",
    "- Cohere Command Models: https://docs.cohere.com/docs/command-r\n",
    "- OCI Python SDK: https://github.com/oracle/oci-python-sdk/tree/master/src/oci/generative_ai_inference\n",
    "\n",
    "**Relevant slack channels:**\n",
    "- #generative-ai-users: *for questions on OCI Gen AI*\n",
    "- #igiu-innovation-lab: *general discussions on your project*\n",
    "- #igiu-ai-learning: *help with sandbox environment or help with running this code*\n",
    "\n",
    "**Env setup:**\n",
    "- sandbox.yaml: Contains OCI config, compartment, and other details.\n",
    "- .env: Load environment variables (e.g., API keys if needed).\n",
    "- configure cwd for jupyter match your workspace python code: \n",
    "    -  vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "    -  change from `${fileDirname}` to `${workspaceFolder}`\n",
    "\n",
    "\n",
    "**How to run in notebook:**\n",
    "- Make sure your runtime environment has all dependencies and access to required config files.\n",
    "- Run the notebook cells in order.\n",
    "\n",
    "### Supported models (https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm)\n",
    "- cohere.command-a-03-2025\n",
    "- cohere.command-r-08-2024\n",
    "- cohere.command-r-plus-08-2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the  variables\n",
    "\n",
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import OnDemandServingMode, EmbedTextDetails,CohereChatRequest, ChatDetails\n",
    "import oci\n",
    "import json, os\n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML\n",
    "\n",
    "#####\n",
    "#make sure your sandbox.yaml file is setup for your environment. You might have to specify the full path depending on  your `cwd` \n",
    "# you can also try making your cwd ofr jupyter match your workspace python code: \n",
    "# vscopde menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "# change from ${fileDirname} to ${workspaceFolder}\n",
    "#####\n",
    "\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "load_dotenv()\n",
    "\n",
    "LLM_MODEL = \"cohere.command-a-03-2025\" \n",
    "PREAMBLE = \"\"\"\n",
    "        Analyze the problem and pick the right set of tools to answer the question\n",
    "\"\"\"\n",
    "MESSAGE = \"\"\"\n",
    "       \"I'd like 4 apples and a fish please\"\n",
    "\"\"\"\n",
    "llm_service_endpoint= \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sep up the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_param = oci.generative_ai_inference.models.CohereParameterDefinition()\n",
    "item_param.description = \"the item requested to be purchased, in all caps eg. Bananas should be BANANAS\"\n",
    "item_param.type = \"str\"\n",
    "item_param.is_required = True\n",
    "\n",
    "quantity_param = oci.generative_ai_inference.models.CohereParameterDefinition()\n",
    "quantity_param.description = \"how many of the items should be purchased\"\n",
    "quantity_param.type = \"int\"\n",
    "quantity_param.is_required = True\n",
    "\n",
    "shop_tool = oci.generative_ai_inference.models.CohereTool()\n",
    "shop_tool.name = \"personal_shopper\"\n",
    "shop_tool.description = \"Returns items and requested volumes to purchase\"\n",
    "shop_tool.parameter_definitions = {\n",
    "    \"item\": item_param,\n",
    "    \"quantity\": quantity_param\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the config file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scfg = EnvYAML(SANDBOX_CONFIG_FILE)\n",
    "if scfg is None or \"oci\" not in scfg or \"bucket\" not in scfg:\n",
    "    raise RuntimeError(\"Invalid sandbox configuration.\")\n",
    "\n",
    "#read the oci config\n",
    "config = oci.config.from_file(os.path.expanduser(scfg[\"oci\"][\"configFile\"]),scfg[\"oci\"][\"profile\"])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## specify the tools to use in the chat request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat request      \n",
    "llm_chat_request = CohereChatRequest()\n",
    "llm_chat_request.preamble_override = PREAMBLE \n",
    "llm_chat_request.message = MESSAGE\n",
    "llm_chat_request.is_stream = False \n",
    "llm_chat_request.max_tokens = 500 # max token to generate, can lead to incomplete responses\n",
    "llm_chat_request.is_force_single_step = True\n",
    "llm_chat_request.tools = [ shop_tool]\n",
    "\n",
    "\n",
    "\n",
    "# set up chat details\n",
    "chat_detail = ChatDetails()\n",
    "chat_detail.serving_mode = OnDemandServingMode(model_id=LLM_MODEL)\n",
    "chat_detail.compartment_id = scfg[\"oci\"][\"compartment\"]\n",
    "chat_detail.chat_request = llm_chat_request\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## call the LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the LLM client \n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "                config=config,\n",
    "                service_endpoint=llm_service_endpoint,\n",
    "                retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "                timeout=(10,240))\n",
    "\n",
    "step = 1\n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "print(f\"**************************Step {step} Result**************************\")\n",
    "print(f\"message = {chat_response.data.chat_response.text}\")\n",
    "print(f\"tool calls = {chat_response.data.chat_response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the tool\n",
    "\n",
    "Note: \n",
    "1. in this example we are not explicity calling the tool, we are just returning a made up response.  you will insert an explicit call to teh toolapi for real code\n",
    "2. We have the call teh llm again with teh tool response for the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat_request.tool_results = []\n",
    "i = 0\n",
    "for call in chat_response.data.chat_response.tool_calls:\n",
    "        tool_result = oci.generative_ai_inference.models.CohereToolResult()\n",
    "        tool_result.call = call\n",
    "        # try to change response to out of stock etc for one or both items and see\n",
    "        tool_result.outputs = [ { \"response\": \"Completed, in stock\" } ] \n",
    "        llm_chat_request.tool_results.append(tool_result)\n",
    "\n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "\n",
    "# Print result\n",
    "print(\"**************************Step 2 Result**************************\")\n",
    "print(f\"message = {chat_response.data.chat_response.text}\")\n",
    "print(f\"tool calls = {chat_response.data.chat_response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming version \n",
    "\n",
    "Steaming response reduces latency, specially if response has a lot of text. but its involved as we have to process events \n",
    "\n",
    "we first define the function to process the events\n",
    "\n",
    "\n",
    "PS: this seems to be broken. refer to python code ( command_r_tool_single_step_demo_streaming.py) for the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tool_calls(chat_response):\n",
    "    for event in chat_response.data.events():\n",
    "        res = json.loads(event.data)\n",
    "        text = res['text']\n",
    "        if 'finishReason' in res:\n",
    "            if 'toolCalls' in res:\n",
    "                #print(f\"\\ntools to use : {res['toolCalls']}\",flush=True)\n",
    "                return text,res['toolCalls']\n",
    "            else:\n",
    "                return text,None\n",
    "        else:\n",
    "            if 'text' in res:\n",
    "                print(res['text'], end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "    return None,None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### call the llm in streaming mode \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat_request.is_stream = True\n",
    "llm_chat_request.tool_results = None\n",
    "step =1 \n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "\n",
    "text,tool_calls= get_tool_calls(chat_response)\n",
    "print(f\"\\n **************************Step {step} Result**************************\")\n",
    "print(f\"message = {text}\")\n",
    "print(f\"tool calls = {tool_calls}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# call tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat_request.tool_results = []\n",
    "for call in tool_calls:\n",
    "    tool_result = oci.generative_ai_inference.models.CohereToolResult()\n",
    "    tool_result.call = call\n",
    "    tool_result.outputs = [ { \"response\": \"Completed\" } ] \n",
    "    llm_chat_request.tool_results.append(tool_result)\n",
    "\n",
    "step = step+1\n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "text,tool_calls= get_tool_calls(chat_response)\n",
    "print(f\"message = {text}\")\n",
    "print(f\"tool calls = {tool_calls}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise : Weather Answering App\n",
    "\n",
    "1. Create an App that  gives the weather information for a given city \n",
    "    * Eg:  \tA. Lucknow will have temp around 25 degrees and will rain \n",
    "\n",
    "2. calls the weather API for a given city\n",
    "    * Create an API \n",
    "        * Input \n",
    "            * City Name  \n",
    "            * DatDays in future\n",
    "        * Output\n",
    "            * Low\n",
    "            * High\n",
    "            * Chance of rain \n",
    "3. Types of questions supported\n",
    "    * What is the weather in lucknow tomm\n",
    "    * What will be highs in capital of india\n",
    "    * Which major city in Karnataka will it rain tomm \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
