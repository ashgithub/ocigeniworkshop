{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Function calling \n",
    "\n",
    "## Classifying the question asked into a class of questions\n",
    "- once classfied the appropriate set of tools for that class can be exposed to LLM \n",
    "- This helps tio reducxe the total number of tools exposed to LLM and reduce context data\n",
    "\n",
    "### Supported models (https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm) \n",
    "- cohere.command-a-03-2025\n",
    "- cohere.command-r-08-2024\n",
    "- cohere.command-r-plus-08-2024\n",
    "\n",
    "\n",
    "Questions use #generative-ai-users  or ##igiu-innovation-lab slack channels \n",
    "\n",
    " if you have errors running sample code reach out for help in #igiu-ai-learning\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the  variables\n",
    "\n",
    "\n",
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import *\n",
    "import oci\n",
    "import json, os \n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML\n",
    "\n",
    "#####\n",
    "#make sure your sandbox.yaml file is setup for your environment. You might have to specify the full path depending on  your `cwd` \n",
    "# you can also try making your cwd ofr jupyter match your workspace python code: \n",
    "# vscopde menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "# change from ${fileDirname} to ${workspaceFolder}\n",
    "#####\n",
    "\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "load_dotenv()\n",
    "\n",
    "LLM_MODEL = \"cohere.command-r-plus-08-2024\" \n",
    "PREAMBLE = \"\"\"\n",
    "        answer in three bullets. respond in hindi \n",
    "\"\"\"\n",
    "MESSAGE = \"\"\"\n",
    "        \"why is indian cricket team so good\"\n",
    "\"\"\"\n",
    "\n",
    "llm_service_endpoint= \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "llm_client = None\n",
    "llm_payload = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scfg = EnvYAML(SANDBOX_CONFIG_FILE)\n",
    "if scfg is None or \"oci\" not in scfg or \"bucket\" not in scfg:\n",
    "    raise RuntimeError(\"Invalid sandbox configuration.\")\n",
    "\n",
    "#read the oci config\n",
    "config = oci.config.from_file(os.path.expanduser(scfg[\"oci\"][\"configFile\"]),scfg[\"oci\"][\"profile\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up Chat Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cohere_chat_request = CohereChatRequest()\n",
    "cohere_chat_request.preamble_override = \"\"\"\n",
    "        \n",
    "        You are a call classifier you carefully analyze teh question and classify it into one of the following categories. \n",
    "        you then return just the category in response\n",
    "        \n",
    "        categories: billing, outage, program, service\n",
    "        \n",
    "        eg: \n",
    "        question:  can i pay my bill by creditcard\n",
    "        answer: billing\n",
    "        question: i need to cancel my service as i am moving\n",
    "        answer: service\n",
    "        question: when will by power come back on?\n",
    "        answer: outage\n",
    "        \n",
    "        \"\"\"\n",
    "cohere_chat_request.is_stream = False \n",
    "cohere_chat_request.max_tokens = 500\n",
    "cohere_chat_request.temperature = 0.75\n",
    "cohere_chat_request.top_p = 0.7\n",
    "cohere_chat_request.frequency_penalty = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up chat details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_detail = ChatDetails()\n",
    "chat_detail.serving_mode = OnDemandServingMode(model_id=LLM_MODEL)\n",
    "chat_detail.compartment_id = scfg[\"oci\"][\"compartment\"]\n",
    "chat_detail.chat_request = cohere_chat_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the LLM client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set up the LLM client \n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "                config=config,\n",
    "                service_endpoint=llm_service_endpoint,\n",
    "                retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "                timeout=(10,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask the question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_chat_request.message = \"why is my bill so high\"\n",
    "\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "llm_text = llm_response.data.chat_response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Chat Result**************************\n",
      "billing\n"
     ]
    }
   ],
   "source": [
    "print(\"**************************Chat Result**************************\")\n",
    "#llm_text = llm_response.data.chat_response.text\n",
    "print(llm_response.data.chat_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "\n",
    "Merge your RAG Agent & Single step tool into same code and use clasiification to decide which to invoke\n",
    "eg: \n",
    "1. If the queston is about weather, cll the single step tool\n",
    "2. if the call is about batteries use the RAG to answer\n",
    "3. if its about something else say you can only answer questions about weather or batteries\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
