{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 2: RAG - Full Pipeline Example\n",
    "\n",
    "### What this file does:\n",
    "Demonstrates a complete Retrieval-Augmented Generation (RAG) solution: Chunk, embed, and store data in Oracle DB. Retrieval: Embed query and perform similarity search. Augmentation: Add retrieved chunks to prompt. Generation: Get response from LLM with citations.\n",
    "\n",
    "**Documentation to reference:**\n",
    "- OCI GenAI Embeddings: https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm#embed-models\n",
    "- OCI GenAI Chat: https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm\n",
    "- Oracle DB Vectors: https://docs.oracle.com/en/database/oracle/oracle-database/23/vecse/\n",
    "- OCI Python SDK: https://github.com/oracle/oci-python-sdk/tree/master/src/oci/generative_ai_inference\n",
    "\n",
    "**Relevant slack channels:**\n",
    "- #generative-ai-users or #igiu-innovation-lab: *For questions*\n",
    "- #igiu-ai-learning: *For errors*\n",
    "\n",
    "**Env setup:**\n",
    "- sandbox.yaml: Needs \"oci\" (configFile, profile, compartment, bucket) and \"db\" (tablePrefix, walletPath, username, password, dsn, walletPass) sections.\n",
    "- .env: Load environment variables if needed.\n",
    "- Download wallet for DB access.\n",
    "- configure cwd for jupyter match your workspace python code: \n",
    "    -  vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "    -  change from `${fileDirname}` to `${workspaceFolder}`\n",
    "\n",
    "\n",
    "**How to run in notebook:**\n",
    "- Ensure dependencies installed (uv sync).\n",
    "- Update sandbox.yaml with DB details and wallet path.\n",
    "- Run cells in order. Requires Oracle DB access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import OnDemandServingMode, EmbedTextDetails, CohereChatRequest, ChatDetails\n",
    "import oracledb\n",
    "import array\n",
    "import oci\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "load_dotenv()\n",
    "EMBED_MODEL = \"cohere.embed-multilingual-v3.0\"\n",
    "LLM_MODEL = \"cohere.command-a-03-2025\"\n",
    "llm_service_endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock chunks (in practice, parse real documents)\n",
    "chunks = [\n",
    "    \"Baseball is a great game \",\n",
    "    \"baseball games typically last 9 innings\",\n",
    "    \"Baseball game can finish in about 2 hours\",\n",
    "    \"Indias favorite pastime is cricket\",\n",
    "    \"England's favorite pastime is football\",\n",
    "    \"Football is called soccer in America\",\n",
    "    \"baseball is americas favorite pastime sport\"\n",
    "]\n",
    "\n",
    "# Metadata for chunks\n",
    "chunk_source = [\n",
    "    {\"chapter\": \"Baseball\", \"question\": \"1\"},\n",
    "    {\"chapter\": \"Baseball\", \"question\": \"2\"},\n",
    "    {\"chapter\": \"Baseball\", \"question\": \"3\"},\n",
    "    {\"chapter\": \"Cricket\", \"question\": \"1\"},\n",
    "    {\"chapter\": \"Football\", \"question\": \"1\"},\n",
    "    {\"chapter\": \"Football\", \"question\": \"2\"},\n",
    "    {\"chapter\": \"Baseball\", \"question\": \"4\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load config and initialize OCI clients for embeddings and chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "scfg = EnvYAML(SANDBOX_CONFIG_FILE)\n",
    "if scfg is None or \"oci\" not in scfg or \"bucket\" not in scfg:\n",
    "    raise RuntimeError(\"Invalid sandbox configuration.\")\n",
    "\n",
    "config = oci.config.from_file(os.path.expanduser(scfg[\"oci\"][\"configFile\"]), scfg[\"oci\"][\"profile\"])\n",
    "compartmentId = scfg[\"oci\"][\"compartment\"]\n",
    "tablename_prefix = scfg[\"db\"][\"tablePrefix\"]\n",
    "wallet = os.path.expanduser(scfg[\"db\"][\"walletPath\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to DB\n",
    "db = oracledb.connect(\n",
    "    config_dir=scfg[\"db\"][\"walletPath\"],\n",
    "    user=scfg[\"db\"][\"username\"],\n",
    "    password=scfg[\"db\"][\"password\"],\n",
    "    dsn=scfg[\"db\"][\"dsn\"],\n",
    "    wallet_location=scfg[\"db\"][\"walletPath\"],\n",
    "    wallet_password=scfg[\"db\"][\"walletPass\"]\n",
    ")\n",
    "cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM client\n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "    config=config,\n",
    "    service_endpoint=llm_service_endpoint,\n",
    "    retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "    timeout=(10, 240)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Vector Table in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Oracle DB table to store text chunks, embeddings, and metadata.\n",
    "# Create table for embeddings\n",
    "sql = [\n",
    "    f\"\"\"drop table if exists {tablename_prefix}_embedding purge\"\"\",\n",
    "    f\"\"\"\n",
    "    create table {tablename_prefix}_embedding (\n",
    "        id number,\n",
    "        text varchar2(4000),\n",
    "        vec vector,\n",
    "        chapter varchar2(100),\n",
    "        section integer,\n",
    "        primary key (id)\n",
    "    )\"\"\"\n",
    "]\n",
    "\n",
    "for s in sql:\n",
    "    cursor.execute(s)\n",
    "\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate Embeddings and Store in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the text chunks and insert them into the vector table.\n",
    "# Build embedding payload\n",
    "embed_text_detail = EmbedTextDetails()\n",
    "embed_text_detail.serving_mode = OnDemandServingMode(model_id=EMBED_MODEL)\n",
    "embed_text_detail.truncate = embed_text_detail.TRUNCATE_END\n",
    "embed_text_detail.input_type = EmbedTextDetails.INPUT_TYPE_SEARCH_DOCUMENT\n",
    "embed_text_detail.compartment_id = compartmentId\n",
    "embed_text_detail.inputs = chunks\n",
    "\n",
    "# Generate embeddings\n",
    "response = llm_client.embed_text(embed_text_detail)\n",
    "embeddings = response.data.embeddings\n",
    "\n",
    "# Insert into DB\n",
    "for i in range(len(embeddings)):\n",
    "    cursor.execute(\n",
    "        f\"insert into {tablename_prefix}_embedding values (:1, :2, :3, :4, :5)\",\n",
    "        [i, chunks[i], array.array(\"f\", embeddings[i]), chunk_source[i][\"chapter\"], chunk_source[i][\"question\"]]\n",
    "    )\n",
    "    print(f\"inserted {i}-{chunks[i]}\")\n",
    "\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Verify Stored Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the contents of the vector table.\n",
    "# Query stored chunks\n",
    "cursor.execute(f\"select id,text from {tablename_prefix}_embedding\")\n",
    "for row in cursor:\n",
    "    print(f\"{row[0]}:{row[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Interactive Query and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed user query, retrieve similar chunks, augment prompt, and generate response.\n",
    "# User query\n",
    "query = input(\"Ask a question: \").strip().lower()\n",
    "q = [query]\n",
    "\n",
    "# Embed query\n",
    "embed_text_detail.inputs = q\n",
    "embed_text_detail.input_type = EmbedTextDetails.INPUT_TYPE_SEARCH_QUERY\n",
    "response = llm_client.embed_text(embed_text_detail)\n",
    "vec = array.array(\"f\", response.data.embeddings[0])\n",
    "\n",
    "# Similarity search (COSINE; try DOT or EUCLIDEAN)\n",
    "cursor.execute(\n",
    "    f\"\"\"\n",
    "    select id,text, vector_distance(vec, :1, COSINE) d, chapter,section\n",
    "    from {tablename_prefix}_embedding\n",
    "    order by d\n",
    "    fetch first 3 rows only\n",
    "    \"\"\",\n",
    "    [vec]\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for row in cursor:\n",
    "    r = [row[0], row[1], row[2], f\"chapter:[{row[3]}]_section:[{row[4]}]\"]\n",
    "    print(r)\n",
    "    rows.append(r)\n",
    "\n",
    "print(rows)\n",
    "\n",
    "# Optional: Rerank here (see cohere rerank example)\n",
    "\n",
    "# Augment prompt\n",
    "docs = []\n",
    "for chunk in rows:\n",
    "    doc = {\n",
    "        \"id\": chunk[3],\n",
    "        \"snippet\": chunk[1]\n",
    "    }\n",
    "    docs.append(doc)\n",
    "\n",
    "# Build chat request\n",
    "cohere_chat_request = CohereChatRequest()\n",
    "# cohere_chat_request.preamble_override = \"answer only from selected docs\"\n",
    "cohere_chat_request.is_stream = False\n",
    "cohere_chat_request.max_tokens = 500\n",
    "cohere_chat_request.temperature = 0.75\n",
    "cohere_chat_request.top_p = 0.7\n",
    "cohere_chat_request.frequency_penalty = 1.0\n",
    "cohere_chat_request.documents = docs\n",
    "\n",
    "chat_detail = ChatDetails()\n",
    "chat_detail.serving_mode = OnDemandServingMode(model_id=LLM_MODEL)\n",
    "chat_detail.compartment_id = compartmentId\n",
    "chat_detail.chat_request = cohere_chat_request\n",
    "\n",
    "# Generate response\n",
    "cohere_chat_request.message = query\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "\n",
    "# Print results\n",
    "print(\"************************** Chat Result **************************\")\n",
    "print(query)\n",
    "print(llm_response.data.chat_response.text)\n",
    "print(\"************************** Citations **************************\")\n",
    "print(llm_response.data.chat_response.citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close DB connections.\n",
    "# Close connections\n",
    "cursor.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation Section\n",
    "Good for experimentation:\n",
    "- Chunking: Replace mock chunks with parsed PDFs (use PyPDFLoader).\n",
    "- Similarity: Change to DOT_PRODUCT or EUCLIDEAN in vector_distance.\n",
    "- Retrieval: Adjust FETCH FIRST (e.g., top 5), add distance filter.\n",
    "- Augmentation: Add reranking before generating.\n",
    "- Generation: Tune temperature (0.3 factual vs. 1.0 creative).\n",
    "- Try different EMBED_MODEL or LLM_MODEL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "1. **Talk-to-Document**: Build an app to upload PDFs, chunk, embed, and query.\n",
    "2. **Text vs. PDF**: Compare using plain text chunks vs. PDF parsing.\n",
    "3. **Reranking**: Integrate Cohere rerank to reorder retrieved chunks.\n",
    "4. **Discussion**: What chunk size works best? How does similarity metric affect results?\n",
    "\n",
    "For help, reach out in #igiu-innovation-lab or #igiu-ai-learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
