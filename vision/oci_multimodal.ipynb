{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCI Multimodal Vision LLM Step-by-step\n",
    "\n",
    "### What this file does:\n",
    "Demonstrates multimodal (image+text) prompts with Oracle Cloud's Generative AI, using only the OCI Python SDK (no LangChain needed).\n",
    "\n",
    "**Documentation to reference:**\n",
    "- OCI Gen AI: https://docs.oracle.com/en-us/iaas/Content/generative-ai/home.htm\n",
    "- OCI Python SDK: https://github.com/oracle/oci-python-sdk/tree/master/src/oci/generative_ai_inference\n",
    "\n",
    "**Relevant slack channels:**\n",
    "- #generative-ai-users: *for questions on OCI Gen AI*\n",
    "- #igiu-innovation-lab: *general discussions on your project*\n",
    "- #igiu-ai-learning: *help with sandbox environment or help with running this code*\n",
    "\n",
    "**Env setup:**\n",
    "- sandbox.yaml: Contains OCI config and compartment.\n",
    "- .env: Load environment variables if needed.\n",
    "- configure cwd for jupyter match your workspace python code: \n",
    "    -  vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "    -  change from `${fileDirname}` to `${workspaceFolder}`\n",
    "\n",
    "\n",
    "**How to run in notebook:**\n",
    "- Make sure your runtime environment has all dependencies and access to required config files.\n",
    "- Run the notebook cells in order.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Requirements\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Environment Setup:** Before interacting with OCI services, you need to configure your environment with credentials, compartment IDs, and dependencies. This is typically done via a YAML config file (sandbox.yaml) and environment variables.\n",
    "- **Dependencies:** Install necessary libraries like the OCI SDK and configuration loaders.\n",
    "- **Libraries:** Use packages for config management, environment variables, and OCI interactions.\n",
    "\n",
    "In this step, we'll install dependencies and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML\n",
    "import oci\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported and environment loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load OCI Configuration\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Configuration Loading:** Securely load settings from a config file to authenticate and specify resources without hardcoding credentials.\n",
    "- **OCI Config:** Includes paths to config files, profiles, compartments, and regions.\n",
    "- **Error Handling:** Validate that the config is loaded correctly to avoid runtime errors.\n",
    "\n",
    "In this step, we'll load and validate the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and load configuration\n",
    "# Make sure your sandbox.yaml file is set up for your environment. You might have to specify the full path depending on your `cwd`.\n",
    "# You can also try making your cwd for jupyter match your workspace python code:\n",
    "# vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "# change from ${fileDirname} to ${workspaceFolder}\n",
    "\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "\n",
    "def load_config(config_path):\n",
    "    try:\n",
    "        return EnvYAML(config_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Configuration file '{config_path}' not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading config: {e}\")\n",
    "        return None\n",
    "\n",
    "scfg = load_config(SANDBOX_CONFIG_FILE)\n",
    "assert scfg is not None and 'oci' in scfg and 'configFile' in scfg['oci'] and 'profile' in scfg['oci'] and 'compartment' in scfg['oci'], \"Check your sandbox.yaml config!\"\n",
    "config = oci.config.from_file(os.path.expanduser(scfg[\"oci\"][\"configFile\"]), scfg[\"oci\"][\"profile\"])\n",
    "compartment_id = scfg[\"oci\"][\"compartment\"]\n",
    "\n",
    "print(\"Configuration loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model List & Endpoint\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Model Selection:** Choose from available OCI Gen AI models for multimodal tasks, including text and image understanding.\n",
    "- **Service Endpoint:** The regional endpoint for the Generative AI service, which handles inference requests.\n",
    "- **Model Variety:** Different models may perform better on certain tasks, so testing multiple is educational.\n",
    "\n",
    "In this step, we'll define the models to test and the service endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "MODEL_LIST = [\n",
    "    \"meta.llama-4-scout-17b-16e-instruct\",\n",
    "    \"openai.gpt-4.1\",\n",
    "    \"xai.grok-4\",\n",
    "]\n",
    "\n",
    "# Define the service endpoint (region-specific)\n",
    "llm_service_endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "print(\"Models and endpoint configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Select and Encode Your Image\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Image Selection:** Choose the image file to analyze. Ensure it's in a supported format (e.g., JPEG).\n",
    "- **Base64 Encoding:** Multimodal models require images to be encoded as base64 strings for transmission.\n",
    "- **Display:** Visualize the image to confirm it's the correct one before processing.\n",
    "\n",
    "In this step, we'll select the image, display it, and encode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import display library\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Select the image\n",
    "IMAGE_PATH = 'vision/dussera-b.jpg'   # Change if you use a different image!\n",
    "\n",
    "print(f\"Selected image: {IMAGE_PATH}\")\n",
    "\n",
    "# Display the image\n",
    "display(Image(filename=IMAGE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode image to base64\n",
    "def encode_image(path):\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Encode the image\n",
    "IMAGE_B64 = encode_image(IMAGE_PATH)\n",
    "\n",
    "print(\"Image encoded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build the Multimodal User Message\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Multimodal Input:** Combine text and image in a single message for the LLM.\n",
    "- **Message Structure:** Use OCI SDK classes to create text and image content objects.\n",
    "- **User Message:** Encapsulate the content in a UserMessage for the chat API.\n",
    "\n",
    "In this step, we'll construct the message with text and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user text prompt\n",
    "USER_TEXT = \"Tell me about this image\"  # You can change this!\n",
    "\n",
    "# Function to build user message\n",
    "def build_user_message(img_b64, text):\n",
    "    content1 = oci.generative_ai_inference.models.TextContent()\n",
    "    content1.text = text\n",
    "    content2 = oci.generative_ai_inference.models.ImageContent()\n",
    "    image_url = oci.generative_ai_inference.models.ImageUrl()\n",
    "    image_url.url = f\"data:image/jpeg;base64,{img_b64}\"\n",
    "    content2.image_url = image_url\n",
    "    message = oci.generative_ai_inference.models.UserMessage()\n",
    "    message.content = [content1, content2]\n",
    "    return message\n",
    "\n",
    "print(\"User message structure defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Chat Request Utilities\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Request Parameters:** Configure the chat request with options like number of generations, streaming, token limits, and temperature.\n",
    "- **Chat Details:** Wrap the request with serving mode, compartment, and model details for the API call.\n",
    "- **API Format:** Use the generic format for multimodal support.\n",
    "\n",
    "In this step, we'll define utilities for building the chat request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create chat request\n",
    "def get_chat_request(message):\n",
    "    chat_request = oci.generative_ai_inference.models.GenericChatRequest()\n",
    "    chat_request.messages = [message]\n",
    "    chat_request.api_format = oci.generative_ai_inference.models.BaseChatRequest.API_FORMAT_GENERIC\n",
    "    chat_request.num_generations = 1\n",
    "    chat_request.is_stream = False\n",
    "    chat_request.max_tokens = 500\n",
    "    chat_request.temperature = 0.75\n",
    "    return chat_request\n",
    "\n",
    "# Function to create chat details\n",
    "def get_chat_detail(llm_request, compartment_id, model_id):\n",
    "    chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "    chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id)\n",
    "    chat_detail.compartment_id = compartment_id\n",
    "    chat_detail.chat_request = llm_request\n",
    "    return chat_detail\n",
    "\n",
    "print(\"Chat request utilities defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Initialize the LLM Client\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Client Initialization:** Create a client instance with config, endpoint, and retry/timeout settings.\n",
    "- **Generative AI Inference:** This client handles multimodal requests to OCI's Gen AI service.\n",
    "- **Timeouts:** Set appropriate timeouts for inference, especially for image processing.\n",
    "\n",
    "In this step, we'll set up the client for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Generative AI Inference client\n",
    "llm_client = oci.generative_ai_inference.GenerativeAiInferenceClient(\n",
    "    config=config,\n",
    "    service_endpoint=llm_service_endpoint,\n",
    "    retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "    timeout=(10, 240)\n",
    ")\n",
    "\n",
    "print(\"LLM client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Run Inference ‚Äì Compare Results!\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Loop Through Models:** Test the multimodal prompt across different models to compare responses.\n",
    "- **Inference Process:** Send the message, receive the response, and extract the text content.\n",
    "- **Performance Timing:** Measure response time for each model to understand efficiency.\n",
    "\n",
    "In this step, we'll run the inference for each model and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through models and run inference\n",
    "for model_id in MODEL_LIST:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"RESULTS FOR MODEL: {model_id}\\n\" + \"=\"*80)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Build the message\n",
    "    user_msg = build_user_message(IMAGE_B64, USER_TEXT)\n",
    "    \n",
    "    # Prepare the request\n",
    "    llm_payload = get_chat_request(user_msg)\n",
    "    chat_detail = get_chat_detail(llm_payload, compartment_id, model_id)\n",
    "    \n",
    "    # Send the request\n",
    "    llm_response = llm_client.chat(chat_detail)\n",
    "    \n",
    "    # Process the response\n",
    "    if (llm_response is not None and hasattr(llm_response, 'data') and hasattr(llm_response.data, 'chat_response') and llm_response.data.chat_response is not None and hasattr(llm_response.data.chat_response, 'choices') and llm_response.data.chat_response.choices):\n",
    "        llm_text = llm_response.data.chat_response.choices[0].message.content[0].text\n",
    "        print(llm_text)\n",
    "    else:\n",
    "        print(\"Error: Invalid response from LLM.\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTime taken: {end_time - start_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play and Explore\n",
    "\n",
    "- Change `USER_TEXT` to ask any question about your picture.\n",
    "- Swap in a different image.\n",
    "- Compare models easily!\n",
    "\n",
    "## üßë‚Äçüíª Project Ideas for Practice\n",
    "\n",
    "Below are some fun project prompts. Try one (or all) after you run a basic image through the models!\n",
    "\n",
    "1. **Business Card ‚Üí vCard**\n",
    "   - Upload a photo of a business card.\n",
    "   - Write a prompt such as: \"Extract all the contact details from this business card and output as a vCard file.\"\n",
    "   - Post-process the model's output to save/download the .vcf file.\n",
    "\n",
    "2. **Agenda/Schedule ‚Üí Calendar File**\n",
    "   - Try an image of a handwritten or printed agenda.\n",
    "   - Prompt: \"Read and convert this agenda into an iCalendar (.ics) file.\"\n",
    "   - Save and import to your calendar app!\n",
    "\n",
    "3. **Driver's License ‚Üí CRM/Create Record**\n",
    "   - Upload an image of a driver's license (redact sensitive fields if needed).\n",
    "   - Prompt: \"Extract all key customer information for a CRM record.\"\n",
    "   - Map the LLM's result into your database or spreadsheet.\n",
    "\n",
    "If you see errors, double-check credentials or configurations. Refer to comments or docs for help.\n",
    "\n",
    "---\n",
    "**Happy building!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
