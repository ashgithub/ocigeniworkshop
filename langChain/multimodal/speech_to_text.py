"""
What this file does:
Transcribes audio clips into text using OCI-hosted GPT-Audio models. Downloads an example WAV file, encodes it to base64, and sends it with a text instruction to extract a transcript.

Documentation to reference:
- Open AI Audio Support: https://developers.openai.com/api/docs/guides/audio
- OCI OpenAI compatible SDK: https://github.com/oracle-samples/oci-openai
- note oci doesnt support audio -> audio just yet (as of feb 2026)

Relevant slack channels:
- #generative-ai-users: for questions on OCI Gen AI
- #igiu-innovation-lab: general discussions on your project
- #igiu-ai-learning: help with sandbox environment or help with running this code

Env setup:
- sandbox.yaml: Contains OCI config, compartment, and profile details.
- .env: Set overrides like `TRANSCRIBE_URL` if you want to point to your own audio sample.

How to run the file:
uv run langChain/multimodal/speech_to_text.py

Comments to important sections of file:
- Step 1: Load config and initialize the OCI OpenAI chat client.
- Step 2: Fetch (or reuse) the audio sample and encode it to base64.
- Step 3: Call GPT-Audio with a transcription instruction and audio payload.
- Step 4: Print the transcript and any relevant usage metadata.
"""

import base64
import os,sys
from pathlib import Path
from typing import Tuple

import requests
from dotenv import load_dotenv
from envyaml import EnvYAML

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from oci_openai_helper import OCIOpenAIHelper

# generated by codex AI - model gpt-4.1

SANDBOX_CONFIG_FILE = "sandbox.yaml"
DEFAULT_MODEL = "openai.gpt-audio"
DEFAULT_AUDIO_URL = "https://cdn.openai.com/API/docs/audio/alloy.wav"
DEFAULT_PROMPT = "What is in this recording?"
DEFAULT_CACHE_PATH = Path("scratch/reference_audio.wav")


# Step 1: Load config and initialize clients
def load_config(config_path: str) -> EnvYAML:
    try:
        return EnvYAML(config_path)
    except FileNotFoundError as exc:
        raise SystemExit(f"Missing configuration file: {config_path}") from exc


def get_openai_client(config: EnvYAML):
    return OCIOpenAIHelper.get_sync_openai_client(config=config)


# Step 2: Fetch audio and convert to base64
def get_audio_settings() -> Tuple[str, str, Path]:
    model = os.getenv("TRANSCRIBE_MODEL", DEFAULT_MODEL)
    audio_url = os.getenv("TRANSCRIBE_URL", DEFAULT_AUDIO_URL)
    cache_path = Path(os.getenv("TRANSCRIBE_CACHE", str(DEFAULT_CACHE_PATH)))
    return model, audio_url, cache_path


def download_audio(audio_url: str, cache_path: Path) -> bytes:
    cache_path.parent.mkdir(parents=True, exist_ok=True)
    if cache_path.exists():
        return cache_path.read_bytes()

    response = requests.get(audio_url, timeout=30)
    response.raise_for_status()
    cache_path.write_bytes(response.content)
    return response.content


def encode_audio(audio_bytes: bytes) -> str:
    return base64.b64encode(audio_bytes).decode("utf-8")


# Step 3: Call GPT-Audio for transcription
def transcribe_audio(client, model: str, prompt: str, audio_b64: str, audio_format: str = "wav"):
    completion = client.chat.completions.create(
        model=model,
        modalities=["text", "audio"],
        audio={"voice": "alloy", "format": audio_format},
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "input_audio",
                        "input_audio": {"data": audio_b64, "format": audio_format},
                    },
                ],
            }
        ],
    )
    choice = completion.choices[0]
    transcript = choice.message.audio.transcript
    usage = completion.usage.to_dict() if completion.usage else {}
    return transcript, usage


# Step 4: Print transcript and metadata
def main() -> None:
    load_dotenv()
    config = load_config(SANDBOX_CONFIG_FILE)
    client = get_openai_client(config)
    model, audio_url, cache_path = get_audio_settings()
    prompt = os.getenv("TRANSCRIBE_PROMPT", DEFAULT_PROMPT)

    audio_bytes = download_audio(audio_url, cache_path)
    audio_b64 = encode_audio(audio_bytes)

    print(f"Transcribing audio from {audio_url}")
    transcript, usage = transcribe_audio(client, model, prompt, audio_b64)

    print("\nTranscript:\n----------")
    print(transcript)
    print("\nToken usage:", usage)
    print("Experiment: set TRANSCRIBE_URL to your own clip to customize.")


if __name__ == "__main__":
    main()
