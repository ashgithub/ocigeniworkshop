""" Sample file to show a semantic search example using Oracle 23ai DB """
import os
import json
import array
import oracledb
import oci
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_oci.embeddings import OCIGenAIEmbeddings

# Reference: https://docs.langchain.com/oss/python/langchain/knowledge-base#build-a-semantic-search-engine-with-langchain

SANDBOX_CONFIG_FILE = "C:/Users/Cristopher Hdz/Desktop/ocigeniworkshop/sandbox.json"

EMBED_MODEL = "cohere.embed-v4.0"
# Availble embedding models https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm#embed-models

# cohere.embed-v4.0
# cohere.embed-multilingual-v3.0
# cohere.embed-multilingual-light-v3.0
# cohere.embed-english-v3.0
# cohere.embed-english-light-v3.0

LLM_ENDPOINT = "https://inference.generativeai.us-chicago-1.oci.oraclecloud.com"

# Step 1: load all the config files
def load_config(config_path):
    """Load configuration from sandbox.json."""
    try:
        with open(config_path, "r") as f:
            return json.load(f)
    except Exception as e:
        raise RuntimeError(f"Failed to load config: {e}")

scfg = load_config(SANDBOX_CONFIG_FILE)

config = oci.config.from_file(
    os.path.expanduser(scfg["oci"]["configFile"]),
    scfg["oci"]["profile"]
)
compartment_id = scfg["oci"]["compartment"]
compartment_id = scfg["oci"]["compartment"]
table_prefix = scfg["db"]["tablePrefix"]
wallet_path = os.path.expanduser(scfg["db"]["walletPath"])

""" Load document and chunk, details on langchain_chunks.py """

pdf_path = "Sample1.pdf"
loader = PyPDFLoader(pdf_path)
docs = loader.load()
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=200,
    add_start_index=True
)
splits = text_splitter.split_documents(docs)
print(f"Created {len(splits)} text chunks for embedding.")

""" Start the embedding phase, details on langchain_embedding.py"""

llm_embed_client = OCIGenAIEmbeddings(
    model_id=EMBED_MODEL,
    service_endpoint=LLM_ENDPOINT,
    compartment_id=compartment_id
)
texts = [chunk.page_content for chunk in splits]
embeddings = llm_embed_client.embed_documents(texts)

print(f"Generated {len(embeddings)} embeddings.")

# Step 2: open a DB connection to store the vectors
# Connect to Oracle DB to store vectors as knowledge base
db_connection = oracledb.connect(
    config_dir=wallet_path,
    user=scfg["db"]["username"],
    password=scfg["db"]["password"],
    dsn=scfg["db"]["dsn"],
    wallet_location=wallet_path,
    wallet_password=scfg["db"]["walletPass"]
)
cursor = db_connection.cursor()

# Step3: Build the vector table
def create_table():
    """Drop and create embedding table."""
    print("Creating table for embeddings...")

    # Use the prefix to avoid usage of the same table per user
    sql_statements = [
        f"DROP TABLE {table_prefix}_embedding PURGE",
        f"""
        CREATE TABLE {table_prefix}_embedding (
            id NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
            text VARCHAR2(4000),
            vec VECTOR,
            source VARCHAR2(100)
        )
        """
    ]

    for stmt in sql_statements:
        try:
            cursor.execute(stmt)
        except Exception as e:
            # Ignore if table doesn't exist and create a new one
            print(f"Skipping error: {e}")

create_table()

# Step 4: Proceed to insert the generated embedding vectors on DB
for i, emb in enumerate(embeddings):
    chunk_text = texts[i][:3900]  # ensure within VARCHAR2(4000) limit according to table constrain
    metadata_source = splits[i].metadata.get("source", "pdf-doc") # Originally from splitter

    cursor.execute(
        f"INSERT INTO {table_prefix}_embedding (text, vec, source) VALUES (:1, :2, :3)",
        [chunk_text, array.array("f", emb), metadata_source],
    )

db_connection.commit()

# Semantic search steps
# Step 5: declare semantic search function
def semantic_search(query, top_k=3):
    """Perform semantic search over Oracle DB using cosine similarity."""

    # Embed the query
    query_embedding = llm_embed_client.embed_query(query)
    query_vec = array.array("f", query_embedding)

    # Run vector similarity search in Oracle DB
    cursor.execute(f"""
        SELECT text, vector_distance(vec, :1, COSINE) AS distance, source
        FROM {table_prefix}_embedding
        ORDER BY distance
        FETCH FIRST {top_k} ROWS ONLY
    """, [query_vec])

    results = cursor.fetchall()
    print("\nTop Results:")
    for i, (text, dist, source) in enumerate(results, 1):
        print(f"--- Result {i} ---")
        print(f"Distance: {dist:.4f}")
        print(f"Source: {source}")
        print(f"Snippet: {text[:200]}...\n")

# Step 6: give the query to search
# Try different search queries about communications in 20th century
while True:
    q = input("\nAsk a semantic search query (or 'q' to quit): ").strip()
    if q.lower() == "q":
        break

    # Change the top_k parameter to retrieve the closest k results
    semantic_search(q,top_k=5)

# Step 7: Close always DB connections
cursor.close()
db_connection.close()
print("DB session closed")