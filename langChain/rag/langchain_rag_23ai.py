""" Sample file with full RAG example """
import os
import json
import array
import oracledb
import oci
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_oci.embeddings import OCIGenAIEmbeddings
from langchain_oci.chat_models import ChatOCIGenAI
from langchain_core.messages import HumanMessage

# Reference: https://docs.langchain.com/oss/python/langchain/rag#returning-source-documents

SANDBOX_CONFIG_FILE = "C:/Users/Cristopher Hdz/Desktop/ocigeniworkshop/sandbox.json"

EMBED_MODEL = "cohere.embed-v4.0"
# Availble embedding models https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm#embed-models

# cohere.embed-multilingual-v3.0
# cohere.embed-v4.0
# cohere.embed-english-v3.0
# cohere.embed-english-light-v3.0
# cohere.embed-multilingual-light-v3.0

LLM_MODEL = "openai.gpt-4.1"
#LLM_MODEL = "cohere.command-r-08-2024"

# available models : https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm
# All available chat models on OCI as of October 2025:
# Cohere:
#   cohere.command-a-03-2025
#   cohere.command-r-08-2024
#   cohere.command-r-plus-08-2024
# Meta:
#   meta.llama-4-maverick
#   meta.llama-4-scout
#   meta.llama-3-3-70b
#   meta.llama-3-2-90b-vision
#   meta.llama-3-2-11b-vision
#   meta.llama-3-1-405b
#   meta.llama-3-1-70b
#   meta.llama-3-70b
# OpenAI (Beta):
#   openai.gpt-oss-120b
#   openai.gpt-oss-20b
# xAI:
#   xai.grok-code-fast-1
#   xai.grok-4-fast
#   xai.grok-4
#   xai.grok-3
#   xai.grok-3-mini
#   xai.grok-3-fast
#   xai.grok-3-mini-fast

LLM_ENDPOINT = "https://inference.generativeai.us-chicago-1.oci.oraclecloud.com"

# Step 1: load all the config files
def load_config(config_path):
    """Load configuration from sandbox.json."""
    try:
        with open(config_path, "r") as f:
            return json.load(f)
    except Exception as e:
        raise RuntimeError(f"Failed to load config: {e}")

scfg = load_config(SANDBOX_CONFIG_FILE)

config = oci.config.from_file(
    os.path.expanduser(scfg["oci"]["configFile"]),
    scfg["oci"]["profile"]
)
compartment_id = scfg["oci"]["compartment"]
compartment_id = scfg["oci"]["compartment"]
table_prefix = scfg["db"]["tablePrefix"]
wallet_path = os.path.expanduser(scfg["db"]["walletPath"])

""" Load document and chunk, details on langchain_chunks.py """

pdf_path = "Sample1.pdf"
loader = PyPDFLoader(pdf_path)
docs = loader.load()
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=200,
    add_start_index=True
)
splits = text_splitter.split_documents(docs)

print(f"Created {len(splits)} text chunks for embedding.")

""" Start the embedding phase, details on langchain_embedding.py"""

llm_embed_client = OCIGenAIEmbeddings(
    model_id=EMBED_MODEL,
    service_endpoint=LLM_ENDPOINT,
    compartment_id=compartment_id
)
texts = [chunk.page_content for chunk in splits]
embeddings = llm_embed_client.embed_documents(texts)

print(f"Generated {len(embeddings)} embeddings.")

# Step 2: declare the client to answer user queries
llm_chat_client = ChatOCIGenAI(
    model_id= LLM_MODEL,
    service_endpoint= LLM_ENDPOINT,
    compartment_id= scfg['oci']['compartment'],
    auth_file_location= scfg["oci"]["configFile"],
    auth_profile= scfg["oci"]["profile"],
)

# Step 3: connect to Oracle DB to store vectors as knowledge base
db_connection = oracledb.connect(
    config_dir=wallet_path,
    user=scfg["db"]["username"],
    password=scfg["db"]["password"],
    dsn=scfg["db"]["dsn"],
    wallet_location=wallet_path,
    wallet_password=scfg["db"]["walletPass"]
)
cursor = db_connection.cursor()

# Step 4: Build the vector table
def create_table():
    """Drop and create embedding table."""
    print("Creating table for embeddings...")

    # Use the prefix to avoid usage of the same table per user
    sql_statements = [
        f"DROP TABLE {table_prefix}_embedding PURGE",
        f"""
        CREATE TABLE {table_prefix}_embedding (
            id NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
            text VARCHAR2(4000),
            vec VECTOR,
            source VARCHAR2(100)
        )
        """
    ]

    for stmt in sql_statements:
        try:
            cursor.execute(stmt)
        except Exception as e:
            # Ignore if table doesn't exist and create a new one
            print(f"Skipping error: {e}")

create_table()

# Step 5: Proceed to insert the generated embedding vectors on DB
for i, emb in enumerate(embeddings):
    chunk_text = texts[i][:3900]  # ensure within VARCHAR2(4000) limit according to table constrain
    metadata_source = splits[i].metadata.get("source", "pdf-doc") # Originally from splitter

    cursor.execute(
        f"INSERT INTO {table_prefix}_embedding (text, vec, source) VALUES (:1, :2, :3)",
        [chunk_text, array.array("f", emb), metadata_source],
    )

db_connection.commit()

# Step 6: provide semantic search function
def semantic_search(query, top_k=3):
    """Perform semantic search with cosine similarity."""
    query_emb = llm_embed_client.embed_query(query)
    query_vec = array.array("f", query_emb)

    cursor.execute(f"""
        SELECT text, vector_distance(vec, :1, COSINE) AS distance, source
        FROM {table_prefix}_embedding
        ORDER BY distance
        FETCH FIRST {top_k} ROWS ONLY
    """, [query_vec])

    rows = cursor.fetchall()
    return [{"text": r[0], "distance": r[1], "source": r[2]} for r in rows]

# Step 7: build a helper function to gather the search results
def build_context_snippet(results):
    """Format retrieved chunks for prompt context."""
    context_parts = []
    for i, r in enumerate(results, 1):
        snippet = r["text"].replace("\n", " ")
        context_parts.append(f"[{i}] (Source: {r['source']}) {snippet}")
    return "\n\n".join(context_parts)

# Step 8: RAG workflow calling the helper functions
def rag_answer(query):
    """Perform RAG"""
    # R: retrieve the results from knowledge base
    results = semantic_search(query, top_k=5)

    # A: augment the prompt. Build a compact context to call the agent
    context = build_context_snippet(results)
    context_prompt = (
        f"You are an assistant answering only from the provided document excerpts.\n"
        f"Use them faithfully and cite their source in square brackets.\n\n"
        f"--- DOCUMENT EXCERPTS ---\n{context}\n\n"
        f"--- QUESTION ---\n{query}\n\n"
    )

    print(f"-----------> Calling the agent with augmented context:\n{context_prompt}")

    # G: generate the response from chat model given the full context
    response = llm_chat_client.invoke([HumanMessage(content=context_prompt)])
    print("\n************************** MODEL ANSWER **************************")
    print(response)
    print("\n************************** CITATIONS **************************")
    for i, r in enumerate(results, 1):
        print(f"[{i}] Source: {r['source']} (distance={r['distance']:.4f})")
    print("--------------------------------------------------------------")

# Loop to call the agent
while True:
    q = input("\nAsk a question (or 'q' to exit): ").strip()
    if q.lower() == "q":
        break
    rag_answer(q)

# Step 9: close always DB connections
cursor.close()
db_connection.close()
print("DB session closed")