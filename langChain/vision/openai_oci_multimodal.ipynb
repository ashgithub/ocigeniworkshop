{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47509cd1",
   "metadata": {},
   "source": [
    "# OCI + LangChain Multimodal LLM Walkthrough\n",
    "\n",
    "**What this notebook does:**\n",
    "Demonstrates multimodal large language model (LLM) capabilities using OCI Generative AI. You'll learn how to analyze images by combining text prompts with visual data, comparing responses from different models.\n",
    "\n",
    "**Documentation to reference:**\n",
    "- OCI Gen AI: https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm\n",
    "- OCI OpenAI compatible SDK: https://github.com/oracle-samples/oci-openai\n",
    "- LangChain: https://docs.langchain.com/oss/python/langchain/overview\n",
    "\n",
    "**Relevant slack channels:**\n",
    "- #generative-ai-users: *for questions on OCI Gen AI*\n",
    "- #igiu-innovation-lab: *general discussions on your project*\n",
    "- #igiu-ai-learning: *help with sandbox environment or help with running this code*\n",
    "\n",
    "**How to run:**\n",
    "Execute cells sequentially. Requires sandbox.yaml configuration.\n",
    "\n",
    "**Steps covered:**\n",
    "1. Prepare the environment & configuration\n",
    "2. Load and inspect the image you want the model to see\n",
    "3. Build the prompt payload (text + image)\n",
    "4. Send it to different OCI-hosted models via LangChain\n",
    "5. Compare the responses and timings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f7c9e",
   "metadata": {},
   "source": [
    "## 1  Environment setup\n",
    "**Requirements** (already in the repo):\n",
    "\n",
    "- Create or edit `sandbox.yaml`  and `.env` with your OCI denv details like `profile` and `compartment`.\n",
    "- configure cwd for jupyter match your workspace python code: \n",
    "    -  vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "    -  change from `${fileDirname}` to `${workspaceFolder}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b0f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, base64, time, pathlib\n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML\n",
    "\n",
    "from langChain.oci_openai_helper import OCIOpenAIHelper\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ea1e3",
   "metadata": {},
   "source": [
    "## 2  Load sandbox configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SANDBOX_CONFIG_FILE = 'sandbox.yaml'\n",
    "\n",
    "def load_config(path):\n",
    "    try:\n",
    "        return EnvYAML(path)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"‚ùå '{path}' not found. Create it from sandbox.yaml.template.\")\n",
    "\n",
    "cfg = load_config(SANDBOX_CONFIG_FILE)\n",
    "compartment_id = cfg['oci']['compartment']\n",
    "profile        = cfg['oci']['profile']\n",
    "print('Profile     :', profile)\n",
    "print('Compartment :', compartment_id[:6] + '‚Ä¶')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f98dad",
   "metadata": {},
   "source": [
    "## 3  Select models & service endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bffd428",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIST = [\n",
    "    'meta.llama-4-scout-17b-16e-instruct',\n",
    "    'openai.gpt-4.1',\n",
    "    'xai.grok-4'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd510c5",
   "metadata": {},
   "source": [
    "## 4  Load & visualize the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9be478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "IMAGE_PATH = 'vision/dussera-b.jpg'  # adjust if needed\n",
    "display(Image(filename=IMAGE_PATH))\n",
    "\n",
    "def encode_image(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "IMAGE_B64 = encode_image(IMAGE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1acc0eb",
   "metadata": {},
   "source": [
    "## 5  Craft the prompt (text + image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab6289",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_TEXT = 'Tell me about this image'\n",
    "\n",
    "def make_prompt(img_b64, text):\n",
    "    return [{\n",
    "        'role': 'user',\n",
    "        'content': [\n",
    "            {'type': 'text', 'text': text},\n",
    "            {\n",
    "                'type': 'image_url',\n",
    "                'image_url': {\n",
    "                    'url': f'data:image/jpeg;base64,{img_b64}'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04fec30",
   "metadata": {},
   "source": [
    "## 6  Query each model & measure latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODEL_LIST:\n",
    "    print('\\n' + '='*80)\n",
    "    print('Model:', model)\n",
    "    llm_client = OCIOpenAIHelper.get_client(\n",
    "        model_name=model,\n",
    "        config=cfg\n",
    "        )\n",
    "\n",
    "    start = time.time()\n",
    "    response = llm_client.invoke(make_prompt(IMAGE_B64, USER_TEXT))\n",
    "    print('Answer:', response.content)\n",
    "    print('‚è± {:.2f}s'.format(time.time() - start))\n",
    "print('\\nDone!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ee4a50",
   "metadata": {},
   "source": [
    "## 7. Experiment and Explore\n",
    "\n",
    "**Safe ways to experiment:**\n",
    "- Change `USER_TEXT` to ask different questions: \"What colors do you see?\", \"Describe the emotions in this scene\", \"Count the number of people\"\n",
    "- Try different `IMAGE_PATH` values, e.g., './langChain/vision/receipt.png' or './langChain/vision/people_walking.mp4' (for video analysis)\n",
    "- Add more models to `MODEL_LIST` or remove some to compare fewer\n",
    "- Modify the prompt structure in `make_prompt()` function\n",
    "\n",
    "## üßë‚Äçüíª Practice Exercises and Discussion Prompts\n",
    "\n",
    "**Beginner Exercises:**\n",
    "1. **Image Description Variations** - Change the prompt to focus on different aspects (colors, objects, emotions)\n",
    "2. **Model Comparison** - Run the same image through all models and note differences in responses\n",
    "3. **Timing Analysis** - Which model is fastest? Why might that be?\n",
    "\n",
    "**Intermediate Projects:**\n",
    "1. **Business Card ‚Üí vCard**\n",
    "   - Upload a photo of a business card\n",
    "   - Prompt: \"Extract all contact details and format as vCard\"\n",
    "   - Save the output as a .vcf file\n",
    "\n",
    "2. **Document Analysis**\n",
    "   - Use receipt.png or other documents\n",
    "   - Extract structured data (prices, dates, items)\n",
    "\n",
    "3. **Video Frame Analysis**\n",
    "   - Try with video files like people_walking.mp4\n",
    "   - Ask about activities, objects, or scene descriptions\n",
    "\n",
    "**Discussion Questions:**\n",
    "- How do different models interpret the same image?\n",
    "- What are the trade-offs between speed and quality?\n",
    "- How might you use this for real-world applications?\n",
    "\n",
    "**Next Steps:**\n",
    "- Check out the corresponding Python script for a non-interactive version\n",
    "- Explore other vision capabilities in the OCI documentation\n",
    "\n",
    "If you encounter errors, verify your sandbox.yaml configuration and API access.\n",
    "\n",
    "---\n",
    "**Happy experimenting!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
