{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP (Model Context Protocol) Tool Integration\n",
    "\n",
    "This notebook demonstrates how to integrate MCP servers with LangChain agents using OCI Generative AI.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to connect to MCP servers\n",
    "- Manual tool calling with external services\n",
    "- Agent execution with MCP tools\n",
    "\n",
    "## Prerequisites\n",
    "- `sandbox.yaml` configured with OCI credentials\n",
    "- MCP servers running (weather server on port 8000)\n",
    "\n",
    "## Documentation\n",
    "- [MCP Specification](https://modelcontextprotocol.io/specification)\n",
    "- [FastMCP](https://gofastmcp.com/getting-started/welcome)\n",
    "- [LangChain MCP](https://docs.langchain.com/oss/python/langchain/mcp)\n",
    "- [OCI Gen AI](https://docs.oracle.com/en-us/iaas/Content/generative-ai/home.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "First, we need to import all the necessary libraries for our MCP integration.\n",
    "\n",
    "**What this does:**\n",
    "- Imports MCP client, LangChain components, and OCI helper\n",
    "- Sets up the Python path to access our helper modules\n",
    "\n",
    "**Why this matters:**\n",
    "Proper imports ensure all components work together seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "from langChain.oci_openai_helper import OCIOpenAIHelper\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Configuration Files\n",
    "\n",
    "Load the configuration files needed for OCI and environment setup.\n",
    "\n",
    "**What this does:**\n",
    "- Loads environment variables from `.env`\n",
    "- Sets up the sandbox configuration file path\n",
    "- Defines the LLM model to use\n",
    "\n",
    "**Why this matters:**\n",
    "Configuration files contain credentials and settings needed for OCI API access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration file paths and model settings\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "LLM_MODEL = \"openai.gpt-4.1\"\n",
    "\n",
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from a YAML file.\"\"\"\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            return EnvYAML(config_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Configuration file '{config_path}' not found.\")\n",
    "        return None\n",
    "\n",
    "print(f\"Configuration loaded. Using model: {LLM_MODEL}\")\n",
    "print(f\"Sandbox config file: {SANDBOX_CONFIG_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize OCI Client\n",
    "\n",
    "Create the OCI Generative AI client that will power our LLM.\n",
    "\n",
    "**What this does:**\n",
    "- Loads the sandbox configuration\n",
    "- Initializes the OCI OpenAI-compatible client\n",
    "- Tests the connection\n",
    "\n",
    "**Why this matters:**\n",
    "This client handles all communication with OCI's AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\nscfg = load_config(SANDBOX_CONFIG_FILE)\n\nif scfg is None:\n    print(\"ERROR: Could not load sandbox configuration!\")\n    print(\"Make sure sandbox.yaml exists and is properly configured.\")\nelse:\n    # Initialize OCI client\n    llm_client = OCIOpenAIHelper.get_langchain_openai_client(\n        model_name=LLM_MODEL,\n        config=scfg\n    )\n    \n    print(\"\u2705 OCI client initialized successfully!\")\n    print(f\"Model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define MCP Server Configuration\n",
    "\n",
    "Set up the configuration for connecting to MCP servers.\n",
    "\n",
    "**What this does:**\n",
    "- Defines server endpoints and transport methods\n",
    "- Weather server uses HTTP transport\n",
    "- Bill server uses stdio (local process) transport\n",
    "\n",
    "**Why this matters:**\n",
    "MCP supports different transport protocols for various deployment scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP server configuration\n",
    "MCP_SERVERS = {\n",
    "    \"weather\": {\n",
    "        \"transport\": \"streamable_http\",\n",
    "        \"url\": \"http://localhost:8000/mcp\",\n",
    "    },\n",
    "    \"bill_server\": {\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"./langChain/function_calling/mcp/bill_mcp_server.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"MCP server configuration defined:\")\n",
    "for server_name, config in MCP_SERVERS.items():\n",
    "    transport = config.get(\"transport\")\n",
    "    if transport == \"streamable_http\":\n",
    "        print(f\"- {server_name}: HTTP server at {config.get('url')}\")\n",
    "    elif transport == \"stdio\":\n",
    "        print(f\"- {server_name}: Local process via stdio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create MCP Client Function\n",
    "\n",
    "Define a function to initialize the MCP client and connect to servers.\n",
    "\n",
    "**What this does:**\n",
    "- Creates MultiServerMCPClient instance\n",
    "- Connects to all configured servers\n",
    "- Retrieves available tools from servers\n",
    "\n",
    "**Why this matters:**\n",
    "This function handles the complex setup of MCP connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_mcp_client():\n",
    "    \"\"\"\n",
    "    Create and initialize MCP client with all configured servers.\n",
    "    \n",
    "    Returns:\n",
    "        client: Initialized MCP client\n",
    "    \"\"\"\n",
    "    print(\"Creating MCP client...\")\n",
    "    \n",
    "    client = MultiServerMCPClient(MCP_SERVERS)\n",
    "    \n",
    "    print(\"Connecting to MCP servers...\")\n",
    "    # The client will connect when we call get_tools()\n",
    "    \n",
    "    return client\n",
    "\n",
    "# Create the MCP client\n",
    "import asyncio\n",
    "mcp_client = await create_mcp_client()\n",
    "print(\"\u2705 MCP client created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Discover Available Tools\n",
    "\n",
    "Connect to MCP servers and discover what tools are available.\n",
    "\n",
    "**What this does:**\n",
    "- Actually connects to the servers\n",
    "- Retrieves tool definitions from each server\n",
    "- Lists all available tools with descriptions\n",
    "\n",
    "**Why this matters:**\n",
    "Before using tools, we need to know what they do and how to call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available tools from MCP servers\n",
    "tools = await mcp_client.get_tools()\n",
    "\n",
    "print(f\"\u2705 Connected to MCP servers! Found {len(tools)} tools:\")\n",
    "print()\n",
    "\n",
    "for i, tool in enumerate(tools, 1):\n",
    "    print(f\"{i}. {tool.name}\")\n",
    "    print(f\"   Description: {tool.description}\")\n",
    "    print()\n",
    "\n",
    "# Create a lookup dictionary for easy tool access\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "print(f\"Tool lookup dictionary created with {len(tools_by_name)} tools.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Bind Tools to LLM\n",
    "\n",
    "Connect the MCP tools to our LLM client so it can use them.\n",
    "\n",
    "**What this does:**\n",
    "- Binds tools to the LLM client\n",
    "- Creates a \"tooled\" model that can call external functions\n",
    "\n",
    "**Why this matters:**\n",
    "The LLM needs to know about available tools to decide when to call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind tools to the LLM client\n",
    "tooled_model = llm_client.bind_tools(tools)\n",
    "\n",
    "print(\"\u2705 Tools bound to LLM client!\")\n",
    "print(f\"Model now has access to {len(tools)} MCP tools.\")\n",
    "print()\n",
    "print(\"The LLM can now:\")\n",
    "print(\"- Decide when to call tools based on user queries\")\n",
    "print(\"- Generate appropriate tool call parameters\")\n",
    "print(\"- Process tool results to generate final responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Single Tool Call\n",
    "\n",
    "Test a simple query that should trigger just one tool.\n",
    "\n",
    "**What this does:**\n",
    "- Sends a simple query to the LLM\n",
    "- Checks if the LLM wants to call any tools\n",
    "- Executes the tool if requested\n",
    "\n",
    "**Why this matters:**\n",
    "Understanding single tool calls helps before moving to complex multi-tool scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple weather query\n",
    "test_query = \"What's the weather like in San Francisco?\"\n",
    "messages = [HumanMessage(test_query)]\n",
    "\n",
    "print(f\"User query: {test_query}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get AI response\n",
    "ai_message = tooled_model.invoke(messages)\n",
    "print(f\"AI Response: {ai_message.content}\")\n",
    "\n",
    "# Check for tool calls\n",
    "tool_calls = getattr(ai_message, \"tool_calls\", None)\n",
    "if tool_calls:\n",
    "    print(f\"\\n\ud83d\udee0\ufe0f  Tool calls detected: {len(tool_calls)}\")\n",
    "    \n",
    "    # Add AI message to conversation\n",
    "    messages.append(ai_message)\n",
    "    \n",
    "    # Execute the tool call\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        print(f\"\\nExecuting tool: {tool_name}\")\n",
    "        \n",
    "        selected_tool = tools_by_name.get(tool_name)\n",
    "        if selected_tool:\n",
    "            tool_msg = await selected_tool.ainvoke(tool_call)\n",
    "            print(f\"Tool result: {tool_msg.content}\")\n",
    "            \n",
    "            # Add tool result to conversation\n",
    "            messages.append(tool_msg)\n",
    "        else:\n",
    "            print(f\"\u274c Tool '{tool_name}' not found!\")\n",
    "            \n",
    "else:\n",
    "    print(\"\\n\u2705 No tool calls needed - direct response from LLM\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Single tool test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Manual Multi-Tool Conversation\n",
    "\n",
    "Demonstrate a conversation that requires multiple tool calls.\n",
    "\n",
    "**What this does:**\n",
    "- Shows the complete conversation loop\n",
    "- Handles multiple rounds of tool calling\n",
    "- Processes complex queries requiring multiple tools\n",
    "\n",
    "**Why this matters:**\n",
    "Real-world applications often need multiple tools to answer complex questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_manual_conversation():\n",
    "    \"\"\"Run a manual conversation with multiple tool calls.\"\"\"\n",
    "    \n",
    "    # Complex query requiring both weather and bill tools\n",
    "    user_query = \"What is my projected bill if the weather is 35 degrees and I have a gas oven? Also, are there any weather alerts for Colorado?\"\n",
    "    messages = [HumanMessage(user_query)]\n",
    "    \n",
    "    print(f\"\ud83c\udfaf User query: {user_query}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    conversation_round = 0\n",
    "    \n",
    "    while True:\n",
    "        conversation_round += 1\n",
    "        print(f\"\\n\ud83d\udd04 Round {conversation_round}:\")\n",
    "        \n",
    "        # Get AI response\n",
    "        ai_message = tooled_model.invoke(messages)\n",
    "        print(f\"\ud83e\udd16 AI: {ai_message.content}\")\n",
    "        \n",
    "        # Check for tool calls\n",
    "        tool_calls = getattr(ai_message, \"tool_calls\", None)\n",
    "        if not tool_calls:\n",
    "            print(\"\\n\u2705 No more tool calls - conversation complete!\")\n",
    "            break\n",
    "        \n",
    "        print(f\"\\n\ud83d\udee0\ufe0f  Tool calls in this round: {len(tool_calls)}\")\n",
    "        messages.append(ai_message)\n",
    "        \n",
    "        # Execute tools\n",
    "        for i, tool_call in enumerate(tool_calls, 1):\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            print(f\"\\n  {i}. Executing: {tool_name}\")\n",
    "            \n",
    "            selected_tool = tools_by_name.get(tool_name)\n",
    "            if selected_tool:\n",
    "                tool_msg = await selected_tool.ainvoke(tool_call)\n",
    "                print(f\"     Result: {tool_msg.content[:100]}...\" if len(tool_msg.content) > 100 else f\"     Result: {tool_msg.content}\")\n",
    "                messages.append(tool_msg)\n",
    "            else:\n",
    "                print(f\"     \u274c Tool '{tool_name}' not found!\")\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Run the conversation\n",
    "conversation_history = await run_manual_conversation()\n",
    "print(f\"\\n\ud83d\udcca Conversation completed with {len(conversation_history)} messages!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Agent-Style Execution\n",
    "\n",
    "Show how the manual approach can be wrapped in an agent-like function.\n",
    "\n",
    "**What this does:**\n",
    "- Creates a reusable function for agent execution\n",
    "- Demonstrates the pattern used in langchain_host.py\n",
    "- Shows how to handle different types of queries\n",
    "\n",
    "**Why this matters:**\n",
    "This bridges manual tool calling with automated agent behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83e\udd16 AI: Here are the latest weather alerts for California and your gas bill projection for 40\u00b0F with a gas oven:\n",
      "\n",
      "### California Weather Alerts\n",
      "- Multiple Flood Advisories and Flash Flood Warnings are in effect for areas including Los Angeles, Ventura, Orange, Santa Barbara, San Bernardino, San Diego, and surrounding counties.\n",
      "- Impacts include minor to severe flooding, rock and mud slides, dangerous surf conditions, possible road closures, and life-threatening conditions near burn scars.\n",
      "- Winter Weather Advisory: Up to 11 inches of snow above 7000 feet in parts of the Sierra Nevada.\n",
      "- High Surf and Beach Hazards: Dangerous rip currents and waves up to 14 feet along coastal areas.\n",
      "- Severe flood watches continue throughout the day for many parts of the state due to heavy rain from atmospheric rivers.\n",
      "- See full alert details and stay updated: [National Weather Service Flood Safety](http://www.weather.gov/safety/flood)\n",
      "\n",
      "**Instructions:** Avoid flooded roadways, heed evacuation orders if present, and be extremely cautious in affected areas. Dangerous surf conditions mean you should stay out of the water.\n",
      "\n",
      "### Gas Bill Projection at 40\u00b0F with Gas Oven\n",
      "- Your projected bill (with a gas oven in use at 40\u00b0F): **$125**\n",
      "- This estimate assumes typical usage patterns for a home with heating needs in cold weather and a gas oven.\n",
      "\n",
      "If you need more detail about a specific city or region in California, or a breakdown of the projected gas bill, let me know!\n",
      "\n",
      "\u2705 Agent completed! Used 2 tools.\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "async def mcp_agent(query):\n",
    "    \"\"\"\n",
    "    Agent-style execution with MCP tools.\n",
    "    Similar to the pattern in langchain_host.py\n",
    "    \"\"\"\n",
    "    print(f\"\ud83e\udd16 Agent processing: {query}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    messages = [HumanMessage(query)]\n",
    "    total_tools_called = 0\n",
    "    \n",
    "    while True:\n",
    "        # Get AI response\n",
    "        ai_message = tooled_model.invoke(messages)\n",
    "        print(f\"\\n\ud83e\udd16 AI: {ai_message.content}\")\n",
    "        \n",
    "        # Check for tool calls\n",
    "        tool_calls = getattr(ai_message, \"tool_calls\", None)\n",
    "        if not tool_calls:\n",
    "            break\n",
    "        \n",
    "        messages.append(ai_message)\n",
    "        \n",
    "        # Execute tools\n",
    "        for tool_call in tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            selected_tool = tools_by_name.get(tool_name)\n",
    "            \n",
    "            if selected_tool:\n",
    "                tool_msg = await selected_tool.ainvoke(tool_call)\n",
    "                messages.append(tool_msg)\n",
    "                total_tools_called += 1\n",
    "                print(f\"\ud83d\udee0\ufe0f  Called {tool_name}\")\n",
    "    \n",
    "    print(f\"\\n\u2705 Agent completed! Used {total_tools_called} tools.\")\n",
    "    return messages\n",
    "\n",
    "# Test the agent with different queries\n",
    "test_queries = [\n",
    "    \"What's the weather in Denver?\",\n",
    "    \"Calculate my bill for 50 degrees with electric oven\",\n",
    "    \"Weather alerts for California and bill projection for 40 degrees gas oven\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    await mcp_agent(query)\n",
    "    print(\"\\n\" + \"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Single Tool Queries\n",
    "Try queries that only use one tool:\n",
    "- \"What's the weather in New York?\"\n",
    "- \"What's my bill projection for 60 degrees with electric oven?\"\n",
    "\n",
    "### Exercise 2: Multi-Tool Queries\n",
    "Create queries requiring both tools:\n",
    "- \"If it's 45 degrees and I have gas heating, what's my bill and any alerts for Texas?\"\n",
    "\n",
    "### Exercise 3: Error Handling\n",
    "Test what happens when servers aren't running:\n",
    "- Stop the weather server and try a weather query\n",
    "- Observe how the system handles connection failures\n",
    "\n",
    "### Exercise 4: Model Comparison\n",
    "Change the LLM_MODEL and compare results:\n",
    "- Try 'openai.gpt-4o' or 'xai.grok-4'\n",
    "- See how different models handle tool calling\n",
    "\n",
    "### Exercise 5: Custom Queries\n",
    "Create your own test queries and observe the tool calling behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}