{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP (Model Context Protocol) Tool Integration\n",
    "\n",
    "This notebook demonstrates how to integrate MCP servers with LangChain agents using OCI Generative AI. It shows both manual tool calling and automatic agent execution with external MCP tools.\n",
    "\n",
    "## What this notebook does\n",
    "\n",
    "Demonstrates MCP server integration with LangChain, showing how to connect to weather and bill projection MCP servers, manually call tools, and use automatic agent execution.\n",
    "\n",
    "## Documentation to reference\n",
    "\n",
    "- OCI Gen AI: https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm\n",
    "- LangChain: https://docs.langchain.com/oss/python/langchain/agents\n",
    "- MCP Servers: https://modelcontextprotocol.io/docs/learn/server-concepts\n",
    "- LangChain MCP clients: https://docs.langchain.com/oss/python/langchain/mcp#model-context-protocol-mcp\n",
    "- OCI OpenAI compatible SDK: https://github.com/oracle-samples/oci-openai\n",
    "\n",
    "## Relevant slack channels\n",
    "\n",
    "- #generative-ai-users: for questions on OCI Gen AI\n",
    "- #igiu-innovation-lab: general discussions on your project\n",
    "- #igiu-ai-learning: help with sandbox environment or help with running this code\n",
    "\n",
    "## Env setup\n",
    "\n",
    "- sandbox.yaml: Contains OCI config, compartment, DB details, and wallet path.\n",
    "- .env: Load environment variables (e.g., API keys if needed).\n",
    "- configure cwd for jupyter match your workspace python code: \n",
    "    -  vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "    -  change from `${fileDirname}` to `${workspaceFolder}`\n",
    "- MCP servers must be running:\n",
    "  - Weather server on http://localhost:8000/mcp\n",
    "  - Bill server via stdio\n",
    "\n",
    "## How to run the notebook\n",
    "\n",
    "1. Start the MCP servers first:\n",
    "   - `uv run langChain/function_calling/mcp/weather_mcp_server.py` (in background)\n",
    "   - The bill server will be started automatically by the MCP client\n",
    "2. Run the cells in order. Make sure your sandbox.yaml is configured.\n",
    "\n",
    "## Comments to important sections of notebook\n",
    "\n",
    "- Cell 1: Instantiate the OCI client and load config\n",
    "- Cell 2: Set up MCP client connections\n",
    "- Manual section: Demonstrates manual MCP tool calling\n",
    "\n",
    "## Experimentation Tips\n",
    "\n",
    "- Try different queries that combine weather and bill information\n",
    "- Experiment with the system_prompt in the agent\n",
    "- Test error handling when MCP servers are unavailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.messages import HumanMessage\n",
    "from langChain.oci_openai_helper import OCIOpenAIHelper\n",
    "\n",
    "def load_config(config_path):\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            return EnvYAML(config_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Configuration file '{config_path}' not found.\")\n",
    "        return None\n",
    "\n",
    "load_dotenv()\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "LLM_MODEL = \"openai.gpt-4.1\"\n",
    "\n",
    "# Step 1: Load configuration and initialize OCI client\n",
    "scfg = load_config(SANDBOX_CONFIG_FILE)\n",
    "\n",
    "llm_client = OCIOpenAIHelper.get_client(\n",
    "    model_name=LLM_MODEL,\n",
    "    config=scfg\n",
    ")\n",
    "\n",
    "print(\"OCI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up MCP client connections\n",
    "\n",
    "Initialize the MCP client to connect to multiple servers (weather and bill projection). This cell demonstrates how to establish connections to external MCP servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set up MCP client with multiple servers\n",
    "async def setup_mcp_client():\n",
    "    \"\"\"\n",
    "    Initialize MCP client with connections to weather and bill servers.\n",
    "    \n",
    "    Make sure the weather server is running on localhost:8000\n",
    "    The bill server will be started automatically via stdio\n",
    "    \"\"\"\n",
    "    client = MultiServerMCPClient(\n",
    "        {\n",
    "            \"weather\": {\n",
    "                \"transport\": \"streamable_http\",\n",
    "                \"url\": \"http://localhost:8000/mcp\",\n",
    "            },\n",
    "            \"bill_server\": {\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"./langChain/function_calling/mcp/bill_mcp_server.py\"],\n",
    "                \"transport\": \"stdio\",\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    tools = await client.get_tools()\n",
    "    \n",
    "    print(f\"Connected to MCP servers. Found {len(tools)} tools:\")\n",
    "    for tool in tools:\n",
    "        print(f\"- {tool.name}: {tool.description}\")\n",
    "    \n",
    "    # Bind tools to the LLM client\n",
    "    tooled_model = llm_client.bind_tools(tools)\n",
    "    tools_by_name = {tool.name: tool for tool in tools}\n",
    "    \n",
    "    return tooled_model, tools_by_name\n",
    "\n",
    "# Test the MCP setup\n",
    "import asyncio\n",
    "tooled_model, tools_by_name = await setup_mcp_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual MCP tool calling\n",
    "\n",
    "This section demonstrates manual tool calling with MCP tools. The process is:\n",
    "1. Send a message to the model\n",
    "2. Check if the model wants to call tools\n",
    "3. Execute the tools manually\n",
    "4. Send the results back to get the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual MCP tool calling demonstration\n",
    "async def manual_mcp_calling():\n",
    "    prompt = \"What is my projected bill if the weather is 40 degrees and I have a gas oven? Also, show me weather alerts for Colorado.\"\n",
    "    messages = [HumanMessage(prompt)]\n",
    "    \n",
    "    print(f\"User query: {prompt}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        # 1. Get AI response\n",
    "        ai_message = tooled_model.invoke(messages)\n",
    "        print(f\"\\nAI Response: {ai_message.content}\")\n",
    "        \n",
    "        # 2. Check for tool calls\n",
    "        if not getattr(ai_message, \"tool_calls\", None):\n",
    "            print(\"\\nNo tool calls detected â€” conversation finished.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"\\n*** Tool calls detected: {len(ai_message.tool_calls)} ***\")\n",
    "        messages.append(ai_message)\n",
    "        \n",
    "        # 3. Execute all tool calls\n",
    "        for tool_call in ai_message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"].lower()\n",
    "            selected_tool = tools_by_name.get(tool_name)\n",
    "            \n",
    "            if not selected_tool:\n",
    "                print(f\"Unknown tool requested: {tool_name}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nExecuting tool: {tool_name}\")\n",
    "            tool_msg = await selected_tool.ainvoke(tool_call)\n",
    "            print(f\"Tool result: {tool_msg.content}\")\n",
    "            \n",
    "            # 4. Add tool result to conversation\n",
    "            messages.append(tool_msg)\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Run manual MCP calling\n",
    "final_messages = await manual_mcp_calling()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Manual MCP calling completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
