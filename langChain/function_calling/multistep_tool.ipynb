{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7dcb947",
   "metadata": {},
   "source": [
    "# Multi-step tool calling: Manual first, then Automatic (Agent)\n",
    "\n",
    "This notebook demonstrates manual multi-tool orchestration first, followed by automatic (agent) orchestration, using two tools. This is designed for students to experiment with multi-step function calling in LangChain using OCI Generative AI.\n",
    "\n",
    "## What this notebook does\n",
    "Demonstrates manual multi-step function calling with custom tools using OCI Generative AI for LLM, followed by automatic agent orchestration.\n",
    "\n",
    "## Documentation to reference\n",
    "- OCI Gen AI: https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm\n",
    "- LangChain: https://docs.langchain.com/oss/python/langchain/agents\n",
    "- How to build tools: https://python.langchain.com/docs/how_to/custom_tools/\n",
    "- OCI OpenAI compatible SDK: https://github.com/oracle-samples/oci-openai  note: supports OpenAI, XAI & Meta models. Also supports OpenAI Responses API \n",
    "- OCI langchain SDK: https://github.com/oracle-devrel/langchain-oci-genai  note: as of Nov 2025 it is not compatible with langchain v1.0. supports all OCI models including Cohere\n",
    "- OCI GenAI SDK: https://github.com/oracle/oci-python-sdk/tree/master/src/oci/generative_ai_inference/models\n",
    "\n",
    "## Relevant slack channels\n",
    "- #generative-ai-users: *for questions on OCI Gen AI* \n",
    "- #igiu-innovation-lab: *general discussions on your project* \n",
    "- #igiu-ai-learning: *help with sandbox environment or help with running this code* \n",
    "\n",
    "## Env setup\n",
    "- sandbox.yaml: Contains OCI config, compartment, DB details, and wallet path.\n",
    "- .env: Load environment variables (e.g., API keys if needed).\n",
    "- configure cwd for jupyter match your workspace python code: \n",
    "    -  vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "    -  change from `${fileDirname}` to `${workspaceFolder}`\n",
    "\n",
    "## How to run the notebook\n",
    "Run the cells in order. Make sure your sandbox.yaml is configured. You can also run equivalent Python files like langChain/function_calling/langchain_multi_step.py or langChain/function_calling/langchain_multi_manual.py using `uv run <path>`.\n",
    "\n",
    "## Comments to important sections of notebook\n",
    "- Cell 1: Instantiate the client and load config\n",
    "- Cell 2: Define the tools\n",
    "- Manual section: Demonstrates manual multi-tool orchestration in a loop\n",
    "- Automatic section: Shows agent-based multi-step tool calling\n",
    "\n",
    "## Experimentation Tips\n",
    "- Try changing the LLM_MODEL to different values like 'meta.llama-3.1-405b-instruct' or 'xai.grok-4' to see how responses vary.\n",
    "- Experiment with the system_prompt in the agent: modify it to change the personality or add more instructions.\n",
    "- Add more tools or modify existing ones to see how the agent handles them.\n",
    "- Try different reasoning parameters in the client (e.g., change 'effort' to 'high')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a739d68",
   "metadata": {},
   "source": [
    "## 1. Instantiate the client\n",
    "\n",
    "This cell sets up the OCI OpenAI client using your configuration. It loads the sandbox.yaml file, sets the model (try experimenting with different models!), and initializes the LLM client with reasoning parameters for multi-step tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602666ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\nfrom dotenv import load_dotenv\nfrom envyaml import EnvYAML\nfrom langchain.tools import tool\nfrom langchain_core.messages import HumanMessage\nfrom langchain.agents import create_agent\n\nfrom langChain.oci_openai_helper import OCIOpenAIHelper\nload_dotenv()\n\ndef load_config(config_path):\n    try:\n        with open(config_path, 'r') as f:\n            return EnvYAML(config_path)\n    except FileNotFoundError:\n        print(f\"Error: Configuration file '{config_path}' not found.\")\n        return None\n\ndef pretty_print(response):\n    for i, m in enumerate(response[\"messages\"], 1):\n        role = getattr(m, \"type\", m.__class__.__name__)\n        content = m.content if isinstance(m.content, str) else str(m.content)\n        print(f\"{i:>2}. [{role.upper()}] {content}\")\n\nSANDBOX_CONFIG_FILE = \"sandbox.yaml\"\nLLM_MODEL = \"openai.gpt-5\"\n\nscfg = load_config(SANDBOX_CONFIG_FILE)\nllm = OCIOpenAIHelper.get_langchain_openai_client(\n    model_name=LLM_MODEL,\n    config=scfg,\n    use_responses_api=True,\n    reasoning={\"effort\": \"low\", \"summary\": \"auto\"}\n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e529624",
   "metadata": {},
   "source": [
    "## 2. Define the tools\n",
    "\n",
    "Here we define two custom tools: one for getting weather and one for projecting bills. These are LangChain tools decorated with @tool. Experiment by adding more tools or modifying their logic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e187d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Gets the weather for a given city\"\"\"\n",
    "    return f\"The weather in {city} is 70 Fahrenheit\"\n",
    "\n",
    "@tool\n",
    "def get_projection_bill(current_bill: int, gas_oven: bool) -> int:\n",
    "    \"\"\"Returns the projected bill depending on current bill and oven usage\"\"\"\n",
    "    if gas_oven:\n",
    "        return current_bill + 45\n",
    "    return current_bill + 4\n",
    "\n",
    "tools = [get_weather, get_projection_bill]\n",
    "tool_map = {t.name.lower(): t for t in tools}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca1ab9",
   "metadata": {},
   "source": [
    "## Manual multi-tool orchestration\n",
    "\n",
    "This section shows how to manually handle multi-step tool calling. The model may decide to call tools, and we loop until it finishes. This gives you control over the process. Notice how the model might ask for clarification (like oven type) before using tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    HumanMessage(\n",
    "        \"Which will be my projected bill? I'm in San Francisco, and I have oven. My past bill was $45\"\n",
    "    )\n",
    "]\n",
    "\n",
    "llm_with_tools = llm.bind_tools([get_weather, get_projection_bill])\n",
    "\n",
    "while True:\n",
    "    ai_message = llm_with_tools.invoke(messages)\n",
    "    print(\"\\nAI response content:\", ai_message.content)\n",
    "    tool_calls = getattr(ai_message, \"tool_calls\", None)\n",
    "    print(\"Tool calls:\", tool_calls)\n",
    "\n",
    "    if not tool_calls:\n",
    "        print(\"\\nNo tool calls detected \u2014 conversation finished.\")\n",
    "        final_message = ai_message\n",
    "        break\n",
    "\n",
    "    messages.append(ai_message)\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"].lower()\n",
    "        selected_tool = tool_map.get(tool_name)\n",
    "        if not selected_tool:\n",
    "            print(f\"Unknown tool requested: {tool_name}\")\n",
    "            continue\n",
    "        print(f\"Executing tool: {tool_name} with args: {tool_call.get('args', {})}\")\n",
    "        tool_msg = selected_tool.invoke(tool_call)\n",
    "        print(\"Tool result:\", tool_msg.content)\n",
    "        messages.append(tool_msg)\n",
    "\n",
    "print(\"\\n************************ Conversation Ended ************************\")\n",
    "print(\"Final model message:\\n\", final_message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4128c3",
   "metadata": {},
   "source": [
    "## Automatic (Agent) multi-step orchestration\n",
    "\n",
    "Now we use LangChain's agent to handle tool calling automatically. The agent decides when to call tools and how to respond. Notice how it can handle multiple steps and even parallel tool calls (like calling get_projection_bill twice for gas and electric scenarios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    llm,\n",
    "    tools=[get_weather, get_projection_bill],\n",
    "    system_prompt=\"use one or more tools to get an answer; reply in Hinglish (Hindi mixed with English)\"\n",
    ")\n",
    "\n",
    "messages_agent = [\n",
    "    HumanMessage(\n",
    "        content=\"Which will be my projected bill? I'm in San Francisco, and I have oven. My past bill was $45\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"************************** Agent Multi-Step invoke and details **************************\")\n",
    "response = agent.invoke({\"messages\": messages_agent})\n",
    "pretty_print(response)\n",
    "\n",
    "print(\"\\n************************** Agent Multi-Step stream **************************\")\n",
    "for chunk in agent.invoke({\"messages\": messages_agent}, stream_mode=\"updates\"):\n",
    "    for step, data in chunk.items():\n",
    "        print(f\"step: {step}\", flush=True)\n",
    "        print(f\"content: {data['messages'][-1].content_blocks}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc1931",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Modify the tools**: Add a third tool, like `get_population(city: str) -> int`, and update the agent to use it. Test with a query like \"What's the weather and population in San Francisco?\"\n",
    "\n",
    "2. **Change the system prompt**: Experiment with different personalities, e.g., \"reply as a strict teacher\" or \"reply in Shakespearean English\". How does it affect tool usage?\n",
    "\n",
    "3. **Parallel tool calls**: In the manual section, modify the code to handle parallel tool calls if the model suggests them.\n",
    "\n",
    "4. **Error handling**: Add error handling in the tool functions, e.g., if `current_bill` is negative, raise an exception. See how the agent responds.\n",
    "\n",
    "5. **Streaming vs. non-streaming**: Compare the streaming output with the regular invoke. What differences do you notice in multi-step scenarios?\n",
    "\n",
    "6. **Model comparison**: Run the notebook with different LLM_MODEL values and compare the reasoning and tool usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}