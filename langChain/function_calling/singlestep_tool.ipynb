{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7089a3",
   "metadata": {},
   "source": [
    "## 1. Instantiate the client\n",
    "\n",
    "This cell sets up the OCI OpenAI client. Load your sandbox.yaml, pick a model (experiment with different ones!), and initialize the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb1c8e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0500af17",
   "metadata": {},
   "source": [
    "# Single-step tool calling: Manual first, then Automatic (Agent)\n",
    "\n",
    "This notebook demonstrates manual tool calling first, followed by automatic (agent) calling, for a single tool. Designed for students to experiment with basic function calling.\n",
    "\n",
    "## What this notebook does\n",
    "Demonstrates manual function calling with a custom tool using OCI Generative AI for LLM, followed by automatic agent orchestration.\n",
    "\n",
    "## Documentation to reference\n",
    "- OCI Gen AI: https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm\n",
    "- LangChain: https://docs.langchain.com/oss/python/langchain/agents\n",
    "- How to build tools: https://python.langchain.com/docs/how_to/custom_tools/\n",
    "- OCI OpenAI compatible SDK: https://github.com/oracle-samples/oci-openai  note: supports OpenAI, XAI & Meta models. Also supports OpenAI Responses API \n",
    "- OCI langchain SDK: https://github.com/oracle-devrel/langchain-oci-genai  note: as of Nov 2025 it is not compatible with langchain v1.0. supports all OCI models including Cohere\n",
    "- OCI GenAI SDK: https://github.com/oracle/oci-python-sdk/tree/master/src/oci/generative_ai_inference/models\n",
    "\n",
    "## Relevant slack channels\n",
    "- #generative-ai-users: *for questions on OCI Gen AI* \n",
    "- #igiu-innovation-lab: *general discussions on your project* \n",
    "- #igiu-ai-learning: *help with sandbox environment or help with running this code*\n",
    "   \n",
    "\n",
    "## Env setup\n",
    "- sandbox.yaml: Contains OCI config, compartment, DB details, and wallet path.\n",
    "- .env: Load environment variables (e.g., API keys if needed).\n",
    "- configure cwd for jupyter match your workspace python code: \n",
    "    -  vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "    -  change from `${fileDirname}` to `${workspaceFolder}`\n",
    "#####\n",
    "\n",
    "## How to run the notebook\n",
    "Run the cells in order. Make sure your sandbox.yaml is configured. You can also run equivalent Python files like langChain/function_calling/langchain_step.py or langChain/function_calling/langchain_step_manual.py using `uv run <path>`.\n",
    "\n",
    "## Comments to important sections of notebook\n",
    "- Cell 1: Instantiate the client and load config\n",
    "- Cell 2: Define the tool\n",
    "- Manual section: Demonstrates manual single tool calling\n",
    "- Automatic section: Shows agent-based single-step tool calling\n",
    "\n",
    "## Experimentation Tips\n",
    "- Try changing the LLM_MODEL to different values like 'meta.llama-3.1-405b-instruct' or 'xai.grok-4' to see how responses vary.\n",
    "- Experiment with the system_prompt in the agent: modify it to change the personality or add more instructions.\n",
    "- Add more tools to the list and see how the agent chooses which one to use.\n",
    "- Try different queries to see if the model calls the tool or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML\n",
    "from langchain.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "from langChain.oci_openai_helper import OCIOpenAIHelper\n",
    "\n",
    "\n",
    "def load_config(config_path):\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            return EnvYAML(config_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Configuration file '{config_path}' not found.\")\n",
    "        return None\n",
    "\n",
    "def pretty_print(response):\n",
    "    for i, m in enumerate(response[\"messages\"], 1):\n",
    "        role = getattr(m, \"type\", m.__class__.__name__)\n",
    "        content = m.content if isinstance(m.content, str) else str(m.content)\n",
    "        print(f\"{i:>2}. [{role.upper()}] {content}\")\n",
    "\n",
    "load_dotenv()\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "LLM_MODEL = \"openai.gpt-5\"\n",
    "\n",
    "scfg = load_config(SANDBOX_CONFIG_FILE)\n",
    "llm = OCIOpenAIHelper.get_langchain_openai_client(\n",
    "    model_name=LLM_MODEL,\n",
    "    config=scfg,\n",
    "    use_responses_api=True,\n",
    "    reasoning={\"effort\": \"low\", \"summary\": \"auto\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b382bf",
   "metadata": {},
   "source": [
    "## 2. Define the tool\n",
    "\n",
    "Define a simple weather tool. This is a LangChain tool that the model can call. Try adding parameters or modifying the return value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Gets the weather for a given city\"\"\"\n",
    "    return f\"The weather in {city} is 70 Fahrenheit\"\n",
    "\n",
    "tools = [get_weather]\n",
    "tool_map = {t.name.lower(): t for t in tools}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual single-tool calling\n",
    "\n",
    "This shows manual tool calling: bind the tool to the model, invoke once to get tool calls, execute the tool, and invoke again with the result. Simple for single-step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(\"How is the weather in San Francisco?\")]\n",
    "llm_with_tools = llm.bind_tools([get_weather])\n",
    "\n",
    "ai_message = llm_with_tools.invoke(messages)\n",
    "print(\"AI response content:\", ai_message.content)\n",
    "print(\"Tool calls:\", getattr(ai_message, \"tool_calls\", None))\n",
    "\n",
    "if getattr(ai_message, \"tool_calls\", None):\n",
    "    messages.append(ai_message)\n",
    "    for tool_call in ai_message.tool_calls:\n",
    "        tool_name = tool_call[\"name\"].lower()\n",
    "        selected_tool = tool_map.get(tool_name)\n",
    "        if not selected_tool:\n",
    "            print(f\"Unknown tool requested: {tool_name}\")\n",
    "            continue\n",
    "        print(f\"Executing tool: {tool_name} with args: {tool_call.get('args', {})}\")\n",
    "        tool_msg = selected_tool.invoke(tool_call)\n",
    "        print(\"Tool result:\", tool_msg.content)\n",
    "        messages.append(tool_msg)\n",
    "\n",
    "final_response = llm_with_tools.invoke(messages)\n",
    "print(\"\\nFinal model message:\")\n",
    "print(final_response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic (Agent) single-step\n",
    "\n",
    "Using LangChain's agent to handle everything automatically. The agent invokes the model, calls tools as needed, and responds. Notice the pirate personality from the system prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    llm,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"talk like a pirate who tells bad dad jokes\"\n",
    ")\n",
    "\n",
    "messages_agent = [HumanMessage(content=\"What's the weather in San Francisco?\")]\n",
    "\n",
    "print(\"Agent single-step invocation:\")\n",
    "response = agent.invoke({\"messages\": messages_agent})\n",
    "pretty_print(response)\n",
    "\n",
    "print(\"\\nAgent single-step stream:\")\n",
    "for chunk in agent.invoke({\"messages\": messages_agent}, stream_mode=\"updates\"):\n",
    "    for step, data in chunk.items():\n",
    "        print(f\"step: {step}\", flush=True)\n",
    "        print(f\"content: {data['messages'][-1].content_blocks}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Add a tool**: Create a second tool, like `get_population(city: str) -> int`, and update both manual and agent sections to use it. Test with queries like \"What's the weather and population in San Francisco?\"\n",
    "\n",
    "2. **Change the system prompt**: Try different prompts, e.g., \"reply as a Shakespearean actor\" or \"be very formal and precise\". How does it change the response style?\n",
    "\n",
    "3. **Model comparison**: Switch the LLM_MODEL to 'xai.grok-4' or 'meta.llama-3.1-405b-instruct' and run the notebook. Note any differences in tool usage or responses.\n",
    "\n",
    "4. **No tool calls**: Modify the query to something that doesn't require a tool, like \"Hello, how are you?\" See if the agent still tries to call tools.\n",
    "\n",
    "5. **Tool errors**: Add a print statement or error in the tool function, e.g., if city is invalid. Observe how the manual and agent handle it.\n",
    "\n",
    "6. **Streaming details**: Analyze the streaming output closely. What does each 'step' represent, and how does it differ from the non-streaming version?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}