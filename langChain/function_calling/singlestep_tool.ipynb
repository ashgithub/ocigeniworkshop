{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Single-step tool calling: Manual first, then Automatic (Agent)\n",
        "\n",
        "This notebook demonstrates manual tool calling first, followed by automatic (agent) calling, for a single tool."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f7089a3",
      "metadata": {},
      "source": [
        "1. instantiate teh client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "from dotenv import load_dotenv\n",
        "from envyaml import EnvYAML\n",
        "from langchain.tools import tool\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.agents import create_agent\n",
        "\n",
        "from langChain.oci_openai_helper import OCIOpenAIHelper\n",
        "\n",
        "\n",
        "def load_config(config_path):\n",
        "    try:\n",
        "        with open(config_path, 'r') as f:\n",
        "            return EnvYAML(config_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Configuration file '{config_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "def pretty_print(response):\n",
        "    for i, m in enumerate(response[\"messages\"], 1):\n",
        "        role = getattr(m, \"type\", m.__class__.__name__)\n",
        "        content = m.content if isinstance(m.content, str) else str(m.content)\n",
        "        print(f\"{i:>2}. [{role.upper()}] {content}\")\n",
        "\n",
        "load_dotenv()\n",
        "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
        "LLM_MODEL = \"openai.gpt-5\"\n",
        "\n",
        "scfg = load_config(SANDBOX_CONFIG_FILE)\n",
        "llm = OCIOpenAIHelper.get_client(\n",
        "    model_name=LLM_MODEL,\n",
        "    config=scfg,\n",
        "    use_responses_api=True,\n",
        "    reasoning={\"effort\": \"low\", \"summary\": \"auto\"}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b382bf",
      "metadata": {},
      "source": [
        "2. define the tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Gets the weather for a given city\"\"\"\n",
        "    return f\"The weather in {city} is 70 Fahrenheit\"\n",
        "\n",
        "tools = [get_weather]\n",
        "tool_map = {t.name.lower(): t for t in tools}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manual single-tool calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI response content: [{'id': 'rs_0d37f2bef9145fdb01690e94d4be388196a5a65d1cbe623b39', 'summary': [{'text': '**Using weather tool**\\n\\nI need to use the get_weather tool for checking the weather in San Francisco. It makes sense to call functions.get_weather directly since I only need one tool here. I’m not sure if the tool will return dummy information, but that’s okay! I’ll just move forward and call get_weather. Once I have the result, I’ll make sure to respond with the details clearly for the user. Here goes!', 'type': 'summary_text'}], 'type': 'reasoning'}, {'arguments': '{\"city\":\"San Francisco\"}', 'call_id': 'call_i07CuJLMl5tSvhjk2oiBGPKG', 'name': 'get_weather', 'type': 'function_call', 'id': 'fc_0d37f2bef9145fdb01690e94d88d8c8196b311d06e183ea38f', 'status': 'completed'}]\n",
            "Tool calls: [{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_i07CuJLMl5tSvhjk2oiBGPKG', 'type': 'tool_call'}]\n",
            "Executing tool: get_weather with args: {'city': 'San Francisco'}\n",
            "Tool result: The weather in San Francisco is 70 Fahrenheit\n",
            "\n",
            "Final model message:\n",
            "[{'type': 'text', 'text': \"It's currently around 70°F in San Francisco, mild and pleasant.\", 'annotations': [], 'id': 'msg_0d37f2bef9145fdb01690e94d9cc0481969046c7660a325734'}]\n"
          ]
        }
      ],
      "source": [
        "messages = [HumanMessage(\"How is the weather in San Francisco?\")]\n",
        "llm_with_tools = llm.bind_tools([get_weather])\n",
        "\n",
        "ai_message = llm_with_tools.invoke(messages)\n",
        "print(\"AI response content:\", ai_message.content)\n",
        "print(\"Tool calls:\", getattr(ai_message, \"tool_calls\", None))\n",
        "\n",
        "if getattr(ai_message, \"tool_calls\", None):\n",
        "    messages.append(ai_message)\n",
        "    for tool_call in ai_message.tool_calls:\n",
        "        tool_name = tool_call[\"name\"].lower()\n",
        "        selected_tool = tool_map.get(tool_name)\n",
        "        if not selected_tool:\n",
        "            print(f\"Unknown tool requested: {tool_name}\")\n",
        "            continue\n",
        "        print(f\"Executing tool: {tool_name} with args: {tool_call.get('args', {})}\")\n",
        "        tool_msg = selected_tool.invoke(tool_call)\n",
        "        print(\"Tool result:\", tool_msg.content)\n",
        "        messages.append(tool_msg)\n",
        "\n",
        "final_response = llm_with_tools.invoke(messages)\n",
        "print(\"\\nFinal model message:\")\n",
        "print(final_response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Automatic (Agent) single-step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent single-step invocation:\n",
            " 1. [HUMAN] What's the weather in San Francisco?\n",
            " 2. [AI] [{'id': 'rs_0b7d1fa2aa3a392f01690e94dde1448196a781f37d91783f92', 'summary': [{'text': \"**Fetching pirate weather**\\n\\nI'm getting ready to answer the weather request! I need to use the get_weather tool to fetch the weather for San Francisco. I'll ask for the weather data using functions.get_weather and make sure to keep the tone fun and pirate-like, sprinkling in some dad jokes along the way. Since I'm only looking for one piece of information, a straightforward call will do. Let’s hoist the sails and fetch that weather!\", 'type': 'summary_text'}], 'type': 'reasoning'}, {'arguments': '{\"city\":\"San Francisco\"}', 'call_id': 'call_3ys63cRttFKMfiHggukIpzuM', 'name': 'get_weather', 'type': 'function_call', 'id': 'fc_0b7d1fa2aa3a392f01690e94e139f0819694557430d4c8de90', 'status': 'completed'}]\n",
            " 3. [TOOL] The weather in San Francisco is 70 Fahrenheit\n",
            " 4. [AI] [{'type': 'text', 'text': 'Arrr, matey! The San Francisco seas be blowin’ a balmy 70 degrees Fahrenheit. Perfect weather fer a brisk walk on the plank—er, the pier!\\n\\nWhy did the pirate bring a jacket to San Francisco? Because the bay has “arrr-tic” microclimates!', 'annotations': [], 'id': 'msg_0b7d1fa2aa3a392f01690e94e25b048196baad46b332401962'}]\n",
            "\n",
            "Agent single-step stream:\n",
            "step: model\n",
            "content: [{'id': 'rs_0ca76cf0a0070db201690e94e5df588197bfb756ea5191a6e8', 'type': 'reasoning', 'reasoning': '**Answering weather question**\\n\\nI need to respond to a weather inquiry, so I’ll use the get_weather tool with the city set to \"San Francisco.\" I think it’s a fun twist to speak like a pirate and include some bad dad jokes! I’ll go ahead and just use this single tool, as it seems like a call to functions.get_weather is all that\\'s needed right now. Let’s see what the weather has to say! Yarrr!'}, {'type': 'tool_call', 'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_xF4r2GVzru0YBe5ZWFZ8Fk5z', 'extras': {'item_id': 'fc_0ca76cf0a0070db201690e94e828508197bd63ad9446ebc67a'}}]\n",
            "step: tools\n",
            "content: [{'type': 'text', 'text': 'The weather in San Francisco is 70 Fahrenheit'}]\n",
            "step: model\n",
            "content: [{'type': 'text', 'text': 'Arrr, matey! In San Francisco the seas be calm and the air be a balmy 70 degrees Fahrenheit. Perfect weather fer a treasure hunt—or at least fer finding me lost sock. Guess it walked the plank without me!', 'annotations': [], 'id': 'msg_0ca76cf0a0070db201690e94e901dc8197bbf66dddb0d124f3'}]\n"
          ]
        }
      ],
      "source": [
        "agent = create_agent(\n",
        "    llm,\n",
        "    tools=[get_weather],\n",
        "    system_prompt=\"talk like a pirate who tells bad dad jokes\"\n",
        ")\n",
        "\n",
        "messages_agent = [HumanMessage(content=\"What's the weather in San Francisco?\")]\n",
        "\n",
        "print(\"Agent single-step invocation:\")\n",
        "response = agent.invoke({\"messages\": messages_agent})\n",
        "pretty_print(response)\n",
        "\n",
        "print(\"\\nAgent single-step stream:\")\n",
        "for chunk in agent.invoke({\"messages\": messages_agent}, stream_mode=\"updates\"):\n",
        "    for step, data in chunk.items():\n",
        "        print(f\"step: {step}\", flush=True)\n",
        "        print(f\"content: {data['messages'][-1].content_blocks}\", flush=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
