{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI & OCI LLMs with LangChain: Educational Demo\n",
    "\n",
    "### What this notebook does:\n",
    "\n",
    "Demonstrates comprehensive usage of OpenAI-compatible LLMs (OpenAI, OCI, Meta, Groq, etc.) via LangChain. Covers basic chat, model performance comparison, batching, streaming, conversation history, structured output, and reasoning capabilities.\n",
    "\n",
    "**Documentation to reference:**\n",
    "\n",
    "- OCI Gen AI Chat Models: https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm\n",
    "- OCI OpenAI Compatible SDK: https://github.com/oracle-samples/oci-openai\n",
    "- LangChain Overview: https://docs.langchain.com/oss/python/langchain/overview\n",
    "- OpenAI API Reference: https://platform.openai.com/docs/api-reference\n",
    "\n",
    "**Relevant slack channels:**\n",
    "\n",
    "- #generative-ai-users: *for questions on OCI Gen AI*\n",
    "- #igiu-innovation-lab: *general discussions on your project*\n",
    "- #igiu-ai-learning: *help with sandbox environment or help with running this code*\n",
    "- configure cwd for jupyter match your workspace python code: \n",
    "    -  vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "    -  change from `${fileDirname}` to `${workspaceFolder}`\n",
    "\n",
    "\n",
    "**Env setup:**\n",
    "\n",
    "- sandbox.yaml: Contains OCI config, compartment details.\n",
    "- .env: Load environment variables (e.g., API keys if needed).\n",
    "\n",
    "**How to run in notebook:**\n",
    "\n",
    "- Make sure your runtime environment has all dependencies and access to required config files.\n",
    "- Run the notebook cells in order from top to bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported LLM Models\n",
    "\n",
    "Here are model IDs you can use with the OCI OpenAI-compatible API (see [Oracle Docs](https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm) for updates):\n",
    "\n",
    "| Family | Example Models |\n",
    "|--------|----------------|\n",
    "| **OpenAI**   | openai.gpt-4.1, openai.gpt-4o, openai.gpt-5 |\n",
    "| **Meta (Llama)** | meta.llama-4-maverick-17b-128e-instruct, meta.llama-4-scout-17b-16e-instruct |\n",
    "| **Cohere** | cohere.command-a-03-2025, cohere.command-plus-latest |\n",
    "| **Groq/XAI** | xai.grok-3, xai.grok-4 |\n",
    "\n",
    "> **TIP:** Experiment with different models to compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment & Configuration\n",
    "\n",
    "Prepare your `sandbox.yaml` (or `sandbox.json`) config with the appropriate OCI credentials and compartment settings. Place it in your working directory or adjust the code below to point to its location.\n",
    "\n",
    "If you have trouble loading credentials or get permission errors, check the file path, format, and profile settings. For help, reach out on the Slack channels above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd14257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OCI + Model Configuration\n",
    "import os,sys,time\n",
    "from envyaml import EnvYAML\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "\n",
    "load_dotenv()\n",
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from a YAML file.\"\"\"\n",
    "    try:\n",
    "        return EnvYAML(config_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Configuration file '{config_path}' not found.\")\n",
    "        return None\n",
    "\n",
    "scfg = load_config(SANDBOX_CONFIG_FILE)\n",
    "if scfg is not None:\n",
    "    print(\"Config loaded successfully! Profile:\", scfg['oci']['profile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b789bb",
   "metadata": {},
   "source": [
    "### Set up your LLM client\n",
    "\n",
    "- Specify your target model (`LLM_MODEL`).\n",
    "- Choose the appropriate service endpoint.\n",
    "- Use the config (`scfg`) loaded above for credentials.\n",
    "\n",
    "You may need to re-instantiate the client when switching models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4505f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory of the notebook to sys.path\nfrom langChain.oci_openai_helper import OCIOpenAIHelper\n\nLLM_MODEL = \"openai.gpt-4.1\"\n# Other ideas: \"meta.llama-4-maverick-17b-128e-instruct\", \"xai.grok-4\"\n\n\nllm_client = OCIOpenAIHelper.get_langchain_openai_client(\n    model_name=LLM_MODEL,\n    config=scfg\n)\n\nprint(f\"Ready to send prompts to model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c714b0a",
   "metadata": {},
   "source": [
    "## Basic Chat Completion\n",
    "\n",
    "Send a single prompt and get a response from your selected LLM. Try different models for different styles and strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07883b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MESSAGE = \"\"\"\n",
    "    why is the sky blue? explain in 2 sentences like I am 5\n",
    "\"\"\"\n",
    "\n",
    "response = llm_client.invoke(MESSAGE)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd16632",
   "metadata": {},
   "source": [
    "### Model Performance Comparison Demo (Loop)\n",
    "\n",
    "Compare how different LLMs respond to the same input. This helps understand model behaviors and strengths.\n",
    "\n",
    "*Try adding/removing models from the list to test others!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8814e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_llms = [\n",
    "    \"openai.gpt-4.1\",\n",
    "    \"openai.gpt-5\",\n",
    "    \"meta.llama-4-maverick-17b-128e-instruct-fp8\",\n",
    "    \"meta.llama-4-scout-17b-16e-instruct\",\n",
    "    \"xai.grok-4\",\n",
    "    \"xai.grok-4-fast-non-reasoning\"\n",
    "]\n",
    "\n",
    "for llm_id in selected_llms:\n",
    "    print(f\"\\n\\n***** Chat Result for {llm_id} *****\")\n",
    "    llm_client.model_name = llm_id \n",
    "    start_time = time.time()\n",
    "    response = llm_client.invoke(MESSAGE)\n",
    "    end_time = time.time()\n",
    "    print(response)\n",
    "    print(f\"\\n Time taken for {llm_id}: {end_time - start_time:.2f} seconds\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb30741",
   "metadata": {},
   "source": [
    "## Batching & Output Control\n",
    "\n",
    "Sending multiple questions at once and managing the output with token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e1338",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"why is sky blue?\", \"why is it dark at night?\"]\n",
    "\n",
    "try:\n",
    "    responses = llm_client.batch(questions)\n",
    "    print(responses)\n",
    "except AttributeError:\n",
    "    batch_responses = [llm_client.invoke(q) for q in questions]\n",
    "    for q, r in zip(questions, batch_responses):\n",
    "        print(f\"Q: {q}\\nA: {r.content}\\n\")\n",
    "\n",
    "# Limit token output\n",
    "try:\n",
    "    llm_client.max_tokens = 10\n",
    "except AttributeError:\n",
    "    pass\n",
    "response = llm_client.invoke(MESSAGE)\n",
    "print(f\"\\n [with max lenth as 10]{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7729459c",
   "metadata": {},
   "source": [
    "## Prompting: System and User Roles\n",
    "\n",
    "LLMs can take structured conversation input. Use a `system` role to guide model behavior and a `user` role for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f45261",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\"role\": \"system\", \"content\": \"You are a poetic assistant who responds in exactly four lines.\"}\n",
    "user_message = {\"role\": \"user\", \"content\": \"What is the meaning of life?\"}\n",
    "messages = [system_message, user_message]\n",
    "llm_client.max_tokens = None\n",
    "response = llm_client.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf1206c",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "Some models and APIs support real-time (token-by-token) streaming. Use streaming for large outputs or real-time applications.\n",
    "\n",
    "Below is a demonstration. (You may need to adapt for your client class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f8407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming example \u2013 requires stream support in your OciOpenAILangChainClient.\n",
    "# For a complete version, see openai_oci_stream.py\n",
    "try:\n",
    "    for chunk in llm_client.stream(MESSAGE):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print()\n",
    "except AttributeError:\n",
    "    print(\"Streaming is not supported in this client. See openai_oci_stream.py for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a444e",
   "metadata": {},
   "source": [
    "## Async LLM Calls\n",
    "\n",
    "For power users: Invoke multiple LLMs concurrently using Python async/await. Useful for batch processing large numbers of queries.\n",
    "\n",
    "> For advanced usage, see `openai_oci_async.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Example: Asynchronously query multiple LLMs (pseudo-code)\n",
    "async def ask_model(model, prompt):\n",
    "    client = OCIOpenAIHelper.get_async_native_client(config=load_config(SANDBOX_CONFIG_FILE))\n",
    "    resp = await  client.responses.create(model=\"xai.grok-4\", input=prompt)\n",
    "    return (model, resp)\n",
    "\n",
    "async def main():\n",
    "    prompts = [\"Summarize the news today.\", \"Write a haiku about data science.\"]\n",
    "    tasks = [ask_model(m, prompts[i % len(prompts)]) for i, m in enumerate(selected_llms)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    for model, resp in results:\n",
    "        print(f\"Model: {model}\\nResponse: {resp}\\n\")\n",
    "\n",
    "# Uncomment to run (Jupyter support required):\n",
    "# asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc902c4",
   "metadata": {},
   "source": [
    "## Conversation History\n",
    "\n",
    "Maintaining message history allows LLMs to handle context and multi-turn conversations (see `openai_oci_history.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7244fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me something about Oracle.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Oracle is one of the largest vendors in enterprise IT, best known for its flagship database.\"}\n",
    "]\n",
    "\n",
    "current_query = {\"role\": \"user\", \"content\": \"What is its flagship product?\"}\n",
    "\n",
    "full_history = past_messages + [current_query]\n",
    "\n",
    "response = llm_client.invoke(full_history)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78b4bd",
   "metadata": {},
   "source": [
    "## Structured Output (JSON)\n",
    "\n",
    "Some LLMs support structured outputs, returning their answers as valid JSON. See `openai_oci_structured_output.py` for advanced schemas. Below is an example prompt designed to elicit a JSON response:\n",
    "\n",
    "> **Note:** Not all models/clients support output schemas or enforced JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "from langChain.openai_oci_client import OciOpenAILangGraphClient\n",
    "\n",
    "# Pydantic class schema\n",
    "class BookInventory(BaseModel):\n",
    "    \"\"\"Information about a book in a store or library inventory.\"\"\"\n",
    "    # Requires always a description\n",
    "\n",
    "    # Name of the field : type = Field description to help model generate\n",
    "    title: str = Field(description=\"Title of the book\")\n",
    "    author: str = Field(description=\"Author of the book\")\n",
    "    publication_year: int = Field(description=\"Publication year of the book\")\n",
    "    in_stock: bool = Field(description=\"True if the book is currently available in stock\")\n",
    "    copies_available: int = Field(ge=0, description=\"Number of copies available\")\n",
    "    price_usd: float = Field(ge=0.0, description=\"Price of the book in USD\")\n",
    "\n",
    "#llm_client = OciOpenAILangChainClient(\n",
    "llm_client = OciOpenAILangGraphClient(\n",
    "        profile=scfg['oci']['profile'],\n",
    "        compartment_id=scfg['oci']['compartment'],\n",
    "        model=LLM_MODEL,\n",
    "        service_endpoint=llm_service_endpoint\n",
    "    )\n",
    "composed_pydantic_model = llm_client.with_structured_output(BookInventory)\n",
    "\n",
    "MESSAGE = \"\"\"\n",
    "  Give me the information about the current science fiction books.\n",
    "\"\"\"\n",
    "\n",
    "response = composed_pydantic_model.invoke(MESSAGE)\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb45e9d",
   "metadata": {},
   "source": [
    "# Reasoning\n",
    "OpenAI responses api provides insights into LLM's reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OCIOpenAIHelper.get_langchain_openai_client(\n    model_name=\"openai.gpt-5\",\n    config=scfg,\n    use_responses_api=True,\n    reasoning={\"effort\": \"low\", \"summary\": \"auto\"},\n)\n\nmessages = [\n    (\n        \"system\",\n        \"You are an expert reasoning assistant. Explain your steps clearly, then provide a brief summary.\"\n    ),\n    (\n        \"human\",\n        \"If there are 21 apples and they are split equally among 3 friends, how many apples does each friend receive?\"\n    ),\n]\nres = llm.invoke(messages)\n\n# Answer\nprint(\"\\n=== Answer ===\")\nprint(getattr(res, \"content\", res))\n\n# Reasoning summary (if available)\nak = getattr(res, \"additional_kwargs\", {}) or {}\nsummary = ak.get(\"reasoning_summary\") or ak.get(\"summary\") or (ak.get(\"reasoning\") or {}).get(\"summary\")\nprint(\"\\n=== Reasoning Summary ===\")\nprint(summary if summary else \"N/A\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Try different models from the table above and compare their style, accuracy, and creativity.\n",
    "- Modify the prompt or system message to guide the LLM's behavior (e.g., \u201cReply in JSON\u201d, \u201cSummarize as bullet points\u201d).\n",
    "- Use batching and token limits to control response length.\n",
    "- Build a short multi-turn conversation and observe how history impacts results.\n",
    "- Try outputting structured JSON (if supported). \n",
    "- Share your findings or questions in the Slack channels above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work Ideas\n",
    "\n",
    "- Integrate the LLM with Slack for notifications or chatbots.\n",
    "- Explore OpenAI function calling and tool integrations.\n",
    "- Build pipelines or agents that chain together multiple LLM invocations.\n",
    "- Test performance and differences between streaming and batch modes.\n",
    "- Experiment with prompt engineering challenges (few-shot, chain-of-thought, etc).\n",
    "- Explore multimodal capabilities (images, audio, documents) using OCI GenAI.\n",
    "\n",
    "*Keep exploring and sharing!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}