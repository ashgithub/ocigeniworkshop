{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI & OCI LLMs with LangChain: Educational Demo\n",
    "\n",
    "Welcome! This notebook demonstrates the use of OpenAI-compatible LLMs (OpenAI, OCI, Meta, Groq, etc.) via LangChain.\n",
    "\n",
    "If you have questions, use the following Slack channels for support and collaboration:\n",
    "- `#generative-ai-users`\n",
    "- `#igiu-innovation-lab` (for peer interaction)\n",
    "- `#igiu-ai-learning` (if you encounter errors)\n",
    "\n",
    "*This notebook is designed for education: code cells are annotated with instructional blocks, and there are exercises throughout for active learning.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported LLM Models\n",
    "\n",
    "Here are model IDs you can use with the OCI OpenAI-compatible API (see [Oracle Docs](https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm) for updates):\n",
    "\n",
    "| Family | Example Models |\n",
    "|--------|----------------|\n",
    "| **OpenAI**   | openai.gpt-4.1, openai.gpt-4o, openai.gpt-5 |\n",
    "| **Meta (Llama)** | meta.llama-4-maverick-17b-128e-instruct, meta.llama-4-scout-17b-16e-instruct |\n",
    "| **Cohere** | cohere.command-a-03-2025, cohere.command-plus-latest |\n",
    "| **Groq/XAI** | xai.grok-3, xai.grok-4 |\n",
    "\n",
    "> **TIP:** Experiment with different models to compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment & Configuration\n",
    "\n",
    "Prepare your `sandbox.yaml` (or `sandbox.json`) config with the appropriate OCI credentials and compartment settings. Place it in your working directory or adjust the code below to point to its location.\n",
    "\n",
    "If you have trouble loading credentials or get permission errors, check the file path, format, and profile settings. For help, reach out on the Slack channels above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded successfully! Profile: INNOLAB-LEARNING\n"
     ]
    }
   ],
   "source": [
    "# Load OCI + Model Configuration\n",
    "import os,sys,time\n",
    "from envyaml import EnvYAML\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "\n",
    "load_dotenv()\n",
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from a YAML file.\"\"\"\n",
    "    try:\n",
    "        return EnvYAML(config_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Configuration file '{config_path}' not found.\")\n",
    "        return None\n",
    "\n",
    "scfg = load_config(SANDBOX_CONFIG_FILE)\n",
    "if scfg is not None:\n",
    "    print(\"Config loaded successfully! Profile:\", scfg['oci']['profile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up your LLM client\n",
    "\n",
    "- Specify your target model (`LLM_MODEL`).\n",
    "- Choose the appropriate service endpoint.\n",
    "- Use the config (`scfg`) loaded above for credentials.\n",
    "\n",
    "You may need to re-instantiate the client when switching models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to send prompts to model: openai.gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "# Add the parent directory of the notebook to sys.path\n",
    "from langChain.openai_oci_client import OciOpenAILangChainClient\n",
    "\n",
    "LLM_MODEL = \"openai.gpt-4.1\"\n",
    "# Other ideas: \"meta.llama-4-maverick-17b-128e-instruct\", \"xai.grok-4\"\n",
    "llm_service_endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "llm_client = OciOpenAILangChainClient(\n",
    "    profile=scfg['oci']['profile'],\n",
    "    compartment_id=scfg['oci']['compartment'],\n",
    "    model=LLM_MODEL,\n",
    "    service_endpoint=llm_service_endpoint,\n",
    ")\n",
    "\n",
    "print(f\"Ready to send prompts to model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chat Completion\n",
    "\n",
    "Send a single prompt and get a response from your selected LLM. Try different models for different styles and strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The sky looks blue because sunlight gets scattered in all directions by the air, and blue light scatters more than the other colors. That’s why we see a big blue sky during the day!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 26, 'total_tokens': 65, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'openai.gpt-4.1', 'system_fingerprint': 'fp_d38c7f4fa7', 'id': 'chatcmpl-CZ2msVY0iUWcbHLO6qoaDjP11Xx1s', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--b2d0d692-58db-43d3-b533-591c7ace6bac-0' usage_metadata={'input_tokens': 26, 'output_tokens': 39, 'total_tokens': 65, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "MESSAGE = \"\"\"\n",
    "    why is the sky blue? explain in 2 sentences like I am 5\n",
    "\"\"\"\n",
    "\n",
    "response = llm_client.invoke(MESSAGE)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection Demo (Loop)\n",
    "\n",
    "Compare how different LLMs respond to the same input. This helps understand model behaviors and strengths.\n",
    "\n",
    "*Try adding/removing models from the list to test others!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Chat Result for openai.gpt-4.1 *****\n",
      "content='The sky looks blue because sunlight gets scattered by the air, and blue light scatters more than other colors. That’s why when you look up, your eyes see mostly the blue light.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 26, 'total_tokens': 64, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'openai.gpt-4.1', 'system_fingerprint': 'fp_d38c7f4fa7', 'id': 'chatcmpl-CZ2mviyCB2BJaUz571OESlEuvNjlZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--538ca80b-0eb9-42bc-ab9e-b422c8edbd98-0' usage_metadata={'input_tokens': 26, 'output_tokens': 38, 'total_tokens': 64, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      " Time taken for openai.gpt-4.1: 1.42 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** Chat Result for openai.gpt-5 *****\n",
      "content='Sunlight has many colors, and tiny bits in the air scatter the blue light more than the others. Because blue gets spread around the most, the sky looks blue to our eyes.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 238, 'prompt_tokens': 25, 'total_tokens': 263, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'openai.gpt-5', 'system_fingerprint': None, 'id': 'chatcmpl-CZ2mxuMbM6WikOIgeyx7iSWIFZ7JF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--25a2f9bc-a605-4f48-896a-3c760edfbdd6-0' usage_metadata={'input_tokens': 25, 'output_tokens': 238, 'total_tokens': 263, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}}\n",
      "\n",
      " Time taken for openai.gpt-5: 5.53 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** Chat Result for meta.llama-4-maverick-17b-128e-instruct-fp8 *****\n",
      "content=\"The sky is blue because when sunlight comes into our atmosphere, it scatters and the blue light bounces around more than the other colors, so our eyes see blue! It's like when you mix a little bit of blue paint with lots of other colors, the blue is what we notice the most, and that's what's happening with the sunlight in the sky.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 29, 'total_tokens': 100, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta.llama-4-maverick-17b-128e-instruct-fp8', 'system_fingerprint': None, 'id': '67917513cdd84c4aa01f3bc0899cadf1', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--d9d2ccd2-9be7-451f-8da3-343298e8ce6c-0' usage_metadata={'input_tokens': 29, 'output_tokens': 71, 'total_tokens': 100, 'input_token_details': {}, 'output_token_details': {}}\n",
      "\n",
      " Time taken for meta.llama-4-maverick-17b-128e-instruct-fp8: 0.71 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** Chat Result for meta.llama-4-scout-17b-16e-instruct *****\n",
      "content=\"The sky looks blue because when sunlight comes into our atmosphere, it scatters everywhere and the blue light scatters more than any other color, so that's what we see! It's like when you mix all the colors of the rainbow together, the blue one is the one that bounces around the most and reaches our eyes!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 29, 'total_tokens': 93, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta.llama-4-scout-17b-16e-instruct', 'system_fingerprint': None, 'id': 'b58b706ae03c46709d34f73491f40622', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--32797652-37e1-4373-ab03-66f000de2b56-0' usage_metadata={'input_tokens': 29, 'output_tokens': 64, 'total_tokens': 93, 'input_token_details': {}, 'output_token_details': {}}\n",
      "\n",
      " Time taken for meta.llama-4-scout-17b-16e-instruct: 0.72 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** Chat Result for xai.grok-4 *****\n",
      "content=\"The sky looks blue because sunlight is made up of all the colors of the rainbow, and when it hits tiny bits of air way up high, the blue color bounces around more than the others and reaches our eyes. That's why during the day, the sky seems like a big blue blanket!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 703, 'total_tokens': 898, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 137, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 702, 'text_tokens': 703, 'image_tokens': 0}, 'num_sources_used': 0}, 'model_name': 'xai.grok-4', 'system_fingerprint': 'fp_19b22b3eaa', 'id': 'f4dcce00-6239-2dd5-62eb-466d57c8e6a1', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--30190f1e-5c87-4efc-9d86-41803b1bd29c-0' usage_metadata={'input_tokens': 703, 'output_tokens': 58, 'total_tokens': 898, 'input_token_details': {'audio': 0, 'cache_read': 702}, 'output_token_details': {'audio': 0, 'reasoning': 137}}\n",
      "\n",
      " Time taken for xai.grok-4: 3.23 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** Chat Result for xai.grok-4-fast-non-reasoning *****\n",
      "content=\"The sky looks blue because sunlight is made up of all the colors of the rainbow, but when it hits tiny bits of air, the blue part scatters all over the place more than the other colors. That's why we see mostly blue when we look up on a clear day!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 187, 'total_tokens': 243, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 156, 'text_tokens': 187, 'image_tokens': 0}, 'num_sources_used': 0}, 'model_name': 'xai.grok-4-fast-non-reasoning', 'system_fingerprint': 'fp_040427a672', 'id': 'fead51bb-6261-1ae8-1393-f778dde09c53', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--cdc25e4e-fc52-44dd-a581-d4121c05f233-0' usage_metadata={'input_tokens': 187, 'output_tokens': 56, 'total_tokens': 243, 'input_token_details': {'audio': 0, 'cache_read': 156}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      " Time taken for xai.grok-4-fast-non-reasoning: 0.87 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_llms = [\n",
    "    \"openai.gpt-4.1\",\n",
    "    \"openai.gpt-5\",\n",
    "    \"meta.llama-4-maverick-17b-128e-instruct-fp8\",\n",
    "    \"meta.llama-4-scout-17b-16e-instruct\",\n",
    "    \"xai.grok-4\",\n",
    "    \"xai.grok-4-fast-non-reasoning\"\n",
    "]\n",
    "\n",
    "for llm_id in selected_llms:\n",
    "    print(f\"\\n\\n***** Chat Result for {llm_id} *****\")\n",
    "    temp_client = OciOpenAILangChainClient(\n",
    "        profile=scfg['oci']['profile'],\n",
    "        compartment_id=scfg['oci']['compartment'],\n",
    "        model=llm_id,\n",
    "        service_endpoint=llm_service_endpoint\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    response = temp_client.invoke(MESSAGE)\n",
    "    end_time = time.time()\n",
    "    print(response)\n",
    "    print(f\"\\n Time taken for {llm_id}: {end_time - start_time:.2f} seconds\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching & Output Control\n",
    "\n",
    "Sending multiple questions at once and managing the output with token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='The sky appears blue because of a phenomenon called **', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 12, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'openai.gpt-4.1', 'system_fingerprint': 'fp_d38c7f4fa7', 'id': 'chatcmpl-CZ2sMQi3LgwR4EKGLZMVoE3tK6Mqm', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None}, id='run--586d055d-1745-43ad-8636-d2377b8afe18-0', usage_metadata={'input_tokens': 12, 'output_tokens': 10, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='It is dark at night because the part of Earth', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 14, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'openai.gpt-4.1', 'system_fingerprint': 'fp_d38c7f4fa7', 'id': 'chatcmpl-CZ2sMrdAgVt2lcZAuHwxcwHO9AoTr', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None}, id='run--5882473a-2178-427b-bd32-c5be81ab3e0d-0', usage_metadata={'input_tokens': 14, 'output_tokens': 10, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      " [with max lenth as 10]content='The sky looks blue because sunlight gets scattered in all' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'openai.gpt-4.1', 'system_fingerprint': 'fp_d38c7f4fa7', 'id': 'chatcmpl-CZ2sOlyTbA6eR3osiwQ2asMWjFYjs', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None} id='run--5a214f25-e557-42e5-b37a-1111bc67c890-0' usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "questions = [\"why is sky blue?\", \"why is it dark at night?\"]\n",
    "\n",
    "try:\n",
    "    responses = llm_client.batch(questions)\n",
    "    print(responses)\n",
    "except AttributeError:\n",
    "    batch_responses = [llm_client.invoke(q) for q in questions]\n",
    "    for q, r in zip(questions, batch_responses):\n",
    "        print(f\"Q: {q}\\nA: {r.content}\\n\")\n",
    "\n",
    "# Limit token output\n",
    "try:\n",
    "    llm_client.max_tokens = 10\n",
    "except AttributeError:\n",
    "    pass\n",
    "response = llm_client.invoke(MESSAGE)\n",
    "print(f\"\\n [with max lenth as 10]{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting: System and User Roles\n",
    "\n",
    "LLMs can take structured conversation input. Use a `system` role to guide model behavior and a `user` role for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The meaning of life is a question vast and deep,  \\nA dance of joy and sorrow, promises to keep.  \\nTo love, to learn, to wonder, and to strive—  \\nIn every heartbeat, find reasons to be alive.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 30, 'total_tokens': 78, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'openai.gpt-4.1', 'system_fingerprint': 'fp_d38c7f4fa7', 'id': 'chatcmpl-CZ2vAUIbURjfAGC75byND6bWdsqTH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--86d58f64-c2fd-4af6-906f-811cc89d51b2-0' usage_metadata={'input_tokens': 30, 'output_tokens': 48, 'total_tokens': 78, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "system_message = {\"role\": \"system\", \"content\": \"You are a poetic assistant who responds in exactly four lines.\"}\n",
    "user_message = {\"role\": \"user\", \"content\": \"What is the meaning of life?\"}\n",
    "messages = [system_message, user_message]\n",
    "llm_client.max_tokens = None\n",
    "response = llm_client.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "Some models and APIs support real-time (token-by-token) streaming. Use streaming for large outputs or real-time applications.\n",
    "\n",
    "Below is a demonstration. (You may need to adapt for your client class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky looks blue because sunlight gets spread out in all directions by the air, and blue light spreads more than the other colors. That’s why our eyes see the sky as blue most of the time!\n"
     ]
    }
   ],
   "source": [
    "# Streaming example – requires stream support in your OciOpenAILangChainClient.\n",
    "# For a complete version, see openai_oci_stream.py\n",
    "try:\n",
    "    for chunk in llm_client.stream(MESSAGE):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print()\n",
    "except AttributeError:\n",
    "    print(\"Streaming is not supported in this client. See openai_oci_stream.py for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async LLM Calls\n",
    "\n",
    "For power users: Invoke multiple LLMs concurrently using Python async/await. Useful for batch processing large numbers of queries.\n",
    "\n",
    "> For advanced usage, see `openai_oci_async.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Example: Asynchronously query multiple LLMs (pseudo-code)\n",
    "async def ask_model(model, prompt):\n",
    "    client = OciOpenAILangChainClient(\n",
    "        profile=scfg['oci']['profile'],\n",
    "        compartment_id=scfg['oci']['compartment'],\n",
    "        model=model,\n",
    "        service_endpoint=llm_service_endpoint\n",
    "    )\n",
    "    resp = client.invoke(prompt)\n",
    "    return (model, resp)\n",
    "\n",
    "async def main():\n",
    "    prompts = [\"Summarize the news today.\", \"Write a haiku about data science.\"]\n",
    "    tasks = [ask_model(m, prompts[i % len(prompts)]) for i, m in enumerate(selected_llms)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    for model, resp in results:\n",
    "        print(f\"Model: {model}\\nResponse: {resp}\\n\")\n",
    "\n",
    "# Uncomment to run (Jupyter support required):\n",
    "# asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation History\n",
    "\n",
    "Maintaining message history allows LLMs to handle context and multi-turn conversations (see `openai_oci_history.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle’s flagship product is the **Oracle Database**. It is a widely used, multi-model relational database management system (RDBMS) known for its scalability, reliability, and robust features, and is commonly used by enterprises around the world to store, manage, and retrieve large volumes of data.\n"
     ]
    }
   ],
   "source": [
    "past_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me something about Oracle.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Oracle is one of the largest vendors in enterprise IT, best known for its flagship database.\"}\n",
    "]\n",
    "\n",
    "current_query = {\"role\": \"user\", \"content\": \"What is its flagship product?\"}\n",
    "\n",
    "full_history = past_messages + [current_query]\n",
    "\n",
    "response = llm_client.invoke(full_history)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output (JSON)\n",
    "\n",
    "Some LLMs support structured outputs, returning their answers as valid JSON. See `openai_oci_structured_output.py` for advanced schemas. Below is an example prompt designed to elicit a JSON response:\n",
    "\n",
    "> **Note:** Not all models/clients support output schemas or enforced JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='The Terraformers' author='Annalee Newitz' publication_year=2023 in_stock=True copies_available=8 price_usd=27.99\n",
      "<class '__main__.BookInventory'>\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "from langChain.openai_oci_client import OciOpenAILangGraphClient\n",
    "\n",
    "# Pydantic class schema\n",
    "class BookInventory(BaseModel):\n",
    "    \"\"\"Information about a book in a store or library inventory.\"\"\"\n",
    "    # Requires always a description\n",
    "\n",
    "    # Name of the field : type = Field description to help model generate\n",
    "    title: str = Field(description=\"Title of the book\")\n",
    "    author: str = Field(description=\"Author of the book\")\n",
    "    publication_year: int = Field(description=\"Publication year of the book\")\n",
    "    in_stock: bool = Field(description=\"True if the book is currently available in stock\")\n",
    "    copies_available: int = Field(ge=0, description=\"Number of copies available\")\n",
    "    price_usd: float = Field(ge=0.0, description=\"Price of the book in USD\")\n",
    "\n",
    "#llm_client = OciOpenAILangChainClient(\n",
    "llm_client = OciOpenAILangGraphClient(\n",
    "        profile=scfg['oci']['profile'],\n",
    "        compartment_id=scfg['oci']['compartment'],\n",
    "        model=LLM_MODEL,\n",
    "        service_endpoint=llm_service_endpoint\n",
    "    )\n",
    "composed_pydantic_model = llm_client.with_structured_output(BookInventory)\n",
    "\n",
    "MESSAGE = \"\"\"\n",
    "  Give me the information about the current science fiction books.\n",
    "\"\"\"\n",
    "\n",
    "response = composed_pydantic_model.invoke(MESSAGE)\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Try different models from the table above and compare their style, accuracy, and creativity.\n",
    "- Modify the prompt or system message to guide the LLM's behavior (e.g., “Reply in JSON”, “Summarize as bullet points”).\n",
    "- Use batching and token limits to control response length.\n",
    "- Build a short multi-turn conversation and observe how history impacts results.\n",
    "- Try outputting structured JSON (if supported). \n",
    "- Share your findings or questions in the Slack channels above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work Ideas\n",
    "\n",
    "- Integrate the LLM with Slack for notifications or chatbots.\n",
    "- Explore OpenAI function calling and tool integrations.\n",
    "- Build pipelines or agents that chain together multiple LLM invocations.\n",
    "- Test performance and differences between streaming and batch modes.\n",
    "- Experiment with prompt engineering challenges (few-shot, chain-of-thought, etc).\n",
    "- Explore multimodal capabilities (images, audio, documents) using OCI GenAI.\n",
    "\n",
    "*Keep exploring and sharing!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
