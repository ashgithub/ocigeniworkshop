{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI & OCI LLMs with LangChain: Educational Demo\n",
    "\n",
    "Welcome! This notebook demonstrates the use of OpenAI-compatible LLMs (OpenAI, OCI, Meta, Groq, etc.) via LangChain.\n",
    "\n",
    "If you have questions, use the following Slack channels for support and collaboration:\n",
    "- `#generative-ai-users`\n",
    "- `#igiu-innovation-lab` (for peer interaction)\n",
    "- `#igiu-ai-learning` (if you encounter errors)\n",
    "\n",
    "*This notebook is designed for education: code cells are annotated with instructional blocks, and there are exercises throughout for active learning.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported LLM Models\n",
    "\n",
    "Here are model IDs you can use with the OCI OpenAI-compatible API (see [Oracle Docs](https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm) for updates):\n",
    "\n",
    "| Family | Example Models |\n",
    "|--------|----------------|\n",
    "| **OpenAI**   | openai.gpt-4.1, openai.gpt-4o, openai.gpt-5 |\n",
    "| **Meta (Llama)** | meta.llama-4-maverick-17b-128e-instruct, meta.llama-4-scout-17b-16e-instruct |\n",
    "| **Cohere** | cohere.command-a-03-2025, cohere.command-plus-latest |\n",
    "| **Groq/XAI** | xai.grok-3, xai.grok-4 |\n",
    "\n",
    "> **TIP:** Experiment with different models to compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment & Configuration\n",
    "\n",
    "Prepare your `sandbox.yaml` (or `sandbox.json`) config with the appropriate OCI credentials and compartment settings. Place it in your working directory or adjust the code below to point to its location.\n",
    "\n",
    "If you have trouble loading credentials or get permission errors, check the file path, format, and profile settings. For help, reach out on the Slack channels above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd14257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded successfully! Profile: INNOLAB-LEARNING\n"
     ]
    }
   ],
   "source": [
    "# Load OCI + Model Configuration\n",
    "import os,sys,time\n",
    "from envyaml import EnvYAML\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "\n",
    "load_dotenv()\n",
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from a YAML file.\"\"\"\n",
    "    try:\n",
    "        return EnvYAML(config_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Configuration file '{config_path}' not found.\")\n",
    "        return None\n",
    "\n",
    "scfg = load_config(SANDBOX_CONFIG_FILE)\n",
    "if scfg is not None:\n",
    "    print(\"Config loaded successfully! Profile:\", scfg['oci']['profile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b789bb",
   "metadata": {},
   "source": [
    "### Set up your LLM client\n",
    "\n",
    "- Specify your target model (`LLM_MODEL`).\n",
    "- Choose the appropriate service endpoint.\n",
    "- Use the config (`scfg`) loaded above for credentials.\n",
    "\n",
    "You may need to re-instantiate the client when switching models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4505f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to send prompts to model: openai.gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "# Add the parent directory of the notebook to sys.path\n",
    "from langChain.oci_openai_helper import OCIOpenAIHelper\n",
    "\n",
    "LLM_MODEL = \"openai.gpt-4.1\"\n",
    "# Other ideas: \"meta.llama-4-maverick-17b-128e-instruct\", \"xai.grok-4\"\n",
    "llm_service_endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "llm_client = OCIOpenAIHelper.get_client(\n",
    "    model_name=LLM_MODEL,\n",
    "    config=scfg\n",
    ")\n",
    "\n",
    "print(f\"Ready to send prompts to model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c714b0a",
   "metadata": {},
   "source": [
    "## Basic Chat Completion\n",
    "\n",
    "Send a single prompt and get a response from your selected LLM. Try different models for different styles and strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e07883b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The sky looks blue because sunlight gets scattered by the air, and blue light scatters more than other colors. That’s why, when you look up, you see a blue sky!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 26, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'openai.gpt-4.1', 'system_fingerprint': 'fp_d38c7f4fa7', 'id': 'chatcmpl-CZAoGCb1faSp1P1X38YKopSSwusR7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--a79e3444-3150-4d83-9f30-46b8d7accc7d-0' usage_metadata={'input_tokens': 26, 'output_tokens': 37, 'total_tokens': 63, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "MESSAGE = \"\"\"\n",
    "    why is the sky blue? explain in 2 sentences like I am 5\n",
    "\"\"\"\n",
    "\n",
    "response = llm_client.invoke(MESSAGE)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd16632",
   "metadata": {},
   "source": [
    "### Model Selection Demo (Loop)\n",
    "\n",
    "Compare how different LLMs respond to the same input. This helps understand model behaviors and strengths.\n",
    "\n",
    "*Try adding/removing models from the list to test others!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c8814e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Chat Result for openai.gpt-4.1 *****\n",
      "content='The sky looks blue because sunlight gets spread out in the air, and blue light scatters more than other colors. So, when you look up, you see more blue than any other color!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 26, 'total_tokens': 65, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'openai.gpt-4.1', 'system_fingerprint': 'fp_d38c7f4fa7', 'id': 'chatcmpl-CZAoK0alEjFrhddYVwhkbFpc0SUIv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--a9e85b3d-9698-4bf5-95df-02d8d8036296-0' usage_metadata={'input_tokens': 26, 'output_tokens': 39, 'total_tokens': 65, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      " Time taken for openai.gpt-4.1: 1.55 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** Chat Result for openai.gpt-5 *****\n",
      "content='Sunlight has many colors, and the tiny bits of air spread the blue color more than the others. So when you look up, the blue color is what reaches your eyes, making the sky look blue.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 179, 'prompt_tokens': 25, 'total_tokens': 204, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'openai.gpt-5', 'system_fingerprint': None, 'id': 'chatcmpl-CZAoL7BtU56f3orVW3d348P6Aiqbc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--a03d1ba4-3e6a-44ad-8106-b662cc8072c2-0' usage_metadata={'input_tokens': 25, 'output_tokens': 179, 'total_tokens': 204, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}\n",
      "\n",
      " Time taken for openai.gpt-5: 5.17 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** Chat Result for meta.llama-4-maverick-17b-128e-instruct-fp8 *****\n",
      "content=\"The sky is blue because when sunlight comes into our atmosphere, it scatters and the blue light bounces around more than the other colors, making the sky look blue! It's like when you shine a flashlight through a jar of water and it makes the water look blue, it's kind of like that, but with the whole air around us!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 29, 'total_tokens': 98, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta.llama-4-maverick-17b-128e-instruct-fp8', 'system_fingerprint': None, 'id': '7e71b29d3c72455580740c745830a696', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--4a4b7989-aa93-4b36-a092-cc9573f06715-0' usage_metadata={'input_tokens': 29, 'output_tokens': 69, 'total_tokens': 98, 'input_token_details': {}, 'output_token_details': {}}\n",
      "\n",
      " Time taken for meta.llama-4-maverick-17b-128e-instruct-fp8: 0.52 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** Chat Result for meta.llama-4-scout-17b-16e-instruct *****\n",
      "content=\"The sky looks blue because when sunlight comes into our atmosphere, it scatters everywhere and the blue light gets scattered more than any other color, so that's what we see! It's like when you mix all the colors of the rainbow together, and the blue one is the one that bounces around the most and reaches our eyes!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 29, 'total_tokens': 94, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta.llama-4-scout-17b-16e-instruct', 'system_fingerprint': None, 'id': 'ec8427617f394e238a509e63881ee34b', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--f22e16e5-fd25-4def-a849-60f7184f2dbd-0' usage_metadata={'input_tokens': 29, 'output_tokens': 65, 'total_tokens': 94, 'input_token_details': {}, 'output_token_details': {}}\n",
      "\n",
      " Time taken for meta.llama-4-scout-17b-16e-instruct: 1.18 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** Chat Result for xai.grok-4 *****\n",
      "content=\"The sun's light is like a big mix of colors, and when it shines through the air way up high, the blue color bounces around a lot more than the others, making the whole sky look blue to us. That's why we see blue during the day, but at sunset, the other colors like red and orange get their turn when the sun is low!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 703, 'total_tokens': 958, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 182, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 702, 'text_tokens': 703, 'image_tokens': 0}, 'num_sources_used': 0}, 'model_name': 'xai.grok-4', 'system_fingerprint': 'fp_19b22b3eaa', 'id': 'fcf1c439-4b4b-cf25-64be-158b634d12c3', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--6308a4d1-6d65-4b41-a672-5ed933b47277-0' usage_metadata={'input_tokens': 703, 'output_tokens': 73, 'total_tokens': 958, 'input_token_details': {'audio': 0, 'cache_read': 702}, 'output_token_details': {'audio': 0, 'reasoning': 182}}\n",
      "\n",
      " Time taken for xai.grok-4: 3.81 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** Chat Result for xai.grok-4-fast-non-reasoning *****\n",
      "content=\"The sky looks blue because sunlight is made up of all colors, but when it hits tiny bits of air, the blue part bounces around a lot more than the other colors, so we see mostly blue when we look up. It's like how a blue toy shines brighter in the light than a red one!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 187, 'total_tokens': 248, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 163, 'text_tokens': 187, 'image_tokens': 0}, 'num_sources_used': 0}, 'model_name': 'xai.grok-4-fast-non-reasoning', 'system_fingerprint': 'fp_040427a672', 'id': '8b1296c3-359c-03ab-f07b-d25d3ef2ece3', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--9c644a38-871f-4af9-95ae-fce35ef9e62f-0' usage_metadata={'input_tokens': 187, 'output_tokens': 61, 'total_tokens': 248, 'input_token_details': {'audio': 0, 'cache_read': 163}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      " Time taken for xai.grok-4-fast-non-reasoning: 0.49 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_llms = [\n",
    "    \"openai.gpt-4.1\",\n",
    "    \"openai.gpt-5\",\n",
    "    \"meta.llama-4-maverick-17b-128e-instruct-fp8\",\n",
    "    \"meta.llama-4-scout-17b-16e-instruct\",\n",
    "    \"xai.grok-4\",\n",
    "    \"xai.grok-4-fast-non-reasoning\"\n",
    "]\n",
    "\n",
    "for llm_id in selected_llms:\n",
    "    print(f\"\\n\\n***** Chat Result for {llm_id} *****\")\n",
    "    llm_client.model_name = llm_id \n",
    "    start_time = time.time()\n",
    "    response = llm_client.invoke(MESSAGE)\n",
    "    end_time = time.time()\n",
    "    print(response)\n",
    "    print(f\"\\n Time taken for {llm_id}: {end_time - start_time:.2f} seconds\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb30741",
   "metadata": {},
   "source": [
    "## Batching & Output Control\n",
    "\n",
    "Sending multiple questions at once and managing the output with token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e7e1338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content=\"The sky appears blue during the day due to a phenomenon called Rayleigh scattering. Here's a concise explanation, backed by physics:\\n\\n### The Science Behind It\\n- **Sunlight and Atmosphere**: Sunlight is white light, made up of all colors of the visible spectrum (red, orange, yellow, green, blue, indigo, violet). As it enters Earth's atmosphere, it interacts with air molecules (mostly nitrogen and oxygen), which are much smaller than the wavelengths of visible light.\\n  \\n- **Scattering Effect**: These molecules scatter the light in all directions. Shorter wavelengths (like blue and violet) scatter much more efficiently than longer ones (like red and orange) because of Rayleigh scattering—the intensity of scattered light is inversely proportional to the fourth power of the wavelength (I ∝ 1/λ⁴). Blue light (wavelength ~450 nm) scatters about 10 times more than red light (~650 nm).\\n\\n- **Why Blue, Not Violet?**: Violet light scatters even more than blue, but our eyes are less sensitive to violet, and the sun emits less violet light to begin with. Plus, some violet is absorbed by the upper atmosphere. The result? The scattered blue light dominates what reaches our eyes from all directions in the sky.\\n\\n- **Evidence and Observations**: This was first explained by Lord Rayleigh (John William Strutt) in the 1870s, building on work by John Tyndall. It's why sunsets are red—the longer wavelengths travel farther through the thicker atmospheric path at dawn/dusk, while blue is scattered away. Astronauts in space see a black sky because there's no atmosphere to scatter light. Experiments with lasers or even simple cloud chambers confirm the wavelength dependence.\\n\\nIf the atmosphere had no particles, the sky would be black like on the Moon. Dust, pollution, or volcanic ash can alter this, making skies hazy or other colors. For a deeper dive, check out resources from NASA or physics textbooks on atmospheric optics.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 396, 'prompt_tokens': 173, 'total_tokens': 569, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 161, 'text_tokens': 173, 'image_tokens': 0}, 'num_sources_used': 0}, 'model_name': 'xai.grok-4-fast-non-reasoning', 'system_fingerprint': 'fp_040427a672', 'id': '87af0164-64ff-5008-5632-7793bccd9896', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--33db91ba-3b4b-4eef-8c35-9037984f3683-0', usage_metadata={'input_tokens': 173, 'output_tokens': 396, 'total_tokens': 569, 'input_token_details': {'audio': 0, 'cache_read': 161}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content=\"### The Basics of Day and Night\\nNighttime darkness is primarily caused by Earth's rotation on its axis. Earth spins once every 24 hours, creating a day-night cycle. When your location on Earth faces away from the Sun, it's night; when it faces toward the Sun, it's day. This rotation means sunlight doesn't reach the entire planet at once—half is always in daylight, and the other half in darkness.\\n\\n### Why So Dark? The Role of the Sun\\nThe Sun is our primary light source, emitting vast amounts of energy as visible light and other wavelengths. During the day, its direct rays illuminate the surface, scattering light in the atmosphere to create a bright sky. At night:\\n- **No Direct Sunlight**: The side of Earth you're on is tilted away from the Sun, so no direct rays hit it.\\n- **Atmospheric Scattering Stops**: Without sunlight, there's little light to scatter, making the sky appear black (or deep blue in some cases).\\n- **Starlight and Moonlight Are Weak**: Stars and the Moon provide some light, but it's minuscule compared to the Sun—about a billion times dimmer. The Sun's output is roughly 3.8 × 10²⁶ watts, while a full Moon reflects only about 0.0001% of that.\\n\\n### Evidence from Astronomy\\nThis is well-established in heliocentric models, confirmed by observations like those from NASA's satellites (e.g., DSCOVR) showing Earth's day-night terminator line in real-time images. Ancient astronomers like Ptolemy noted the cycle, but Copernicus and Galileo solidified the rotation explanation in the 16th-17th centuries using telescopes to observe planetary motions.\\n\\nIn short, it's dark at night because the Sun isn't shining on your half of the planet—simple geometry of a spinning sphere in space. If Earth didn't rotate, we'd have eternal day or night depending on our position.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 382, 'prompt_tokens': 175, 'total_tokens': 557, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 161, 'text_tokens': 175, 'image_tokens': 0}, 'num_sources_used': 0}, 'model_name': 'xai.grok-4-fast-non-reasoning', 'system_fingerprint': 'fp_040427a672', 'id': '6f61ef3c-ced2-f1fe-3fbc-ca043c88c06c', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--e84697a6-8fc4-4697-a00b-c8c74f368122-0', usage_metadata={'input_tokens': 175, 'output_tokens': 382, 'total_tokens': 557, 'input_token_details': {'audio': 0, 'cache_read': 161}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      " [with max lenth as 10]content='The sky looks blue because sunlight is made up of' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 187, 'total_tokens': 197, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 162, 'text_tokens': 187, 'image_tokens': 0}, 'num_sources_used': 0}, 'model_name': 'xai.grok-4-fast-non-reasoning', 'system_fingerprint': 'fp_040427a672', 'id': '901fbb64-8a6d-7135-2189-c5e22a64ee14', 'service_tier': None, 'finish_reason': 'length', 'logprobs': None} id='run--1206c4dd-0d33-4955-ac35-c43a37627c58-0' usage_metadata={'input_tokens': 187, 'output_tokens': 10, 'total_tokens': 197, 'input_token_details': {'audio': 0, 'cache_read': 162}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "questions = [\"why is sky blue?\", \"why is it dark at night?\"]\n",
    "\n",
    "try:\n",
    "    responses = llm_client.batch(questions)\n",
    "    print(responses)\n",
    "except AttributeError:\n",
    "    batch_responses = [llm_client.invoke(q) for q in questions]\n",
    "    for q, r in zip(questions, batch_responses):\n",
    "        print(f\"Q: {q}\\nA: {r.content}\\n\")\n",
    "\n",
    "# Limit token output\n",
    "try:\n",
    "    llm_client.max_tokens = 10\n",
    "except AttributeError:\n",
    "    pass\n",
    "response = llm_client.invoke(MESSAGE)\n",
    "print(f\"\\n [with max lenth as 10]{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7729459c",
   "metadata": {},
   "source": [
    "## Prompting: System and User Roles\n",
    "\n",
    "LLMs can take structured conversation input. Use a `system` role to guide model behavior and a `user` role for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f45261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"In the vast cosmic dance, life's meaning unfolds,  \\nA quest for connection, where hearts and stars entwine.  \\nIt's love's quiet whisper in the soul's hidden folds,  \\nAnd purpose we craft, one breath at a time.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 187, 'total_tokens': 235, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 160, 'text_tokens': 187, 'image_tokens': 0}, 'num_sources_used': 0}, 'model_name': 'xai.grok-4-fast-non-reasoning', 'system_fingerprint': 'fp_040427a672', 'id': '585a6fb0-f190-f2e4-3a21-0a295fa834b9', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--56bb76f4-c580-4c07-b47b-c86380291487-0' usage_metadata={'input_tokens': 187, 'output_tokens': 48, 'total_tokens': 235, 'input_token_details': {'audio': 0, 'cache_read': 160}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "system_message = {\"role\": \"system\", \"content\": \"You are a poetic assistant who responds in exactly four lines.\"}\n",
    "user_message = {\"role\": \"user\", \"content\": \"What is the meaning of life?\"}\n",
    "messages = [system_message, user_message]\n",
    "llm_client.max_tokens = None\n",
    "response = llm_client.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf1206c",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "Some models and APIs support real-time (token-by-token) streaming. Use streaming for large outputs or real-time applications.\n",
    "\n",
    "Below is a demonstration. (You may need to adapt for your client class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f6f8407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky looks blue because sunlight is made up of all colors, but when it hits tiny bits of air, the blue part scatters everywhere more than the other colors. That's why we see mostly blue when we look up on a clear day!\n"
     ]
    }
   ],
   "source": [
    "# Streaming example – requires stream support in your OciOpenAILangChainClient.\n",
    "# For a complete version, see openai_oci_stream.py\n",
    "try:\n",
    "    for chunk in llm_client.stream(MESSAGE):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print()\n",
    "except AttributeError:\n",
    "    print(\"Streaming is not supported in this client. See openai_oci_stream.py for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a444e",
   "metadata": {},
   "source": [
    "## Async LLM Calls\n",
    "\n",
    "For power users: Invoke multiple LLMs concurrently using Python async/await. Useful for batch processing large numbers of queries.\n",
    "\n",
    "> For advanced usage, see `openai_oci_async.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Example: Asynchronously query multiple LLMs (pseudo-code)\n",
    "async def ask_model(model, prompt):\n",
    "    client = OCIOpenAIHelper.get_async_native_client(config=load_config(SANDBOX_CONFIG_FILE))\n",
    "    resp = await  client.responses.create(model=\"xai.grok-4\", input=prompt)\n",
    "    return (model, resp)\n",
    "\n",
    "async def main():\n",
    "    prompts = [\"Summarize the news today.\", \"Write a haiku about data science.\"]\n",
    "    tasks = [ask_model(m, prompts[i % len(prompts)]) for i, m in enumerate(selected_llms)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    for model, resp in results:\n",
    "        print(f\"Model: {model}\\nResponse: {resp}\\n\")\n",
    "\n",
    "# Uncomment to run (Jupyter support required):\n",
    "# asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc902c4",
   "metadata": {},
   "source": [
    "## Conversation History\n",
    "\n",
    "Maintaining message history allows LLMs to handle context and multi-turn conversations (see `openai_oci_history.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7244fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle's flagship product is its relational database management system, Oracle Database (often just called Oracle DB). It's a powerful, scalable platform used for storing, managing, and retrieving large volumes of data in enterprise environments.\n"
     ]
    }
   ],
   "source": [
    "past_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me something about Oracle.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Oracle is one of the largest vendors in enterprise IT, best known for its flagship database.\"}\n",
    "]\n",
    "\n",
    "current_query = {\"role\": \"user\", \"content\": \"What is its flagship product?\"}\n",
    "\n",
    "full_history = past_messages + [current_query]\n",
    "\n",
    "response = llm_client.invoke(full_history)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78b4bd",
   "metadata": {},
   "source": [
    "## Structured Output (JSON)\n",
    "\n",
    "Some LLMs support structured outputs, returning their answers as valid JSON. See `openai_oci_structured_output.py` for advanced schemas. Below is an example prompt designed to elicit a JSON response:\n",
    "\n",
    "> **Note:** Not all models/clients support output schemas or enforced JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a10e759a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='The Three-Body Problem' author='Liu Cixin' publication_year=2014 in_stock=True copies_available=5 price_usd=16.99\n",
      "<class '__main__.BookInventory'>\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "from langChain.openai_oci_client import OciOpenAILangGraphClient\n",
    "\n",
    "# Pydantic class schema\n",
    "class BookInventory(BaseModel):\n",
    "    \"\"\"Information about a book in a store or library inventory.\"\"\"\n",
    "    # Requires always a description\n",
    "\n",
    "    # Name of the field : type = Field description to help model generate\n",
    "    title: str = Field(description=\"Title of the book\")\n",
    "    author: str = Field(description=\"Author of the book\")\n",
    "    publication_year: int = Field(description=\"Publication year of the book\")\n",
    "    in_stock: bool = Field(description=\"True if the book is currently available in stock\")\n",
    "    copies_available: int = Field(ge=0, description=\"Number of copies available\")\n",
    "    price_usd: float = Field(ge=0.0, description=\"Price of the book in USD\")\n",
    "\n",
    "#llm_client = OciOpenAILangChainClient(\n",
    "llm_client = OciOpenAILangGraphClient(\n",
    "        profile=scfg['oci']['profile'],\n",
    "        compartment_id=scfg['oci']['compartment'],\n",
    "        model=LLM_MODEL,\n",
    "        service_endpoint=llm_service_endpoint\n",
    "    )\n",
    "composed_pydantic_model = llm_client.with_structured_output(BookInventory)\n",
    "\n",
    "MESSAGE = \"\"\"\n",
    "  Give me the information about the current science fiction books.\n",
    "\"\"\"\n",
    "\n",
    "response = composed_pydantic_model.invoke(MESSAGE)\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb45e9d",
   "metadata": {},
   "source": [
    "# Resoaning\n",
    "OpenAI responses api provides insights into LLM's reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e53b16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Answer ===\n",
      "[{'type': 'text', 'text': 'Steps:\\n- Identify the operation: splitting equally among 3 friends means division.\\n- Compute 21 ÷ 3 = 7.\\n- Check: 7 apples per friend × 3 friends = 21 apples, which matches.\\n\\nSummary: Each friend receives 7 apples.', 'annotations': []}]\n",
      "\n",
      "=== Reasoning Summary ===\n",
      "[{'text': '**Explaining division steps**\\n\\nI need to follow the system instructions and explain the steps for a simple division problem clearly. I’ll break it down for 21 divided by 3, which equals 7 apples each. I should outline the steps: first, identify the operation, then perform the division, and finally, do a quick check. After that, I’ll provide a brief summary sentence. It’s important to keep the formatting straightforward, maybe even using a bullet list for clarity.', 'type': 'summary_text'}]\n"
     ]
    }
   ],
   "source": [
    "llm = OCIOpenAIHelper.get_client(\n",
    "    model_name=\"openai.gpt-5\",\n",
    "    config=scfg,\n",
    "    use_responses_api=True,\n",
    "    reasoning={\"effort\": \"low\", \"summary\": \"auto\"},\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an expert reasoning assistant. Explain your steps clearly, then provide a brief summary.\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"If there are 21 apples and they are split equally among 3 friends, how many apples does each friend receive?\"\n",
    "    ),\n",
    "]\n",
    "res = llm.invoke(messages)\n",
    "\n",
    "# Answer\n",
    "print(\"\\n=== Answer ===\")\n",
    "print(getattr(res, \"content\", res))\n",
    "\n",
    "# Reasoning summary (if available)\n",
    "ak = getattr(res, \"additional_kwargs\", {}) or {}\n",
    "summary = ak.get(\"reasoning_summary\") or ak.get(\"summary\") or (ak.get(\"reasoning\") or {}).get(\"summary\")\n",
    "print(\"\\n=== Reasoning Summary ===\")\n",
    "print(summary if summary else \"N/A\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Try different models from the table above and compare their style, accuracy, and creativity.\n",
    "- Modify the prompt or system message to guide the LLM's behavior (e.g., “Reply in JSON”, “Summarize as bullet points”).\n",
    "- Use batching and token limits to control response length.\n",
    "- Build a short multi-turn conversation and observe how history impacts results.\n",
    "- Try outputting structured JSON (if supported). \n",
    "- Share your findings or questions in the Slack channels above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work Ideas\n",
    "\n",
    "- Integrate the LLM with Slack for notifications or chatbots.\n",
    "- Explore OpenAI function calling and tool integrations.\n",
    "- Build pipelines or agents that chain together multiple LLM invocations.\n",
    "- Test performance and differences between streaming and batch modes.\n",
    "- Experiment with prompt engineering challenges (few-shot, chain-of-thought, etc).\n",
    "- Explore multimodal capabilities (images, audio, documents) using OCI GenAI.\n",
    "\n",
    "*Keep exploring and sharing!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
