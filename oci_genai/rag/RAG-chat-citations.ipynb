{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 2: RAG - Passing Documents and Citations\n",
    "\n",
    "### What this file does:\n",
    "Demonstrates how to pass custom documents directly to OCI Generative AI (Cohere models) for chat responses with citations. This is a basic form of Retrieval-Augmented Generation (RAG) without external storageâ€”documents are provided in the request.\n",
    "\n",
    "**Supported models:** cohere.command-r-08-2024, cohere.command-r-plus-08-2024, cohere.command-a-03-2025. For other models (e.g., Llama), use the generic API.\n",
    "\n",
    "**Documentation to reference:**\n",
    "- OCI GenAI Chat: https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm\n",
    "- Cohere Documents and Citations: https://docs.cohere.com https://docs.cohere.com/v2/docs/rag-citations#citation-modes\n",
    "- OCI Python SDK: https://github.com/oracle/oci-python-sdk/tree/master/src/oci/generative_ai_inference\n",
    "\n",
    "**Relevant slack channels:**\n",
    "- #generative-ai-users or #igiu-innovation-lab: *For questions*\n",
    "- #igiu-ai-learning: *For errors running code*\n",
    "\n",
    "**Env setup:**\n",
    "- sandbox.yaml: Needs \"oci\" (configFile, profile, compartment) sections.\n",
    "- .env: Load environment variables if needed.\n",
    "-  configure cwd for jupyter match your workspace python code: \n",
    "    -  vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "    -  change from `${fileDirname}` to `${workspaceFolder}`\n",
    "\n",
    "\n",
    "**How to run in notebook:**\n",
    "- Ensure dependencies installed (uv sync).\n",
    "- Run cells in order.\n",
    "- Update LLM_MODEL if using a different Cohere variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import OnDemandServingMode, CohereChatRequest, ChatDetails\n",
    "import oci\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "load_dotenv()\n",
    "LLM_MODEL = \"cohere.command-a-03-2025\"  # Options: cohere.command-r-08-2024, cohere.command-r-plus-08-2024\n",
    "llm_service_endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load configuration, import libraries, and initialize the OCI client for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sandbox config\n",
    "scfg = EnvYAML(SANDBOX_CONFIG_FILE)\n",
    "if scfg is None or \"oci\" not in scfg:\n",
    "    raise RuntimeError(\"Invalid sandbox configuration.\")\n",
    "\n",
    "# Set up OCI config\n",
    "config = oci.config.from_file(os.path.expanduser(scfg[\"oci\"][\"configFile\"]), scfg[\"oci\"][\"profile\"])\n",
    "\n",
    "# Initialize LLM client\n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "    config=config,\n",
    "    service_endpoint=llm_service_endpoint,\n",
    "    retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "    timeout=(10, 240)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Chat Request Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Cohere chat request with preamble, parameters, and documents for citation.\n",
    "# Set up chat request with preamble and parameters\n",
    "llm_chat_request = CohereChatRequest()\n",
    "# llm_chat_request.preamble_override = \"Provide factual answers based on documents provided. Include citations if possible. Say you can't answer if not in documents.\"\n",
    "llm_chat_request.is_stream = False\n",
    "llm_chat_request.max_tokens = 500  # Max tokens to generate\n",
    "llm_chat_request.temperature = 1.0  # Higher = more random (default 0.3)\n",
    "# llm_chat_request.seed = 7555  # For deterministic results (not guaranteed)\n",
    "llm_chat_request.top_p = 0.7  # Nucleus sampling (default 0.75)\n",
    "llm_chat_request.top_k = 0  # Top-k sampling (0 = off, max 500)\n",
    "llm_chat_request.frequency_penalty = 0.0  # Reduce repetition (max 1.9)\n",
    "# llm_chat_request.documents = get_documents()  # Restricts to provided docs\n",
    "llm_chat_request.citation_quality = CohereChatRequest.CITATION_QUALITY_FAST  # Or CITATION_QUALITY_ACCURATE for better (slower) citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for testing\n",
    "docs = [\n",
    "    {\n",
    "        \"title\": \"Oracle\",\n",
    "        \"snippet\": \"Oracle database services and products offer customers cost-optimized and high-performance versions of Oracle Database, the world's leading converged, multi-model database management system, as well as in-memory, NoSQL and MySQL databases. Oracle Autonomous Database, available on premises via Oracle Cloud@Customer or in the Oracle Cloud Infrastructure, enables customers to simplify relational database environments and reduce management workloads.\",\n",
    "        \"website\": \"https://www.oracle.com/database\",\n",
    "        \"id\": \"ORA001\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Amazon\",\n",
    "        \"snippet\": \"AWS provides the broadest selection of purpose-built databases allowing you to save, grow, and innovate faster. Purpose Built: Choose from 15+ purpose-built database engines including relational, key-value, document, in-memory, graph, time series, wide column, and ledger databases. Performance at Scale: Get relational databases that are 3-5X faster than popular alternatives, or non-relational databases that give you microsecond to sub-millisecond latency. Fully Managed: AWS continuously monitors your clusters to keep your workloads running with self-healing storage and automated scaling, so that you can focus on application development. Secure & Highly Available: AWS databases are built for business-critical, enterprise workloads, offering high availability, reliability, and security.\",\n",
    "        \"website\": \"https://aws.amazon.com/free/database\",\n",
    "        \"id\": \"AWS001\"\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_chat_request.documents = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up chat details\n",
    "chat_detail = ChatDetails()\n",
    "chat_detail.serving_mode = OnDemandServingMode(model_id=LLM_MODEL)\n",
    "chat_detail.compartment_id = scfg[\"oci\"][\"compartment\"]\n",
    "chat_detail.chat_request = llm_chat_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Ask a Question and Get Response with Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a query to the model and print the response along with citations.\n",
    "# Set the user message\n",
    "# llm_chat_request.seed = 7555  # Uncomment to try deterministic results\n",
    "llm_chat_request.message = \"Tell me about AWS databases.\"\n",
    "\n",
    "# Get response\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "\n",
    "# Print results\n",
    "print(\"************************** Chat Result **************************\")\n",
    "print(llm_response.data.chat_response.text)\n",
    "print(\"************************** Citations **************************\")\n",
    "print(llm_response.data.chat_response.citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Update History for Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add previous chat messages to enable conversation history.\n",
    "# Update history with previous messages\n",
    "previous_chat_message = oci.generative_ai_inference.models.CohereUserMessage(message=\"Tell me something about Oracle.\")\n",
    "previous_chat_reply = oci.generative_ai_inference.models.CohereChatBotMessage(message=\"Oracle is one of the largest vendors in the enterprise IT market and the shorthand name of its flagship product. The database software sits at the center of many corporate IT\")\n",
    "\n",
    "llm_chat_request.chat_history = [previous_chat_message, previous_chat_reply]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Follow-Up Question with History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a follow-up question using the history for context.\n",
    "# Clear documents for history test\n",
    "# llm_chat_request.documents = []\n",
    "\n",
    "llm_chat_request.message = \"Tell me more about its databases.\"  # Refers to previous Oracle context\n",
    "\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "\n",
    "# Print results\n",
    "print(\"************************** Chat Result **************************\")\n",
    "print(llm_response.data.chat_response.text)\n",
    "print(\"************************** Citations **************************\")\n",
    "print(llm_response.data.chat_response.citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation Section\n",
    "Try these variations to learn more:\n",
    "- Change the model to cohere.command-r-plus-08-2024 and compare responses.\n",
    "- Adjust temperature (e.g., 0.3 for factual, 1.0 for creative) and observe changes.\n",
    "- Add your own documents (e.g., snippets from PDFs) and query about them.\n",
    "- Enable/disable chat_history and see how context affects answers.\n",
    "- Set citation_quality to ACCURATE for more precise citations (may be slower).\n",
    "- Query something not in documents to test handling of unknowns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "1. **Custom Documents**: Replace sample docs with text from a favorite book or article. Ask questions and check citations.\n",
    "2. **History Depth**: Add more messages to chat_history and experiment with long conversations.\n",
    "3. **Parameter Tuning**: Create a loop to test different temperature/top_p values and rate response quality.\n",
    "4. **Discussion**: How do citations improve trust in AI responses? What are limitations of this approach compared to full RAG?\n",
    "\n",
    "For help or ideas, reach out in #igiu-innovation-lab or #igiu-ai-learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
