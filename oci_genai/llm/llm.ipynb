{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# LLM Chat Examples with OCI Generative AI\n",
    "\n",
    "### What this file does:\n",
    "\n",
    "Demonstrates basic chat functionality using OCI Generative AI with Cohere models, including parameter experimentation, conversation history, structured outputs (JSON), and streaming responses. Shows how to make chat requests, configure parameters like temperature, max tokens, and seed, and experiment with different settings for reproducible and controlled results. This notebook builds on concepts from the accompanying Python files (e.g., cohere_chat.py, cohere_chat_history.py).\n",
    "\n",
    "**Documentation to reference:**\n",
    "\n",
    "- OCI Gen AI: https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm\n",
    "- Cohere Chat Models: https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm\n",
    "- OCI Python SDK: https://github.com/oracle/oci-python-sdk/tree/master/src/oci/generative_ai_inference/models\n",
    "- Cohere Chat API: https://docs.cohere.com/docs/chat-api\n",
    "\n",
    "**Relevant slack channels:**\n",
    "\n",
    "- #generative-ai-users: *for questions on OCI Gen AI*\n",
    "- #igiu-innovation-lab: *general discussions on your project*\n",
    "- #igiu-ai-learning: *help with sandbox environment or running this code*\n",
    "\n",
    "**Env setup:**\n",
    "\n",
    "- sandbox.yaml: Contains OCI config, compartment, and other details.\n",
    "- .env: Load environment variables (e.g., API keys if needed).\n",
    "- configure cwd for jupyter match your workspace python code:\n",
    "  - vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "  - change from `${fileDirname}` to `${workspaceFolder}`\n",
    "\n",
    "**How to run in notebook:**\n",
    "\n",
    "- Make sure your runtime environment has all dependencies and access to required config files.\n",
    "- Run the notebook cells in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-models",
   "metadata": {},
   "source": [
    "### Supported models\n",
    "\n",
    "See [OCI Chat Models Documentation](https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm) for the latest list.\n",
    "\n",
    "**Cohere models (generic API):**\n",
    "\n",
    "- cohere.command-a-03-2025\n",
    "- cohere.command-r-08-2024\n",
    "- cohere.command-r-plus-08-2024\n",
    "\n",
    "**OpenAI-compatible models (generic API):**\n",
    "\n",
    "- openai.gpt-4.1\n",
    "- openai.gpt-4.1-mini\n",
    "- openai.gpt-4.1-nano\n",
    "- openai.gpt-o1\n",
    "- openai.gpt-o3\n",
    "- openai.gpt-4o\n",
    "- openai.gpt-4o-mini\n",
    "- openai.gpt-5\n",
    "- openai.gpt-5-mini\n",
    "- openai.gpt-5-nano\n",
    "- xai.grok-3\n",
    "- xai.grok-4\n",
    "- xai.grok-4-fast-reasoning\n",
    "- xai.grok-4-fast-non-reasoning\n",
    "- meta.llama-3.1-405b-instruct\n",
    "- meta.llama-3.3-70b-instruct\n",
    "- meta.llama-3.2-90b-vision-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-setup",
   "metadata": {},
   "source": [
    "### Step 1: Load config and initialize clients\n",
    "\n",
    "Set up the OCI Generative AI client and load configuration. This step initializes the connection to OCI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import *\n",
    "import oci\n",
    "import json, os \n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML\n",
    "\n",
    "# make sure your sandbox.yaml file is setup for your environment. You might have to specify the full path depending on your `cwd` \n",
    "# you can also try making your cwd for jupyter match your workspace python code: \n",
    "# vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "# change from ${fileDirname} to ${workspaceFolder}\n",
    "\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "load_dotenv()\n",
    "\n",
    "CHAT_MODEL = \"cohere.command-r-plus-08-2024\" \n",
    "PREAMBLE = \"\"\"\n",
    "        answer in three bullets. respond in hindi \n",
    "\"\"\"\n",
    "MESSAGE = \"\"\"\n",
    "        \"why is indian cricket team so good\"\n",
    "\"\"\"\n",
    "\n",
    "llm_service_endpoint= \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "scfg = EnvYAML(SANDBOX_CONFIG_FILE)\n",
    "if scfg is None or \"oci\" not in scfg or \"bucket\" not in scfg:\n",
    "    raise RuntimeError(\"Invalid sandbox configuration.\")\n",
    "\n",
    "#read the oci config\n",
    "config = oci.config.from_file(os.path.expanduser(scfg[\"oci\"][\"configFile\"]),scfg[\"oci\"][\"profile\"])\n",
    "                               \n",
    "# chat request      \n",
    "llm_chat_request = CohereChatRequest()\n",
    "llm_chat_request.preamble_override = PREAMBLE \n",
    "llm_chat_request.message = MESSAGE\n",
    "llm_chat_request.is_stream = False \n",
    "llm_chat_request.max_tokens = 500 # max token to generate, can lead to incomplete responses\n",
    "llm_chat_request.temperature = 1.0 # higer value menas more randon, defaul = 0.3\n",
    "llm_chat_request.top_p = 0.7  # ensures only tokens with toptal probabely of p are considered, max value = 0.99, min 0.01, default 0.75\n",
    "llm_chat_request.top_k = 0  #Ensures that only top k tokens are considered, 0 turns it off, max = 500\n",
    "llm_chat_request.frequency_penalty = 0.0 # reduces the repeatedness of tokens max value 1.9=0, min 0,0\n",
    "\n",
    "\n",
    "# set up chat details\n",
    "chat_detail = ChatDetails()\n",
    "chat_detail.serving_mode = OnDemandServingMode(model_id=CHAT_MODEL)\n",
    "chat_detail.compartment_id = scfg[\"oci\"][\"compartment\"]\n",
    "chat_detail.chat_request = llm_chat_request\n",
    "\n",
    "# set up the LLM client \n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "                config=config,\n",
    "                service_endpoint=llm_service_endpoint,\n",
    "                retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "                timeout=(10,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-basic-chat",
   "metadata": {},
   "source": [
    "### Step 2: Basic Chat Functionality\n",
    "\n",
    "Send a simple chat request to the model. This demonstrates the core chat API call. Experiment by changing the message or preamble to see how the response varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-chat",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat_request.preamble_override = \"respond in bullet\" \n",
    "llm_chat_request.message = \"tell me 2 things about oracle primavera cloud \"\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "llm_text = llm_response.data.chat_response.text\n",
    "        \n",
    "print (llm_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-predictability",
   "metadata": {},
   "source": [
    "### Step 3: Response Predictability\n",
    "\n",
    "Without a seed, the same question can yield different responses due to randomness. Adding a seed makes responses more deterministic (best effort). Run the cell multiple times to observe differences.\n",
    "\n",
    "**Experiment:** Change the seed value (e.g., 1234, 9999) and rerun to test reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predictability",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = llm_client.chat(chat_detail)\n",
    "llm_text = llm_response.data.chat_response.text\n",
    "# same question can result in differet response        \n",
    "print (llm_text)\n",
    "\n",
    "## Play with seed\n",
    "llm_chat_request.seed = 7555 # adding the seed will make response more predictable\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "llm_text = llm_response.data.chat_response.text\n",
    "        \n",
    "print (llm_text)\n",
    "\n",
    "llm_chat_request.seed = 7555\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "llm_text = llm_response.data.chat_response.text\n",
    "print (llm_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-output-length",
   "metadata": {},
   "source": [
    "### Step 4: Controlling Output Length\n",
    "\n",
    "The `max_tokens` parameter limits response length. Lower values truncate responses; check `finish_reason` to see if it stopped due to length.\n",
    "\n",
    "**Experiment:** Try max_tokens = 10, 100, or 1000 and observe how the response and finish_reason change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "output-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  controlling the output length.  \n",
    "llm_chat_request.max_tokens = 60\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "print (llm_response.data.chat_response.text)\n",
    "print (llm_response.data.chat_response.finish_reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-history",
   "metadata": {},
   "source": [
    "### Step 5: Conversation History Support\n",
    "\n",
    "For conversational bots, maintain history by adding previous messages (user and assistant) to the request. This provides context for follow-up questions (e.g., pronouns like 'it'). History is typically added in pairs.\n",
    "\n",
    "**Experiment:** Add more history pairs or change the follow-up message to test context retention. Try removing history to see ambiguous responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "history",
   "metadata": {},
   "outputs": [],
   "source": [
    "## History  support\n",
    "# optionally update the PREAMBLE \n",
    "# PREAMBLE = \" Answer the questions in a professional tone, based on thw conversation history\"\n",
    "\n",
    "\n",
    "previous_chat_message = oci.generative_ai_inference.models.CohereUserMessage(message=\"Tell me something about Oracle.\")\n",
    "previous_chat_reply = oci.generative_ai_inference.models.CohereChatBotMessage(message=\"Oracle is one of the largest vendors in the enterprise IT market and the shorthand name of its flagship product. The database software sits at the center of many corporate IT\")\n",
    "llm_chat_request.chat_history = [previous_chat_message, previous_chat_reply]\n",
    "# ask the question \n",
    "llm_chat_request.seed = None\n",
    "\n",
    "#llm_chat_request.message = \"Where is Oracle's HQ?\"\n",
    "\n",
    "llm_chat_request.message = \"what is its flagship product?\"  # LLM knows what \"it\" means due to us adding history records\n",
    "\n",
    "\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "\n",
    "# Print result\n",
    "print(\"**************************Chat Result**************************\")\n",
    "llm_text = llm_response.data.chat_response.text\n",
    "\n",
    "print(llm_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-json",
   "metadata": {},
   "source": [
    "### Step 6: JSON Format for Structured Outputs\n",
    "\n",
    "Use `response_format` with CohereResponseJsonFormat() for structured JSON responses (supported on 08-2024+ models). Specify schemas for validation (e.g., objects, arrays). Always instruct JSON in preamble for better adherence.\n",
    "\n",
    "**Note:** Schemas enforce structure; mismatched schemas (e.g., single object vs. array) limit output.\n",
    "\n",
    "**Experiment:** Create a schema for extracting entities from text (e.g., names, dates) and test with different inputs. Try without schema to compare free-form JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  JSON format\n",
    "\n",
    "\n",
    "# change top the supported model\n",
    "chat_detail.serving_mode = OnDemandServingMode(model_id=\"cohere.command-r-08-2024\")\n",
    "llm_chat_request.response_format = CohereResponseJsonFormat()\n",
    "llm_chat_request.preamble_override = \"Answer in json format\"\n",
    "llm_chat_request.message = \"list 3 Latest novels from indian authors \"\n",
    "\n",
    "llm_chat_request.max_tokens = 500\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "llm_text = llm_response.data.chat_response.text\n",
    "print (llm_text)\n",
    "\n",
    "  \n",
    "\n",
    "#Here we specify a simple JSON. note the scheme only allows for one entry.\n",
    "#so even thouhg we ask for multiple answer will be just one\n",
    "response_schema = {\n",
    "            \"type\": \"object\",\n",
    "            \"required\": [\"title\", \"author\", \"publication_year\"],\n",
    "            \"properties\": {\n",
    "                \"title\": {\"type\": \"string\"},\n",
    "                \"author\": {\"type\": \"string\"},\n",
    "                \"publication_year\": {\"type\": \"integer\"},\n",
    "            },\n",
    "        }\n",
    "\n",
    "llm_chat_request.response_format = CohereResponseJsonFormat(schema = response_schema)\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "llm_text = llm_response.data.chat_response.text\n",
    "print (llm_text)\n",
    "\n",
    "### nested schema with array\n",
    "\n",
    "\n",
    "#Here we allso for an array so the answer will be multiple entries\n",
    "#also see how we were able to break name into first & last name\n",
    "response_schema = {\n",
    "            \"type\": \"object\",\n",
    "            \"required\": [\"authors\"],\n",
    "            \"properties\" : {\n",
    "                \"authors\" : {\n",
    "                        \"type\" : \"array\",\n",
    "                        \"items\" : {\n",
    "                                \"type\" : \"object\",\n",
    "                                \"required\": [\"title\", \"author\", \"publication_year\"],\n",
    "                                \"properties\": {\n",
    "                                        \"title\": {\"type\": \"string\"},\n",
    "                                        \"author\": {\n",
    "                                                \"type\": \"object\",\n",
    "                                                \"required\" : [\"fname\", \"lname\"],\n",
    "                                                \"properties\" : {\n",
    "                                                        \"fname\" : {\"type\":\"string\"},       \n",
    "                                                        \"lname\" : {\"type\":\"string\"}\n",
    "                                                }\n",
    "                                        },\n",
    "                                        \"publication_year\": {\"type\": \"integer\"}\n",
    "                                }\n",
    "                        }        \n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "llm_chat_request.response_format = CohereResponseJsonFormat(schema = response_schema)\n",
    "llm_chat_request.message = \"list three most popular novels from indian authors last 2022\"\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "llm_text = llm_response.data.chat_response.text\n",
    "print (llm_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-streaming",
   "metadata": {},
   "source": [
    "### Step 7: Streaming Responses\n",
    "\n",
    "Enable streaming (`is_stream=True`) to receive responses token-by-token via Server-Sent Events (SSE). This is ideal for real-time UIs. Process events to print progressively.\n",
    "\n",
    "**Experiment:** Change the message to a longer query and observe token-by-token generation. Compare timing with non-streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Streaming response\n",
    "\n",
    "\n",
    "\n",
    "#ask the question \n",
    "llm_chat_request.preamble_override = \"Answer as a poem\" \n",
    "llm_chat_request.message = \"why is sky blue\"\n",
    "llm_chat_request.is_stream = True \n",
    "llm_chat_request.response_format = CohereResponseTextFormat()\n",
    "\n",
    "\n",
    "# geerate response\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "\n",
    "\n",
    "# stream twh output\n",
    "import json \n",
    "for event in llm_response.data.events():\n",
    "    res = json.loads(event.data)\n",
    "    if 'finishReason' in res.keys():\n",
    "        print(f\"\\nFinish reason: {res['finishReason']}\")\n",
    "        break\n",
    "    if 'text' in res:\n",
    "        print(res['text'], end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "### Practice Exercises and Discussion Prompts\n",
    "\n",
    "1. **Basic Chat Experiments:**\n",
    "   - Try zero-shot, few-shot (add examples in preamble), and chain-of-thought prompting.\n",
    "   - Test output formats: freeform text, bullets, JSON, CSV, SQL.\n",
    "   - Input different languages (e.g., Hindi, French) and observe multilingual support.\n",
    "\n",
    "2. **Language Analysis Tasks:**\n",
    "   - Use the LLM for proofreading: Feed in text with errors and ask for corrections.\n",
    "   - Entity extraction: Ask to extract names, dates, locations from a news article.\n",
    "   - Summarization and sentiment: Summarize a paragraph and analyze its tone.\n",
    "\n",
    "3. **Data Generation:**\n",
    "   - Generate SQL INSERT statements for sample data.\n",
    "   - Create JSON/CSV outputs for mock datasets.\n",
    "\n",
    "4. **Guardrails and Safety:**\n",
    "   - Test harmful prompts and observe model refusals.\n",
    "   - Implement simple content filters in the preamble.\n",
    "\n",
    "5. **Build a Mini-Project:**\n",
    "   - Create a conversational bot that remembers user preferences (e.g., response language).\n",
    "   - Extract entities from a poem and generate a structured summary.\n",
    "\n",
    "**Discussion Prompts:**\n",
    "- How does adding history affect response quality for ambiguous queries?\n",
    "- When would you use structured JSON vs. free-text outputs in applications?\n",
    "- Discuss trade-offs between streaming (real-time) and non-streaming (complete response) in UI design.\n",
    "\n",
    "For help, reach out in #igiu-ai-learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
