{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCI Vision Video Analysis (Beginner Notebook)\n",
    "\n",
    "### What this file does:\n",
    "Analyze video with Oracle Cloud Vision's pre-trained models for labels, objects, text, and faces, using the official Python SDK. This walkthrough notebook is adapted from the logic in `vision/oci_vision_video.py`.\n",
    "\n",
    "**Documentation to reference:**\n",
    "- OCI Vision Video Analysis: https://docs.oracle.com/en-us/iaas/Content/vision/using/stored_video_analysis.htm#pretrained_image_analysis_video\n",
    "- OCI Python SDK: https://github.com/oracle/oci-python-sdk/tree/master/src/oci/ai_vision\n",
    "\n",
    "**Relevant slack channels:**\n",
    "- #oci_ai_vision_support: *for OCI Vision API questions*\n",
    "- #igiu-innovation-lab: *general discussions on your project*\n",
    "- #igiu-ai-learning: *help with sandbox environment or help with running this code*\n",
    "\n",
    "**Env setup:**\n",
    "- sandbox.yaml: Contains OCI config, compartment, and bucket details.\n",
    "- .env: Load environment variables if needed.\n",
    "- configure cwd for jupyter match your workspace python code: \n",
    "    -  vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "    -  change from `${fileDirname}` to `${workspaceFolder}`\n",
    "\n",
    "\n",
    "**How to run in notebook:**\n",
    "- Make sure your runtime environment has all dependencies and access to required config files.\n",
    "- Run the notebook cells in order.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Requirements\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Environment Setup:** Before interacting with OCI services, you need to configure your environment with credentials, compartment IDs, and bucket details. This is typically done via a YAML config file (sandbox.yaml) and environment variables.\n",
    "- **Dependencies:** Install necessary libraries like the OCI SDK and configuration loaders.\n",
    "- **Libraries:** Use packages for config management, environment variables, and OCI interactions.\n",
    "\n",
    "In this step, we'll install dependencies and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import oci\n",
    "from oci.object_storage import ObjectStorageClient\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported and environment loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load OCI/Sandbox Configuration\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Configuration Loading:** Securely load settings from a config file to authenticate and specify resources without hardcoding credentials.\n",
    "- **OCI Config:** Includes paths to config files, profiles, compartments, and bucket details.\n",
    "- **Error Handling:** Validate that the config is loaded correctly to avoid runtime errors.\n",
    "\n",
    "In this step, we'll load and validate the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and load configuration\n",
    "# Make sure your sandbox.yaml file is set up for your environment. You might have to specify the full path depending on your `cwd`.\n",
    "# You can also try making your cwd for jupyter match your workspace python code:\n",
    "# vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "# change from ${fileDirname} to ${workspaceFolder}\n",
    "\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "VIDEO_PATH = Path(\"vision/mall.mp4\")\n",
    "\n",
    "def load_config(config_path):\n",
    "    try:\n",
    "        return EnvYAML(config_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Configuration file '{config_path}' not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading config: {e}\")\n",
    "        return None\n",
    "\n",
    "scfg = load_config(SANDBOX_CONFIG_FILE)\n",
    "assert scfg is not None and 'oci' in scfg and 'bucket' in scfg, \"Check your sandbox.yaml config!\"\n",
    "oci_cfg = oci.config.from_file(os.path.expanduser(scfg[\"oci\"][\"configFile\"]), scfg[\"oci\"][\"profile\"])\n",
    "bucket_cfg = scfg[\"bucket\"]\n",
    "compartment_id = scfg[\"oci\"][\"compartment\"]\n",
    "\n",
    "print(\"Configuration loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload Video to Object Storage\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Object Storage:** OCI's scalable storage for files. Videos must be uploaded here for analysis.\n",
    "- **Bucket and Namespace:** Organize files in buckets within a namespace for access control.\n",
    "- **Prefix:** Use prefixes to categorize objects, similar to folders.\n",
    "\n",
    "In this step, we'll upload the selected video to Object Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload file to Object Storage\n",
    "def upload(oci_cfg, bucket_cfg, file_path):\n",
    "    if not file_path.exists():\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return False\n",
    "    object_storage_client = ObjectStorageClient(oci_cfg)\n",
    "    print(f\"Uploading file {file_path} ...\")\n",
    "    object_storage_client.put_object(\n",
    "        bucket_cfg[\"namespace\"], \n",
    "        bucket_cfg[\"bucketName\"], \n",
    "        f\"{bucket_cfg['prefix']}/{file_path.name}\", \n",
    "        open(file_path, 'rb')\n",
    "    )\n",
    "    print(\"Upload completed!\")\n",
    "    return True\n",
    "\n",
    "# Perform the upload\n",
    "uploaded = upload(oci_cfg, bucket_cfg, VIDEO_PATH)\n",
    "if not uploaded:\n",
    "    raise ValueError(\"Upload failed. Check your video and configuration.\")\n",
    "else:\n",
    "    print(\"Video uploaded successfully to Object Storage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Analysis Features\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Video Analysis Features:** OCI Vision can analyze videos for labels (scenes), objects, text (OCR), and faces over time.\n",
    "- **Feature Selection:** Choose analyses based on your needs to optimize cost and performance.\n",
    "- **Input/Output Locations:** Specify where the video is stored and where results will be saved.\n",
    "\n",
    "In this step, we'll define the features and prepare the analysis request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import specific models\n",
    "from oci.ai_vision.models import (\n",
    "    ObjectLocation, ObjectListInlineInputLocation, OutputLocation,\n",
    "    CreateVideoJobDetails, VideoObjectDetectionFeature,\n",
    "    VideoFaceDetectionFeature, VideoLabelDetectionFeature, VideoTextDetectionFeature\n",
    ")\n",
    "\n",
    "# Define helper functions for input and output locations\n",
    "def get_input_location(bucket_cfg, file_name):\n",
    "    object_location = ObjectLocation(\n",
    "        namespace_name=bucket_cfg[\"namespace\"],\n",
    "        bucket_name=bucket_cfg[\"bucketName\"],\n",
    "        object_name=f\"{bucket_cfg['prefix']}/{file_name}\",\n",
    "    )\n",
    "    return ObjectListInlineInputLocation(object_locations=[object_location])\n",
    "\n",
    "def get_output_location(bucket_cfg):\n",
    "    return OutputLocation(\n",
    "        namespace_name=bucket_cfg[\"namespace\"],\n",
    "        bucket_name=bucket_cfg[\"bucketName\"],\n",
    "        prefix=bucket_cfg[\"prefix\"],\n",
    "    )\n",
    "\n",
    "# Configure features: text detection, face detection, label detection, object detection\n",
    "features = [\n",
    "    VideoTextDetectionFeature(),\n",
    "    VideoFaceDetectionFeature(),\n",
    "    VideoLabelDetectionFeature(),\n",
    "    VideoObjectDetectionFeature()\n",
    "]\n",
    "\n",
    "print(\"Analysis features configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Submit Video Analysis Job\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Asynchronous Processing:** Video analysis is asynchronous, meaning you submit a job and check its status later.\n",
    "- **Job Creation:** Create a job with specified features, input location, output location, and compartment.\n",
    "- **Job ID:** Use the returned job ID to track progress.\n",
    "\n",
    "In this step, we'll submit the video for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Vision client\n",
    "vision_client = oci.ai_vision.AIServiceVisionClient(config=oci_cfg)\n",
    "\n",
    "# Create and submit the job\n",
    "job_details = CreateVideoJobDetails(\n",
    "    features=features,\n",
    "    input_location=get_input_location(bucket_cfg, VIDEO_PATH.name),\n",
    "    output_location=get_output_location(bucket_cfg),\n",
    "    compartment_id=compartment_id,\n",
    ")\n",
    "res = vision_client.create_video_job(create_video_job_details=job_details)\n",
    "job_id = None\n",
    "if res is not None and hasattr(res, 'data') and hasattr(res.data, 'id'):\n",
    "    job_id = res.data.id\n",
    "    print(f\"Job {job_id} created. State: {getattr(res.data,'lifecycle_state','?')}\")\n",
    "else:\n",
    "    print(f\"Error submitting video job: {res}\")\n",
    "    raise ValueError(\"Failed to create video job.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Poll for Job Completion\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Polling:** Regularly check the job's status until it completes.\n",
    "- **Lifecycle States:** Jobs progress through states like 'ACCEPTED', 'IN_PROGRESS', 'SUCCEEDED', or 'FAILED'.\n",
    "- **Progress Tracking:** Monitor percentage complete for user feedback.\n",
    "\n",
    "In this step, we'll wait for the analysis job to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll for completion\n",
    "poll_seconds = 0\n",
    "state = None\n",
    "if job_id is not None:\n",
    "    while True:\n",
    "        get_res = vision_client.get_video_job(video_job_id=job_id)\n",
    "        state = getattr(get_res.data, 'lifecycle_state', None)\n",
    "        percent = getattr(get_res.data, 'percent_complete', None)\n",
    "        print(f\"Status: {state} after {poll_seconds}s (progress={percent}%)\")\n",
    "        if state not in [\"IN_PROGRESS\", \"ACCEPTED\"]:\n",
    "            break\n",
    "        time.sleep(5)\n",
    "        poll_seconds += 5\n",
    "    print(f\"Job finished with state: {state}\")\n",
    "else:\n",
    "    raise ValueError(\"No job ID available.\")\n",
    "\n",
    "if state != \"SUCCEEDED\":\n",
    "    raise ValueError(f\"Job did not succeed. Final state: {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download and Parse Video Results\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Result Retrieval:** Results are stored as JSON in Object Storage after job completion.\n",
    "- **Result Structure:** Includes detections with timestamps, labels, confidence scores, and segments.\n",
    "- **Parsing:** Extract and display insights in a readable format.\n",
    "\n",
    "In this step, we'll retrieve the results and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pretty print the response\n",
    "def pretty_print_response(data):\n",
    "    if not isinstance(data, dict):\n",
    "        print(\"Not a dict response!\")\n",
    "        return\n",
    "    video_labels = data.get(\"videoLabels\", [])\n",
    "    if video_labels:\n",
    "        print(\"\\nLabels:\")\n",
    "        for lab in video_labels:\n",
    "            print(f\"  {lab.get('name', 'Unknown')}\")\n",
    "            for seg in lab.get(\"segments\", []):\n",
    "                span = seg.get(\"videoSegment\", {})\n",
    "                start = span.get(\"startTimeOffsetMs\", \"N/A\")\n",
    "                end = span.get(\"endTimeOffsetMs\", \"N/A\")\n",
    "                conf = seg.get(\"confidence\", 0)\n",
    "                print(f\"    {start}-{end}ms  conf={conf:.2f}\")\n",
    "    video_objects = data.get(\"videoObjects\", [])\n",
    "    if video_objects:\n",
    "        print(\"\\nObjects:\")\n",
    "        for obj in video_objects:\n",
    "            label = obj.get(\"name\", \"Unknown\")\n",
    "            for seg in obj.get(\"segments\", []):\n",
    "                span = seg.get(\"videoSegment\", {})\n",
    "                start = span.get(\"startTimeOffsetMs\", \"N/A\")\n",
    "                end = seg.get(\"videoSegment\", {}).get(\"endTimeOffsetMs\", \"N/A\")\n",
    "                conf = seg.get(\"confidence\", 0)\n",
    "                print(f\"    {start}-{end}ms  {label}  conf={conf:.2f}\")\n",
    "    video_texts = data.get(\"videoTexts\", [])\n",
    "    if video_texts:\n",
    "        print(\"\\nText Lines:\")\n",
    "        for txt in video_texts:\n",
    "            content = txt.get(\"text\", \"\")\n",
    "            for seg in txt.get(\"segments\", []):\n",
    "                span = seg.get(\"videoSegment\", {})\n",
    "                start = span.get(\"startTimeOffsetMs\", \"N/A\")\n",
    "                end = span.get(\"endTimeOffsetMs\", \"N/A\")\n",
    "                conf = seg.get(\"confidence\", 0)\n",
    "                print(f\"    {start}-{end}ms  \\\"{content}\\\"  conf={conf:.2f}\")\n",
    "    video_faces = data.get(\"videoFaces\", [])\n",
    "    if video_faces:\n",
    "        print(\"\\nFaces:\")\n",
    "        for face in video_faces:\n",
    "            for seg in face.get(\"segments\", []):\n",
    "                span = seg.get(\"videoSegment\", {})\n",
    "                start = span.get(\"startTimeOffsetMs\", \"N/A\")\n",
    "                end = span.get(\"endTimeOffsetMs\", \"N/A\")\n",
    "                conf = seg.get(\"confidence\", 0)\n",
    "                print(f\"    {start}-{end}ms  Face  conf={conf:.2f}\")\n",
    "    if not any([video_labels, video_objects, video_texts, video_faces]):\n",
    "        print(\"No features detected.\")\n",
    "\n",
    "# Download and parse results\n",
    "if job_id is not None and state == \"SUCCEEDED\":\n",
    "    object_storage_client = ObjectStorageClient(oci_cfg)\n",
    "    prefix = bucket_cfg['prefix']\n",
    "    object_name = f\"{prefix}/{job_id}/{prefix}/{VIDEO_PATH.name}.json\"\n",
    "    response = object_storage_client.get_object(\n",
    "        bucket_cfg['namespace'], bucket_cfg['bucketName'], object_name)\n",
    "    if hasattr(response, 'data') and hasattr(response.data, 'content'):\n",
    "        json_data = json.loads(response.data.content.decode('utf-8'))\n",
    "        pretty_print_response(json_data)\n",
    "        print(\"\\nResults parsed successfully.\")\n",
    "    else:\n",
    "        print('Failed to download or parse job results!')\n",
    "else:\n",
    "    print(\"Job not successful, cannot retrieve results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Done! Next Steps\n",
    "\n",
    "- Try different video files or formats.\n",
    "- Experiment with enabling/disabling specific features.\n",
    "- Monitor progress in real-time for longer videos.\n",
    "\n",
    "## üßë‚Äçüíª Project Ideas for Practice\n",
    "\n",
    "Below are some practical project prompts. Try one (or all) after you run a basic video through the models!\n",
    "\n",
    "1. **Security Monitoring:** Detect and report when a person enters a building without wearing a helmet or safety gear.\n",
    "2. **Content Moderation:** Flag videos with inappropriate objects, text, or faces for automated review.\n",
    "3. **Traffic Analysis:** Track vehicles and pedestrians in surveillance footage, counting entries/exits.\n",
    "4. **Sports Highlight Detection:** Identify key moments in a game video based on object and label detection.\n",
    "5. **Accessibility Tool:** Generate descriptions of video content for visually impaired users using detected labels and objects.\n",
    "\n",
    "If you see errors, double-check credentials or configurations. Refer to comments or docs for help.\n",
    "\n",
    "---\n",
    "**Happy building!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
