{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593962b2",
   "metadata": {},
   "source": [
    "# OCI Speech-to-Text: Step by Step Transcription\n",
    "\n",
    "**What this notebook does:**\n",
    "Demonstrates step-by-step audio transcription using OCI Speech-to-Text. Lists MP3 files, selects one, uploads to Object Storage, submits transcription job, polls for completion, displays transcript results.\n",
    "\n",
    "**Documentation to reference:**\n",
    "- Service docs:    https://docs.oracle.com/en-us/iaas/Content/speech/home.htm\n",
    "- Python SDK:      https://github.com/oracle/oci-python-sdk/tree/master/src/oci/ai_speech\n",
    "- Real time:       https://github.com/oracle/oci-ai-speech-realtime-python-sdk\n",
    "- Model comparison: https://docs.oracle.com/en-us/iaas/Content/speech/using/speech.htm#compare-models\n",
    "\n",
    "**Relevant slack channels:**\n",
    "- #oci_speech_service_users: *questions about speech service*\n",
    "- #igiu-innovation-lab: *project ideas*\n",
    "- #igiu-ai-learning: *issues with code or enviornment*\n",
    "\n",
    "**Env setup:**\n",
    "- sandbox.yaml: Contains OCI config, compartment, bucket details.\n",
    "- .env: Load environment variables if needed.\n",
    "- configure cwd for jupyter match your workspace python code: \n",
    "    -  vscode menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "    -  change from `${fileDirname}` to `${workspaceFolder}`\n",
    "\n",
    "\n",
    "**How to run in notebook:**\n",
    "- Run cells in order.\n",
    "\n",
    "**MP3 Demos to try:**\n",
    "- `voice_sample_english.mp3` (included)\n",
    "- Add more to the `speech/voice_sample-*.mp3` directory if you like!\n",
    "\n",
    "*Notebook prints transcript(s) below, no local file output!*\n",
    "\n",
    "**Experimentation ideas:**\n",
    "- Try different MP3 files by changing AUDIO_FILE selection.\n",
    "- Switch between 'WHISPER_MEDIUM' and 'ORACLE' models.\n",
    "- Experiment with different language codes for ORACLE model.\n",
    "- Add more output formats like SRT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-links",
   "metadata": {},
   "source": [
    "##  Helpful Links and Prerequisites\n",
    "Before starting, review these resources:\n",
    "- **Service docs:** https://docs.oracle.com/en-us/iaas/Content/speech/home.htm\n",
    "- **Python SDK:** https://github.com/oracle/oci-python-sdk/tree/master/src/oci/ai_speech\n",
    "- **Real-time SDK:** https://github.com/oracle/oci-ai-speech-realtime-python-sdk\n",
    "- **Support Slack channels:** #oci_speech_service_users (for speech service questions), #igiu-innovation-lab (project ideas), #igiu-ai-learning (troubleshooting)\n",
    "\n",
    "**Prerequisites:**\n",
    "- Ensure `sandbox.yaml` is set up with OCI config, compartment, and bucket details.\n",
    "- Place MP3 files in the `speech/` directory (e.g., `voice_sample_english.mp3`).\n",
    "- This notebook assumes you have the necessary permissions and dependencies installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-setup",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports\n",
    "In this step, we import necessary libraries, load the configuration, and set up clients. This prepares the environment for interacting with OCI services like Object Storage and Speech AI.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Loading YAML config for OCI credentials and bucket settings.\n",
    "- Initializing Object Storage client for uploading audio files.\n",
    "- Later, we'll use the Speech client for transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import oci\n",
    "from oci.object_storage import ObjectStorageClient\n",
    "from oci.ai_speech import AIServiceSpeechClient\n",
    "from dotenv import load_dotenv\n",
    "from envyaml import EnvYAML\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load config (assuming it's defined elsewhere or inline)\n",
    "def load_yaml(path: Path):\n",
    "    try:\n",
    "        return EnvYAML(path)\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(f\"Invalid sandbox yaml {path}.\")\n",
    "\n",
    "# Load sandbox config\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.yaml\"\n",
    "scfg = load_yaml(SANDBOX_CONFIG_FILE)\n",
    "if scfg is None or \"oci\" not in scfg or \"bucket\" not in scfg:\n",
    "    raise RuntimeError(\"Invalid sandbox configuration.\")\n",
    "\n",
    "# Extract bucket and OCI config\n",
    "oci_cfg = oci.config.from_file(os.path.expanduser(scfg[\"oci\"][\"configFile\"]), scfg[\"oci\"][\"profile\"])\n",
    "compartment_id = scfg[\"oci\"][\"compartment\"]\n",
    "prefix = bucket_cfg[\"prefix\"]\n",
    "\n",
    "# Initialize Object Storage client\n",
    "client = ObjectStorageClient(oci_cfg)\n",
    "\n",
    "print(\"Setup complete: Config loaded, Object Storage client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-select-audio",
   "metadata": {},
   "source": [
    "## Step 2: Select Audio File\n",
    "Here, we list available MP3 files in the speech directory and select one for transcription. This demonstrates how to prepare input audio for the Speech API.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Scanning a directory for audio files.\n",
    "- Selecting a file (you can modify AUDIO_FILE to choose different samples).\n",
    "- Supported formats: MP3 (and others like WAV, but this example uses MP3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing audio files\n",
    "audio_dir = Path(\"speech\")\n",
    "\n",
    "# List MP3 files\n",
    "mp3_files = list(audio_dir.glob(\"*.mp3\"))\n",
    "if not mp3_files:\n",
    "    raise RuntimeError(\"No MP3 files found in speech directory.\")\n",
    "\n",
    "print(\"Available MP3 files:\")\n",
    "for i, f in enumerate(mp3_files):\n",
    "    print(f\"{i}: {f.name}\")\n",
    "\n",
    "# Select the first file (change index to try others)\n",
    "selected_index = 0  # Modify this to select a different file\n",
    "AUDIO_FILE = mp3_files[selected_index]\n",
    "\n",
    "print(f\"Selected audio file: {AUDIO_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-upload",
   "metadata": {},
   "source": [
    "## Step 3: Upload Audio to Object Storage\n",
    "Upload the selected audio file to OCI Object Storage. The Speech API requires audio files to be stored in Object Storage for processing.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Using Object Storage client to upload files.\n",
    "- Constructing object names with prefixes for organization.\n",
    "- This step ensures the audio is accessible by the Speech service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct object name in bucket\n",
    "object_name = f\"{prefix}/{AUDIO_FILE.name}\"\n",
    "\n",
    "# Upload the file\n",
    "with AUDIO_FILE.open(\"rb\") as fh:\n",
    "    client.put_object(bucket_cfg[\"namespace\"], bucket_cfg[\"bucketName\"], object_name, fh)\n",
    "\n",
    "print(f\"Uploaded {AUDIO_FILE} â†’ oci://{bucket_cfg['namespace']}/{bucket_cfg['bucketName']}/{object_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-submit-job",
   "metadata": {},
   "source": [
    "## Step 4: Submit Transcription Job\n",
    "Create and submit a transcription job to the Speech API. Configure the model, language, and output settings.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Choosing between models: 'WHISPER_MEDIUM' (auto-detects language) or 'ORACLE' (specify language).\n",
    "- Supported language codes for ORACLE: en-US, es-ES, pt-BR, etc. (list provided in code).\n",
    "- Additional settings: diarization (speaker identification), punctuation, output formats (e.g., SRT).\n",
    "- The job is asynchronous; we get a job ID to track progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "submit-job",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supported language codes for ORACLE model\n",
    "SUPPORTED_LANGUAGE_CODES = {\n",
    "    \"en-US\": \"English - United States\",\n",
    "    \"es-ES\": \"Spanish - Spain\",\n",
    "    \"pt-BR\": \"Portuguese - Brazil\",\n",
    "    \"en-GB\": \"English - Great Britain\",\n",
    "    \"en-AU\": \"English - Australia\",\n",
    "    \"en-IN\": \"English - India\",\n",
    "    \"hi-IN\": \"Hindi - India\",\n",
    "    \"fr-FR\": \"French - France\",\n",
    "    \"de-DE\": \"German - Germany\",\n",
    "    \"it-IT\": \"Italian - Italy\",\n",
    "}\n",
    "\n",
    "# Speech service endpoint (Phoenix region)\n",
    "SPEECH_SERVICE_ENDPOINT = \"https://speech.aiservice.us-phoenix-1.oci.oraclecloud.com\"\n",
    "\n",
    "# Initialize Speech client\n",
    "speech_client = AIServiceSpeechClient(\n",
    "    config=oci_cfg,\n",
    "    signer=oci.signer.Signer(\n",
    "        tenancy=oci_cfg[\"tenancy\"],\n",
    "        user=oci_cfg[\"user\"],\n",
    "        fingerprint=oci_cfg[\"fingerprint\"],\n",
    "        private_key_file_location=oci_cfg[\"key_file\"],\n",
    "    ),\n",
    "    service_endpoint=SPEECH_SERVICE_ENDPOINT,\n",
    ")\n",
    "\n",
    "# Model configuration\n",
    "model_type = \"WHISPER_MEDIUM\"  # Change to \"ORACLE\" if needed\n",
    "language_code = \"auto\" if model_type == \"WHISPER_MEDIUM\" else \"en-US\"  # Adjust for ORACLE\n",
    "\n",
    "# Input location (the uploaded audio)\n",
    "object_location = oci.ai_speech.models.ObjectLocation(\n",
    "    namespace_name=bucket_cfg[\"namespace\"],\n",
    "    bucket_name=bucket_cfg[\"bucketName\"],\n",
    "    object_names=[object_name],\n",
    ")\n",
    "input_location = oci.ai_speech.models.ObjectListInlineInputLocation(\n",
    "    location_type=\"OBJECT_LIST_INLINE_INPUT_LOCATION\",\n",
    "    object_locations=[object_location],\n",
    ")\n",
    "\n",
    "# Output location (same bucket, with prefix)\n",
    "output_location = oci.ai_speech.models.OutputLocation(\n",
    "    namespace_name=bucket_cfg[\"namespace\"],\n",
    "    bucket_name=bucket_cfg[\"bucketName\"],\n",
    "    prefix=prefix,\n",
    ")\n",
    "\n",
    "# Normalization and settings\n",
    "normalization = oci.ai_speech.models.TranscriptionNormalization(\n",
    "    is_punctuation_enabled=True\n",
    ")\n",
    "transcription_settings = oci.ai_speech.models.TranscriptionSettings(\n",
    "    diarization=oci.ai_speech.models.Diarization(is_diarization_enabled=True)\n",
    ")\n",
    "\n",
    "# Model details\n",
    "model_details = oci.ai_speech.models.TranscriptionModelDetails(\n",
    "    language_code=language_code,\n",
    "    model_type=model_type,\n",
    "    domain=\"GENERIC\",\n",
    "    transcription_settings=transcription_settings,\n",
    ")\n",
    "\n",
    "# Job details\n",
    "job_details = oci.ai_speech.models.CreateTranscriptionJobDetails(\n",
    "    display_name=f\"{prefix}-nb-stt-job\",\n",
    "    compartment_id=compartment_id,\n",
    "    description=\"STT Jupyter Notebook Demo\",\n",
    "    model_details=model_details,\n",
    "    input_location=input_location,\n",
    "    output_location=output_location,\n",
    "    normalization=normalization,\n",
    "    additional_transcription_formats=[\"SRT\"],\n",
    ")\n",
    "\n",
    "# Submit the job\n",
    "response = speech_client.create_transcription_job(create_transcription_job_details=job_details)\n",
    "job_id = response.data.id\n",
    "print(f\"Transcription job submitted! OCID: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-wait",
   "metadata": {},
   "source": [
    "## Step 5: Wait for Transcription Job to Complete\n",
    "Poll the job status until it finishes. This step demonstrates asynchronous job handling.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Asynchronous processing: Jobs take time; we poll every few seconds.\n",
    "- Possible states: In progress, Succeeded, Failed.\n",
    "- Once succeeded, we can retrieve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wait-job",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job(client, job_id, poll_interval=5):\n",
    "    while True:\n",
    "        job = client.get_transcription_job(job_id).data\n",
    "        state = job.lifecycle_state\n",
    "        print(f\"Job state: {state}\")\n",
    "        if state == \"SUCCEEDED\":\n",
    "            return job\n",
    "        elif state == \"FAILED\":\n",
    "            raise Exception(\"Transcription job failed!\")\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "# Wait for the job to complete\n",
    "job_result = wait_for_job(speech_client, job_id)\n",
    "print(\"Transcription job completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-results",
   "metadata": {},
   "source": [
    "## Step 6: Retrieve and Display Transcription Results\n",
    "Download and print the transcript files from Object Storage. This shows how to access job outputs.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Listing objects in the bucket under the output prefix.\n",
    "- Retrieving text-based outputs (e.g., .txt, .srt).\n",
    "- Displaying results inline (no local file saves in this notebook).\n",
    "- Experiment with different files or settings by re-running earlier steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output objects\n",
    "outputs = client.list_objects(\n",
    "    namespace_name=bucket_cfg[\"namespace\"],\n",
    "    bucket_name=bucket_cfg[\"bucketName\"],\n",
    "    prefix=job_result.output_location.prefix,\n",
    ").data.objects\n",
    "\n",
    "# Print transcripts\n",
    "for obj in outputs:\n",
    "    if obj.name.lower().endswith(('.txt', '.srt')):\n",
    "        resp = client.get_object(bucket_cfg[\"namespace\"], bucket_cfg[\"bucketName\"], obj.name)\n",
    "        try:\n",
    "            text = resp.data.content.decode('utf-8')\n",
    "        except Exception:\n",
    "            text = resp.data.content.decode('latin-1')\n",
    "        print(\"\\n----- Output: {} -----\\n\".format(obj.name))\n",
    "        print(text[:10000])  # Limit to first 10k chars for display\n",
    "        print(\"\\n[...End of {}...]\\n\".format(obj.name))\n",
    "\n",
    "print(\"\\n---\\nTry another file by re-running Step 2!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-ideas",
   "metadata": {},
   "source": [
    "## Experiment and Project Ideas\n",
    "\n",
    "Now that you've seen the basic steps for transcribing audio with OCI Speech-to-Text, here are some ideas to expand your understanding and build projects:\n",
    "\n",
    "### Basic Experiments\n",
    "- **Compare Models:** Switch between WHISPER_MEDIUM and ORACLE models for the same audio file. Note differences in accuracy, language detection, and processing time.\n",
    "- **Language Variations:** For ORACLE model, try different language codes (e.g., 'es-ES' for Spanish, 'fr-FR' for French) on multilingual audio.\n",
    "- **Diarization Toggle:** Enable/disable speaker diarization in the transcription settings and observe how it affects the output.\n",
    "- **Output Formats:** Add more formats like 'WEBVTT' or 'SBV' to the `additional_transcription_formats` list and compare the results.\n",
    "\n",
    "### Intermediate Projects\n",
    "- **Batch Processing:** Modify the notebook to process multiple audio files in a loop, uploading and submitting jobs for each.\n",
    "- **Audio Preprocessing:** Add steps to convert audio formats or trim files before uploading (using libraries like pydub).\n",
    "- **Cost Estimation:** Calculate estimated costs based on audio duration and model used (check OCI pricing docs).\n",
    "- **Error Handling:** Improve robustness by adding retries for failed uploads or jobs, and better error messages.\n",
    "\n",
    "### Advanced Projects\n",
    "- **Real-Time Integration:** Combine with the real-time speech SDK for live transcription in a web app.\n",
    "- **Post-Processing with AI:** Use the transcript as input to other OCI AI services like Language for sentiment analysis or summarization.\n",
    "- **Multi-Modal App:** Build a full application that transcribes audio, then uses Vision API on related images or videos.\n",
    "- **Custom Vocabulary:** If supported, experiment with custom vocabularies or domain-specific models for specialized audio (e.g., medical or legal terms).\n",
    "\n",
    "### Tips for Learning\n",
    "- Always check the OCI documentation for the latest features and supported formats.\n",
    "- Join the #oci_speech_service_users Slack channel for community support.\n",
    "- Experiment in a free tier or sandbox environment to avoid unexpected costs.\n",
    "\n",
    "Have fun exploring OCI Speech-to-Text!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
